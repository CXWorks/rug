{"dependencies":{"<deque::Buffer<T> as std::clone::Clone>::clone":["deque::Buffer","std::marker::Sized"],"<deque::Flavor as std::clone::Clone>::clone":["deque::Flavor"],"<deque::Flavor as std::cmp::Eq>::assert_receiver_is_total_eq":["deque::Flavor"],"<deque::Flavor as std::cmp::PartialEq>::eq":["deque::Flavor"],"<deque::Flavor as std::fmt::Debug>::fmt":["deque::Flavor","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<deque::Injector<T> as std::default::Default>::default":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"<deque::Injector<T> as std::fmt::Debug>::fmt":["crossbeam_utils::CachePadded","deque::Injector","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<deque::Injector<T> as std::ops::Drop>::drop":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"<deque::Inner<T> as std::ops::Drop>::drop":["crossbeam_utils::CachePadded","deque::Inner","std::marker::Sized","std::sync::atomic::AtomicIsize"],"<deque::Steal<T> as std::clone::Clone>::clone":["deque::Steal","std::marker::Sized"],"<deque::Steal<T> as std::cmp::Eq>::assert_receiver_is_total_eq":["deque::Steal","std::marker::Sized"],"<deque::Steal<T> as std::cmp::PartialEq>::eq":["deque::Steal","std::marker::Sized"],"<deque::Steal<T> as std::fmt::Debug>::fmt":["deque::Steal","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<deque::Steal<T> as std::iter::FromIterator<deque::Steal<T>>>::from_iter":["deque::Steal","std::iter::IntoIterator","std::marker::Sized"],"<deque::Stealer<T> as std::clone::Clone>::clone":["deque::Flavor","deque::Stealer","std::marker::Sized","std::sync::Arc"],"<deque::Stealer<T> as std::fmt::Debug>::fmt":["deque::Flavor","deque::Stealer","std::fmt::Formatter","std::marker::Sized","std::result::Result","std::sync::Arc"],"<deque::Worker<T> as std::fmt::Debug>::fmt":["deque::Flavor","deque::Worker","std::cell::Cell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::Arc"],"deque::Block":["deque::Block","deque::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"deque::Block::<T>::destroy":["deque::Block","deque::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"deque::Block::<T>::new":["deque::Block","deque::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"deque::Block::<T>::wait_next":["deque::Block","deque::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"deque::Buffer":["deque::Buffer","std::marker::Sized"],"deque::Buffer::<T>::alloc":["deque::Buffer","std::marker::Sized"],"deque::Buffer::<T>::at":["deque::Buffer","std::marker::Sized"],"deque::Buffer::<T>::dealloc":["deque::Buffer","std::marker::Sized"],"deque::Buffer::<T>::read":["deque::Buffer","std::marker::Sized"],"deque::Buffer::<T>::write":["deque::Buffer","std::marker::Sized"],"deque::Flavor":["deque::Flavor"],"deque::Injector":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"deque::Injector::<T>::is_empty":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"deque::Injector::<T>::len":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"deque::Injector::<T>::new":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"deque::Injector::<T>::push":["crossbeam_utils::CachePadded","deque::Injector","std::marker::PhantomData","std::marker::Sized"],"deque::Injector::<T>::steal":["crossbeam_utils::CachePadded","deque::Injector","deque::Steal","std::marker::PhantomData","std::marker::Sized"],"deque::Injector::<T>::steal_batch":["crossbeam_utils::CachePadded","deque::Flavor","deque::Injector","deque::Steal","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Injector::<T>::steal_batch_and_pop":["crossbeam_utils::CachePadded","deque::Flavor","deque::Injector","deque::Steal","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Inner":["crossbeam_utils::CachePadded","deque::Inner","std::marker::Sized","std::sync::atomic::AtomicIsize"],"deque::Position":["deque::Position","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"deque::Slot":["deque::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicUsize"],"deque::Slot::<T>::wait_write":["deque::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicUsize"],"deque::Steal":["deque::Steal","std::marker::Sized"],"deque::Steal::<T>::is_empty":["deque::Steal","std::marker::Sized"],"deque::Steal::<T>::is_retry":["deque::Steal","std::marker::Sized"],"deque::Steal::<T>::is_success":["deque::Steal","std::marker::Sized"],"deque::Steal::<T>::or_else":["deque::Steal","std::marker::Sized","std::ops::FnOnce"],"deque::Steal::<T>::success":["deque::Steal","std::marker::Sized","std::option::Option"],"deque::Stealer":["deque::Flavor","deque::Stealer","std::marker::Sized","std::sync::Arc"],"deque::Stealer::<T>::is_empty":["deque::Flavor","deque::Stealer","std::marker::Sized","std::sync::Arc"],"deque::Stealer::<T>::steal":["deque::Flavor","deque::Steal","deque::Stealer","std::marker::Sized","std::sync::Arc"],"deque::Stealer::<T>::steal_batch":["deque::Flavor","deque::Steal","deque::Stealer","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Stealer::<T>::steal_batch_and_pop":["deque::Flavor","deque::Steal","deque::Stealer","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::is_empty":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::len":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::new_fifo":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::new_lifo":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::pop":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::Arc"],"deque::Worker::<T>::push":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::reserve":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::resize":["deque::Flavor","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"],"deque::Worker::<T>::stealer":["deque::Flavor","deque::Stealer","deque::Worker","std::cell::Cell","std::marker::PhantomData","std::marker::Sized","std::sync::Arc"]},"glob_path_import":{},"self_to_fn":{"deque::Block":["impl<T> Block<T> {\n    /// Creates an empty block that starts at `start_index`.\n    fn new() -> Block<T> {\n        // SAFETY: This is safe because:\n        //  [1] `Block::next` (AtomicPtr) may be safely zero initialized.\n        //  [2] `Block::slots` (Array) may be safely zero initialized because of [3, 4].\n        //  [3] `Slot::task` (UnsafeCell) may be safely zero initialized because it\n        //       holds a MaybeUninit.\n        //  [4] `Slot::state` (AtomicUsize) may be safely zero initialized.\n        unsafe { MaybeUninit::zeroed().assume_init() }\n    }\n\n    /// Waits until the next pointer is set.\n    fn wait_next(&self) -> *mut Block<T> {\n        let backoff = Backoff::new();\n        loop {\n            let next = self.next.load(Ordering::Acquire);\n            if !next.is_null() {\n                return next;\n            }\n            backoff.snooze();\n        }\n    }\n\n    /// Sets the `DESTROY` bit in slots starting from `start` and destroys the block.\n    unsafe fn destroy(this: *mut Block<T>, count: usize) {\n        // It is not necessary to set the `DESTROY` bit in the last slot because that slot has\n        // begun destruction of the block.\n        for i in (0..count).rev() {\n            let slot = (*this).slots.get_unchecked(i);\n\n            // Mark the `DESTROY` bit if a thread is still using the slot.\n            if slot.state.load(Ordering::Acquire) & READ == 0\n                && slot.state.fetch_or(DESTROY, Ordering::AcqRel) & READ == 0\n            {\n                // If a thread is still using the slot, it will continue destruction of the block.\n                return;\n            }\n        }\n\n        // No thread is using the block, now it is safe to destroy it.\n        drop(Box::from_raw(this));\n    }\n}"],"deque::Buffer":["impl<T> Buffer<T> {\n    /// Allocates a new buffer with the specified capacity.\n    fn alloc(cap: usize) -> Buffer<T> {\n        debug_assert_eq!(cap, cap.next_power_of_two());\n\n        let mut v = Vec::with_capacity(cap);\n        let ptr = v.as_mut_ptr();\n        mem::forget(v);\n\n        Buffer { ptr, cap }\n    }\n\n    /// Deallocates the buffer.\n    unsafe fn dealloc(self) {\n        drop(Vec::from_raw_parts(self.ptr, 0, self.cap));\n    }\n\n    /// Returns a pointer to the task at the specified `index`.\n    unsafe fn at(&self, index: isize) -> *mut T {\n        // `self.cap` is always a power of two.\n        self.ptr.offset(index & (self.cap - 1) as isize)\n    }\n\n    /// Writes `task` into the specified `index`.\n    ///\n    /// This method might be concurrently called with another `read` at the same index, which is\n    /// technically speaking a data race and therefore UB. We should use an atomic store here, but\n    /// that would be more expensive and difficult to implement generically for all types `T`.\n    /// Hence, as a hack, we use a volatile write instead.\n    unsafe fn write(&self, index: isize, task: T) {\n        ptr::write_volatile(self.at(index), task)\n    }\n\n    /// Reads a task from the specified `index`.\n    ///\n    /// This method might be concurrently called with another `write` at the same index, which is\n    /// technically speaking a data race and therefore UB. We should use an atomic load here, but\n    /// that would be more expensive and difficult to implement generically for all types `T`.\n    /// Hence, as a hack, we use a volatile write instead.\n    unsafe fn read(&self, index: isize) -> T {\n        ptr::read_volatile(self.at(index))\n    }\n}","impl<T> Clone for Buffer<T> {\n    fn clone(&self) -> Buffer<T> {\n        Buffer {\n            ptr: self.ptr,\n            cap: self.cap,\n        }\n    }\n}","impl<T> Copy for Buffer<T> {}","unsafe impl<T> Send for Buffer<T> {}"],"deque::Flavor":["Clone","Copy","Debug","Eq","PartialEq"],"deque::Injector":["impl<T> Default for Injector<T> {\n    fn default() -> Self {\n        let block = Box::into_raw(Box::new(Block::<T>::new()));\n        Self {\n            head: CachePadded::new(Position {\n                block: AtomicPtr::new(block),\n                index: AtomicUsize::new(0),\n            }),\n            tail: CachePadded::new(Position {\n                block: AtomicPtr::new(block),\n                index: AtomicUsize::new(0),\n            }),\n            _marker: PhantomData,\n        }\n    }\n}","impl<T> Drop for Injector<T> {\n    fn drop(&mut self) {\n        let mut head = self.head.index.load(Ordering::Relaxed);\n        let mut tail = self.tail.index.load(Ordering::Relaxed);\n        let mut block = self.head.block.load(Ordering::Relaxed);\n\n        // Erase the lower bits.\n        head &= !((1 << SHIFT) - 1);\n        tail &= !((1 << SHIFT) - 1);\n\n        unsafe {\n            // Drop all values between `head` and `tail` and deallocate the heap-allocated blocks.\n            while head != tail {\n                let offset = (head >> SHIFT) % LAP;\n\n                if offset < BLOCK_CAP {\n                    // Drop the task in the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    let p = &mut *slot.task.get();\n                    p.as_mut_ptr().drop_in_place();\n                } else {\n                    // Deallocate the block and move to the next one.\n                    let next = (*block).next.load(Ordering::Relaxed);\n                    drop(Box::from_raw(block));\n                    block = next;\n                }\n\n                head = head.wrapping_add(1 << SHIFT);\n            }\n\n            // Deallocate the last remaining block.\n            drop(Box::from_raw(block));\n        }\n    }\n}","impl<T> Injector<T> {\n    /// Creates a new injector queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Injector;\n    ///\n    /// let q = Injector::<i32>::new();\n    /// ```\n    pub fn new() -> Injector<T> {\n        Self::default()\n    }\n\n    /// Pushes a task into the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Injector;\n    ///\n    /// let w = Injector::new();\n    /// w.push(1);\n    /// w.push(2);\n    /// ```\n    pub fn push(&self, task: T) {\n        let backoff = Backoff::new();\n        let mut tail = self.tail.index.load(Ordering::Acquire);\n        let mut block = self.tail.block.load(Ordering::Acquire);\n        let mut next_block = None;\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (tail >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n                tail = self.tail.index.load(Ordering::Acquire);\n                block = self.tail.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // If we're going to have to install the next block, allocate it in advance in order to\n            // make the wait for other threads as short as possible.\n            if offset + 1 == BLOCK_CAP && next_block.is_none() {\n                next_block = Some(Box::new(Block::<T>::new()));\n            }\n\n            let new_tail = tail + (1 << SHIFT);\n\n            // Try advancing the tail forward.\n            match self.tail.index.compare_exchange_weak(\n                tail,\n                new_tail,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, install the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next_block = Box::into_raw(next_block.unwrap());\n                        let next_index = new_tail.wrapping_add(1 << SHIFT);\n\n                        self.tail.block.store(next_block, Ordering::Release);\n                        self.tail.index.store(next_index, Ordering::Release);\n                        (*block).next.store(next_block, Ordering::Release);\n                    }\n\n                    // Write the task into the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.task.get().write(MaybeUninit::new(task));\n                    slot.state.fetch_or(WRITE, Ordering::Release);\n\n                    return;\n                },\n                Err(t) => {\n                    tail = t;\n                    block = self.tail.block.load(Ordering::Acquire);\n                    backoff.spin();\n                }\n            }\n        }\n    }\n\n    /// Steals a task from the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::{Injector, Steal};\n    ///\n    /// let q = Injector::new();\n    /// q.push(1);\n    /// q.push(2);\n    ///\n    /// assert_eq!(q.steal(), Steal::Success(1));\n    /// assert_eq!(q.steal(), Steal::Success(2));\n    /// assert_eq!(q.steal(), Steal::Empty);\n    /// ```\n    pub fn steal(&self) -> Steal<T> {\n        let mut head;\n        let mut block;\n        let mut offset;\n\n        let backoff = Backoff::new();\n        loop {\n            head = self.head.index.load(Ordering::Acquire);\n            block = self.head.block.load(Ordering::Acquire);\n\n            // Calculate the offset of the index into the block.\n            offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n            } else {\n                break;\n            }\n        }\n\n        let mut new_head = head + (1 << SHIFT);\n\n        if new_head & HAS_NEXT == 0 {\n            atomic::fence(Ordering::SeqCst);\n            let tail = self.tail.index.load(Ordering::Relaxed);\n\n            // If the tail equals the head, that means the queue is empty.\n            if head >> SHIFT == tail >> SHIFT {\n                return Steal::Empty;\n            }\n\n            // If head and tail are not in the same block, set `HAS_NEXT` in head.\n            if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                new_head |= HAS_NEXT;\n            }\n        }\n\n        // Try moving the head index forward.\n        if self\n            .head\n            .index\n            .compare_exchange_weak(head, new_head, Ordering::SeqCst, Ordering::Acquire)\n            .is_err()\n        {\n            return Steal::Retry;\n        }\n\n        unsafe {\n            // If we've reached the end of the block, move to the next one.\n            if offset + 1 == BLOCK_CAP {\n                let next = (*block).wait_next();\n                let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                if !(*next).next.load(Ordering::Relaxed).is_null() {\n                    next_index |= HAS_NEXT;\n                }\n\n                self.head.block.store(next, Ordering::Release);\n                self.head.index.store(next_index, Ordering::Release);\n            }\n\n            // Read the task.\n            let slot = (*block).slots.get_unchecked(offset);\n            slot.wait_write();\n            let task = slot.task.get().read().assume_init();\n\n            // Destroy the block if we've reached the end, or if another thread wanted to destroy\n            // but couldn't because we were busy reading from the slot.\n            if (offset + 1 == BLOCK_CAP) || (slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0) {\n                Block::destroy(block, offset);\n            }\n\n            Steal::Success(task)\n        }\n    }\n\n    /// Steals a batch of tasks and pushes them into a worker.\n    ///\n    /// How many tasks exactly will be stolen is not specified. That said, this method will try to\n    /// steal around half of the tasks in the queue, but also not more than some constant limit.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::{Injector, Worker};\n    ///\n    /// let q = Injector::new();\n    /// q.push(1);\n    /// q.push(2);\n    /// q.push(3);\n    /// q.push(4);\n    ///\n    /// let w = Worker::new_fifo();\n    /// let _ = q.steal_batch(&w);\n    /// assert_eq!(w.pop(), Some(1));\n    /// assert_eq!(w.pop(), Some(2));\n    /// ```\n    pub fn steal_batch(&self, dest: &Worker<T>) -> Steal<()> {\n        let mut head;\n        let mut block;\n        let mut offset;\n\n        let backoff = Backoff::new();\n        loop {\n            head = self.head.index.load(Ordering::Acquire);\n            block = self.head.block.load(Ordering::Acquire);\n\n            // Calculate the offset of the index into the block.\n            offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n            } else {\n                break;\n            }\n        }\n\n        let mut new_head = head;\n        let advance;\n\n        if new_head & HAS_NEXT == 0 {\n            atomic::fence(Ordering::SeqCst);\n            let tail = self.tail.index.load(Ordering::Relaxed);\n\n            // If the tail equals the head, that means the queue is empty.\n            if head >> SHIFT == tail >> SHIFT {\n                return Steal::Empty;\n            }\n\n            // If head and tail are not in the same block, set `HAS_NEXT` in head. Also, calculate\n            // the right batch size to steal.\n            if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                new_head |= HAS_NEXT;\n                // We can steal all tasks till the end of the block.\n                advance = (BLOCK_CAP - offset).min(MAX_BATCH);\n            } else {\n                let len = (tail - head) >> SHIFT;\n                // Steal half of the available tasks.\n                advance = ((len + 1) / 2).min(MAX_BATCH);\n            }\n        } else {\n            // We can steal all tasks till the end of the block.\n            advance = (BLOCK_CAP - offset).min(MAX_BATCH);\n        }\n\n        new_head += advance << SHIFT;\n        let new_offset = offset + advance;\n\n        // Try moving the head index forward.\n        if self\n            .head\n            .index\n            .compare_exchange_weak(head, new_head, Ordering::SeqCst, Ordering::Acquire)\n            .is_err()\n        {\n            return Steal::Retry;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = new_offset - offset;\n        dest.reserve(batch_size);\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        unsafe {\n            // If we've reached the end of the block, move to the next one.\n            if new_offset == BLOCK_CAP {\n                let next = (*block).wait_next();\n                let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                if !(*next).next.load(Ordering::Relaxed).is_null() {\n                    next_index |= HAS_NEXT;\n                }\n\n                self.head.block.store(next, Ordering::Release);\n                self.head.index.store(next_index, Ordering::Release);\n            }\n\n            // Copy values from the injector into the destination queue.\n            match dest.flavor {\n                Flavor::Fifo => {\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add(i as isize), task);\n                    }\n                }\n\n                Flavor::Lifo => {\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add((batch_size - 1 - i) as isize), task);\n                    }\n                }\n            }\n\n            atomic::fence(Ordering::Release);\n\n            // Update the back index in the destination queue.\n            //\n            // This ordering could be `Relaxed`, but then thread sanitizer would falsely report\n            // data races because it doesn't understand fences.\n            dest.inner\n                .back\n                .store(dest_b.wrapping_add(batch_size as isize), Ordering::Release);\n\n            // Destroy the block if we've reached the end, or if another thread wanted to destroy\n            // but couldn't because we were busy reading from the slot.\n            if new_offset == BLOCK_CAP {\n                Block::destroy(block, offset);\n            } else {\n                for i in offset..new_offset {\n                    let slot = (*block).slots.get_unchecked(i);\n\n                    if slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0 {\n                        Block::destroy(block, offset);\n                        break;\n                    }\n                }\n            }\n\n            Steal::Success(())\n        }\n    }\n\n    /// Steals a batch of tasks, pushes them into a worker, and pops a task from that worker.\n    ///\n    /// How many tasks exactly will be stolen is not specified. That said, this method will try to\n    /// steal around half of the tasks in the queue, but also not more than some constant limit.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::{Injector, Steal, Worker};\n    ///\n    /// let q = Injector::new();\n    /// q.push(1);\n    /// q.push(2);\n    /// q.push(3);\n    /// q.push(4);\n    ///\n    /// let w = Worker::new_fifo();\n    /// assert_eq!(q.steal_batch_and_pop(&w), Steal::Success(1));\n    /// assert_eq!(w.pop(), Some(2));\n    /// ```\n    pub fn steal_batch_and_pop(&self, dest: &Worker<T>) -> Steal<T> {\n        let mut head;\n        let mut block;\n        let mut offset;\n\n        let backoff = Backoff::new();\n        loop {\n            head = self.head.index.load(Ordering::Acquire);\n            block = self.head.block.load(Ordering::Acquire);\n\n            // Calculate the offset of the index into the block.\n            offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n            } else {\n                break;\n            }\n        }\n\n        let mut new_head = head;\n        let advance;\n\n        if new_head & HAS_NEXT == 0 {\n            atomic::fence(Ordering::SeqCst);\n            let tail = self.tail.index.load(Ordering::Relaxed);\n\n            // If the tail equals the head, that means the queue is empty.\n            if head >> SHIFT == tail >> SHIFT {\n                return Steal::Empty;\n            }\n\n            // If head and tail are not in the same block, set `HAS_NEXT` in head.\n            if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                new_head |= HAS_NEXT;\n                // We can steal all tasks till the end of the block.\n                advance = (BLOCK_CAP - offset).min(MAX_BATCH + 1);\n            } else {\n                let len = (tail - head) >> SHIFT;\n                // Steal half of the available tasks.\n                advance = ((len + 1) / 2).min(MAX_BATCH + 1);\n            }\n        } else {\n            // We can steal all tasks till the end of the block.\n            advance = (BLOCK_CAP - offset).min(MAX_BATCH + 1);\n        }\n\n        new_head += advance << SHIFT;\n        let new_offset = offset + advance;\n\n        // Try moving the head index forward.\n        if self\n            .head\n            .index\n            .compare_exchange_weak(head, new_head, Ordering::SeqCst, Ordering::Acquire)\n            .is_err()\n        {\n            return Steal::Retry;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = new_offset - offset - 1;\n        dest.reserve(batch_size);\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        unsafe {\n            // If we've reached the end of the block, move to the next one.\n            if new_offset == BLOCK_CAP {\n                let next = (*block).wait_next();\n                let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                if !(*next).next.load(Ordering::Relaxed).is_null() {\n                    next_index |= HAS_NEXT;\n                }\n\n                self.head.block.store(next, Ordering::Release);\n                self.head.index.store(next_index, Ordering::Release);\n            }\n\n            // Read the task.\n            let slot = (*block).slots.get_unchecked(offset);\n            slot.wait_write();\n            let task = slot.task.get().read().assume_init();\n\n            match dest.flavor {\n                Flavor::Fifo => {\n                    // Copy values from the injector into the destination queue.\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i + 1);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add(i as isize), task);\n                    }\n                }\n\n                Flavor::Lifo => {\n                    // Copy values from the injector into the destination queue.\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i + 1);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add((batch_size - 1 - i) as isize), task);\n                    }\n                }\n            }\n\n            atomic::fence(Ordering::Release);\n\n            // Update the back index in the destination queue.\n            //\n            // This ordering could be `Relaxed`, but then thread sanitizer would falsely report\n            // data races because it doesn't understand fences.\n            dest.inner\n                .back\n                .store(dest_b.wrapping_add(batch_size as isize), Ordering::Release);\n\n            // Destroy the block if we've reached the end, or if another thread wanted to destroy\n            // but couldn't because we were busy reading from the slot.\n            if new_offset == BLOCK_CAP {\n                Block::destroy(block, offset);\n            } else {\n                for i in offset..new_offset {\n                    let slot = (*block).slots.get_unchecked(i);\n\n                    if slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0 {\n                        Block::destroy(block, offset);\n                        break;\n                    }\n                }\n            }\n\n            Steal::Success(task)\n        }\n    }\n\n    /// Returns `true` if the queue is empty.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Injector;\n    ///\n    /// let q = Injector::new();\n    ///\n    /// assert!(q.is_empty());\n    /// q.push(1);\n    /// assert!(!q.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        let head = self.head.index.load(Ordering::SeqCst);\n        let tail = self.tail.index.load(Ordering::SeqCst);\n        head >> SHIFT == tail >> SHIFT\n    }\n\n    /// Returns the number of tasks in the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Injector;\n    ///\n    /// let q = Injector::new();\n    ///\n    /// assert_eq!(q.len(), 0);\n    /// q.push(1);\n    /// assert_eq!(q.len(), 1);\n    /// q.push(1);\n    /// assert_eq!(q.len(), 2);\n    /// ```\n    pub fn len(&self) -> usize {\n        loop {\n            // Load the tail index, then load the head index.\n            let mut tail = self.tail.index.load(Ordering::SeqCst);\n            let mut head = self.head.index.load(Ordering::SeqCst);\n\n            // If the tail index didn't change, we've got consistent indices to work with.\n            if self.tail.index.load(Ordering::SeqCst) == tail {\n                // Erase the lower bits.\n                tail &= !((1 << SHIFT) - 1);\n                head &= !((1 << SHIFT) - 1);\n\n                // Fix up indices if they fall onto block ends.\n                if (tail >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    tail = tail.wrapping_add(1 << SHIFT);\n                }\n                if (head >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    head = head.wrapping_add(1 << SHIFT);\n                }\n\n                // Rotate indices so that head falls into the first block.\n                let lap = (head >> SHIFT) / LAP;\n                tail = tail.wrapping_sub((lap * LAP) << SHIFT);\n                head = head.wrapping_sub((lap * LAP) << SHIFT);\n\n                // Remove the lower bits.\n                tail >>= SHIFT;\n                head >>= SHIFT;\n\n                // Return the difference minus the number of blocks between tail and head.\n                return tail - head - tail / LAP;\n            }\n        }\n    }\n}","impl<T> fmt::Debug for Injector<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Worker { .. }\")\n    }\n}","unsafe impl<T: Send> Send for Injector<T> {}","unsafe impl<T: Send> Sync for Injector<T> {}"],"deque::Inner":["impl<T> Drop for Inner<T> {\n    fn drop(&mut self) {\n        // Load the back index, front index, and buffer.\n        let b = self.back.load(Ordering::Relaxed);\n        let f = self.front.load(Ordering::Relaxed);\n\n        unsafe {\n            let buffer = self.buffer.load(Ordering::Relaxed, epoch::unprotected());\n\n            // Go through the buffer from front to back and drop all tasks in the queue.\n            let mut i = f;\n            while i != b {\n                buffer.deref().at(i).drop_in_place();\n                i = i.wrapping_add(1);\n            }\n\n            // Free the memory allocated by the buffer.\n            buffer.into_owned().into_box().dealloc();\n        }\n    }\n}"],"deque::Slot":["impl<T> Slot<T> {\n    /// Waits until a task is written into the slot.\n    fn wait_write(&self) {\n        let backoff = Backoff::new();\n        while self.state.load(Ordering::Acquire) & WRITE == 0 {\n            backoff.snooze();\n        }\n    }\n}"],"deque::Steal":["Clone","Copy","Eq","PartialEq","impl<T> FromIterator<Steal<T>> for Steal<T> {\n    /// Consumes items until a `Success` is found and returns it.\n    ///\n    /// If no `Success` was found, but there was at least one `Retry`, then returns `Retry`.\n    /// Otherwise, `Empty` is returned.\n    fn from_iter<I>(iter: I) -> Steal<T>\n    where\n        I: IntoIterator<Item = Steal<T>>,\n    {\n        let mut retry = false;\n        for s in iter {\n            match &s {\n                Steal::Empty => {}\n                Steal::Success(_) => return s,\n                Steal::Retry => retry = true,\n            }\n        }\n\n        if retry {\n            Steal::Retry\n        } else {\n            Steal::Empty\n        }\n    }\n}","impl<T> Steal<T> {\n    /// Returns `true` if the queue was empty at the time of stealing.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Steal::{Empty, Retry, Success};\n    ///\n    /// assert!(!Success(7).is_empty());\n    /// assert!(!Retry::<i32>.is_empty());\n    ///\n    /// assert!(Empty::<i32>.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        match self {\n            Steal::Empty => true,\n            _ => false,\n        }\n    }\n\n    /// Returns `true` if at least one task was stolen.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Steal::{Empty, Retry, Success};\n    ///\n    /// assert!(!Empty::<i32>.is_success());\n    /// assert!(!Retry::<i32>.is_success());\n    ///\n    /// assert!(Success(7).is_success());\n    /// ```\n    pub fn is_success(&self) -> bool {\n        match self {\n            Steal::Success(_) => true,\n            _ => false,\n        }\n    }\n\n    /// Returns `true` if the steal operation needs to be retried.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Steal::{Empty, Retry, Success};\n    ///\n    /// assert!(!Empty::<i32>.is_retry());\n    /// assert!(!Success(7).is_retry());\n    ///\n    /// assert!(Retry::<i32>.is_retry());\n    /// ```\n    pub fn is_retry(&self) -> bool {\n        match self {\n            Steal::Retry => true,\n            _ => false,\n        }\n    }\n\n    /// Returns the result of the operation, if successful.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Steal::{Empty, Retry, Success};\n    ///\n    /// assert_eq!(Empty::<i32>.success(), None);\n    /// assert_eq!(Retry::<i32>.success(), None);\n    ///\n    /// assert_eq!(Success(7).success(), Some(7));\n    /// ```\n    pub fn success(self) -> Option<T> {\n        match self {\n            Steal::Success(res) => Some(res),\n            _ => None,\n        }\n    }\n\n    /// If no task was stolen, attempts another steal operation.\n    ///\n    /// Returns this steal result if it is `Success`. Otherwise, closure `f` is invoked and then:\n    ///\n    /// * If the second steal resulted in `Success`, it is returned.\n    /// * If both steals were unsuccessful but any resulted in `Retry`, then `Retry` is returned.\n    /// * If both resulted in `None`, then `None` is returned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Steal::{Empty, Retry, Success};\n    ///\n    /// assert_eq!(Success(1).or_else(|| Success(2)), Success(1));\n    /// assert_eq!(Retry.or_else(|| Success(2)), Success(2));\n    ///\n    /// assert_eq!(Retry.or_else(|| Empty), Retry::<i32>);\n    /// assert_eq!(Empty.or_else(|| Retry), Retry::<i32>);\n    ///\n    /// assert_eq!(Empty.or_else(|| Empty), Empty::<i32>);\n    /// ```\n    pub fn or_else<F>(self, f: F) -> Steal<T>\n    where\n        F: FnOnce() -> Steal<T>,\n    {\n        match self {\n            Steal::Empty => f(),\n            Steal::Success(_) => self,\n            Steal::Retry => {\n                if let Steal::Success(res) = f() {\n                    Steal::Success(res)\n                } else {\n                    Steal::Retry\n                }\n            }\n        }\n    }\n}","impl<T> fmt::Debug for Steal<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Steal::Empty => f.pad(\"Empty\"),\n            Steal::Success(_) => f.pad(\"Success(..)\"),\n            Steal::Retry => f.pad(\"Retry\"),\n        }\n    }\n}"],"deque::Stealer":["impl<T> Clone for Stealer<T> {\n    fn clone(&self) -> Stealer<T> {\n        Stealer {\n            inner: self.inner.clone(),\n            flavor: self.flavor,\n        }\n    }\n}","impl<T> Stealer<T> {\n    /// Returns `true` if the queue is empty.\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::new_lifo();\n    /// let s = w.stealer();\n    ///\n    /// assert!(s.is_empty());\n    /// w.push(1);\n    /// assert!(!s.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        let f = self.inner.front.load(Ordering::Acquire);\n        atomic::fence(Ordering::SeqCst);\n        let b = self.inner.back.load(Ordering::Acquire);\n        b.wrapping_sub(f) <= 0\n    }\n\n    /// Steals a task from the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::{Steal, Worker};\n    ///\n    /// let w = Worker::new_lifo();\n    /// w.push(1);\n    /// w.push(2);\n    ///\n    /// let s = w.stealer();\n    /// assert_eq!(s.steal(), Steal::Success(1));\n    /// assert_eq!(s.steal(), Steal::Success(2));\n    /// ```\n    pub fn steal(&self) -> Steal<T> {\n        // Load the front index.\n        let f = self.inner.front.load(Ordering::Acquire);\n\n        // A SeqCst fence is needed here.\n        //\n        // If the current thread is already pinned (reentrantly), we must manually issue the\n        // fence. Otherwise, the following pinning will issue the fence anyway, so we don't\n        // have to.\n        if epoch::is_pinned() {\n            atomic::fence(Ordering::SeqCst);\n        }\n\n        let guard = &epoch::pin();\n\n        // Load the back index.\n        let b = self.inner.back.load(Ordering::Acquire);\n\n        // Is the queue empty?\n        if b.wrapping_sub(f) <= 0 {\n            return Steal::Empty;\n        }\n\n        // Load the buffer and read the task at the front.\n        let buffer = self.inner.buffer.load(Ordering::Acquire, guard);\n        let task = unsafe { buffer.deref().read(f) };\n\n        // Try incrementing the front index to steal the task.\n        if self\n            .inner\n            .front\n            .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n            .is_err()\n        {\n            // We didn't steal this task, forget it.\n            mem::forget(task);\n            return Steal::Retry;\n        }\n\n        // Return the stolen task.\n        Steal::Success(task)\n    }\n\n    /// Steals a batch of tasks and pushes them into another worker.\n    ///\n    /// How many tasks exactly will be stolen is not specified. That said, this method will try to\n    /// steal around half of the tasks in the queue, but also not more than some constant limit.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w1 = Worker::new_fifo();\n    /// w1.push(1);\n    /// w1.push(2);\n    /// w1.push(3);\n    /// w1.push(4);\n    ///\n    /// let s = w1.stealer();\n    /// let w2 = Worker::new_fifo();\n    ///\n    /// let _ = s.steal_batch(&w2);\n    /// assert_eq!(w2.pop(), Some(1));\n    /// assert_eq!(w2.pop(), Some(2));\n    /// ```\n    pub fn steal_batch(&self, dest: &Worker<T>) -> Steal<()> {\n        if Arc::ptr_eq(&self.inner, &dest.inner) {\n            if dest.is_empty() {\n                return Steal::Empty;\n            } else {\n                return Steal::Success(());\n            }\n        }\n\n        // Load the front index.\n        let mut f = self.inner.front.load(Ordering::Acquire);\n\n        // A SeqCst fence is needed here.\n        //\n        // If the current thread is already pinned (reentrantly), we must manually issue the\n        // fence. Otherwise, the following pinning will issue the fence anyway, so we don't\n        // have to.\n        if epoch::is_pinned() {\n            atomic::fence(Ordering::SeqCst);\n        }\n\n        let guard = &epoch::pin();\n\n        // Load the back index.\n        let b = self.inner.back.load(Ordering::Acquire);\n\n        // Is the queue empty?\n        let len = b.wrapping_sub(f);\n        if len <= 0 {\n            return Steal::Empty;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = cmp::min((len as usize + 1) / 2, MAX_BATCH);\n        dest.reserve(batch_size);\n        let mut batch_size = batch_size as isize;\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let mut dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        // Load the buffer.\n        let buffer = self.inner.buffer.load(Ordering::Acquire, guard);\n\n        match self.flavor {\n            // Steal a batch of tasks from the front at once.\n            Flavor::Fifo => {\n                // Copy the batch from the source to the destination buffer.\n                match dest.flavor {\n                    Flavor::Fifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i));\n                                dest_buffer.write(dest_b.wrapping_add(i), task);\n                            }\n                        }\n                    }\n                    Flavor::Lifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i));\n                                dest_buffer.write(dest_b.wrapping_add(batch_size - 1 - i), task);\n                            }\n                        }\n                    }\n                }\n\n                // Try incrementing the front index to steal the batch.\n                if self\n                    .inner\n                    .front\n                    .compare_exchange(\n                        f,\n                        f.wrapping_add(batch_size),\n                        Ordering::SeqCst,\n                        Ordering::Relaxed,\n                    )\n                    .is_err()\n                {\n                    return Steal::Retry;\n                }\n\n                dest_b = dest_b.wrapping_add(batch_size);\n            }\n\n            // Steal a batch of tasks from the front one by one.\n            Flavor::Lifo => {\n                for i in 0..batch_size {\n                    // If this is not the first steal, check whether the queue is empty.\n                    if i > 0 {\n                        // We've already got the current front index. Now execute the fence to\n                        // synchronize with other threads.\n                        atomic::fence(Ordering::SeqCst);\n\n                        // Load the back index.\n                        let b = self.inner.back.load(Ordering::Acquire);\n\n                        // Is the queue empty?\n                        if b.wrapping_sub(f) <= 0 {\n                            batch_size = i;\n                            break;\n                        }\n                    }\n\n                    // Read the task at the front.\n                    let task = unsafe { buffer.deref().read(f) };\n\n                    // Try incrementing the front index to steal the task.\n                    if self\n                        .inner\n                        .front\n                        .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n                        .is_err()\n                    {\n                        // We didn't steal this task, forget it and break from the loop.\n                        mem::forget(task);\n                        batch_size = i;\n                        break;\n                    }\n\n                    // Write the stolen task into the destination buffer.\n                    unsafe {\n                        dest_buffer.write(dest_b, task);\n                    }\n\n                    // Move the source front index and the destination back index one step forward.\n                    f = f.wrapping_add(1);\n                    dest_b = dest_b.wrapping_add(1);\n                }\n\n                // If we didn't steal anything, the operation needs to be retried.\n                if batch_size == 0 {\n                    return Steal::Retry;\n                }\n\n                // If stealing into a FIFO queue, stolen tasks need to be reversed.\n                if dest.flavor == Flavor::Fifo {\n                    for i in 0..batch_size / 2 {\n                        unsafe {\n                            let i1 = dest_b.wrapping_sub(batch_size - i);\n                            let i2 = dest_b.wrapping_sub(i + 1);\n                            let t1 = dest_buffer.read(i1);\n                            let t2 = dest_buffer.read(i2);\n                            dest_buffer.write(i1, t2);\n                            dest_buffer.write(i2, t1);\n                        }\n                    }\n                }\n            }\n        }\n\n        atomic::fence(Ordering::Release);\n\n        // Update the back index in the destination queue.\n        //\n        // This ordering could be `Relaxed`, but then thread sanitizer would falsely report data\n        // races because it doesn't understand fences.\n        dest.inner.back.store(dest_b, Ordering::Release);\n\n        // Return with success.\n        Steal::Success(())\n    }\n\n    /// Steals a batch of tasks, pushes them into another worker, and pops a task from that worker.\n    ///\n    /// How many tasks exactly will be stolen is not specified. That said, this method will try to\n    /// steal around half of the tasks in the queue, but also not more than some constant limit.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::{Steal, Worker};\n    ///\n    /// let w1 = Worker::new_fifo();\n    /// w1.push(1);\n    /// w1.push(2);\n    /// w1.push(3);\n    /// w1.push(4);\n    ///\n    /// let s = w1.stealer();\n    /// let w2 = Worker::new_fifo();\n    ///\n    /// assert_eq!(s.steal_batch_and_pop(&w2), Steal::Success(1));\n    /// assert_eq!(w2.pop(), Some(2));\n    /// ```\n    pub fn steal_batch_and_pop(&self, dest: &Worker<T>) -> Steal<T> {\n        if Arc::ptr_eq(&self.inner, &dest.inner) {\n            match dest.pop() {\n                None => return Steal::Empty,\n                Some(task) => return Steal::Success(task),\n            }\n        }\n\n        // Load the front index.\n        let mut f = self.inner.front.load(Ordering::Acquire);\n\n        // A SeqCst fence is needed here.\n        //\n        // If the current thread is already pinned (reentrantly), we must manually issue the\n        // fence. Otherwise, the following pinning will issue the fence anyway, so we don't\n        // have to.\n        if epoch::is_pinned() {\n            atomic::fence(Ordering::SeqCst);\n        }\n\n        let guard = &epoch::pin();\n\n        // Load the back index.\n        let b = self.inner.back.load(Ordering::Acquire);\n\n        // Is the queue empty?\n        let len = b.wrapping_sub(f);\n        if len <= 0 {\n            return Steal::Empty;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = cmp::min((len as usize - 1) / 2, MAX_BATCH - 1);\n        dest.reserve(batch_size);\n        let mut batch_size = batch_size as isize;\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let mut dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        // Load the buffer\n        let buffer = self.inner.buffer.load(Ordering::Acquire, guard);\n\n        // Read the task at the front.\n        let mut task = unsafe { buffer.deref().read(f) };\n\n        match self.flavor {\n            // Steal a batch of tasks from the front at once.\n            Flavor::Fifo => {\n                // Copy the batch from the source to the destination buffer.\n                match dest.flavor {\n                    Flavor::Fifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i + 1));\n                                dest_buffer.write(dest_b.wrapping_add(i), task);\n                            }\n                        }\n                    }\n                    Flavor::Lifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i + 1));\n                                dest_buffer.write(dest_b.wrapping_add(batch_size - 1 - i), task);\n                            }\n                        }\n                    }\n                }\n\n                // Try incrementing the front index to steal the batch.\n                if self\n                    .inner\n                    .front\n                    .compare_exchange(\n                        f,\n                        f.wrapping_add(batch_size + 1),\n                        Ordering::SeqCst,\n                        Ordering::Relaxed,\n                    )\n                    .is_err()\n                {\n                    // We didn't steal this task, forget it.\n                    mem::forget(task);\n                    return Steal::Retry;\n                }\n\n                dest_b = dest_b.wrapping_add(batch_size);\n            }\n\n            // Steal a batch of tasks from the front one by one.\n            Flavor::Lifo => {\n                // Try incrementing the front index to steal the task.\n                if self\n                    .inner\n                    .front\n                    .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n                    .is_err()\n                {\n                    // We didn't steal this task, forget it.\n                    mem::forget(task);\n                    return Steal::Retry;\n                }\n\n                // Move the front index one step forward.\n                f = f.wrapping_add(1);\n\n                // Repeat the same procedure for the batch steals.\n                for i in 0..batch_size {\n                    // We've already got the current front index. Now execute the fence to\n                    // synchronize with other threads.\n                    atomic::fence(Ordering::SeqCst);\n\n                    // Load the back index.\n                    let b = self.inner.back.load(Ordering::Acquire);\n\n                    // Is the queue empty?\n                    if b.wrapping_sub(f) <= 0 {\n                        batch_size = i;\n                        break;\n                    }\n\n                    // Read the task at the front.\n                    let tmp = unsafe { buffer.deref().read(f) };\n\n                    // Try incrementing the front index to steal the task.\n                    if self\n                        .inner\n                        .front\n                        .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n                        .is_err()\n                    {\n                        // We didn't steal this task, forget it and break from the loop.\n                        mem::forget(tmp);\n                        batch_size = i;\n                        break;\n                    }\n\n                    // Write the previously stolen task into the destination buffer.\n                    unsafe {\n                        dest_buffer.write(dest_b, mem::replace(&mut task, tmp));\n                    }\n\n                    // Move the source front index and the destination back index one step forward.\n                    f = f.wrapping_add(1);\n                    dest_b = dest_b.wrapping_add(1);\n                }\n\n                // If stealing into a FIFO queue, stolen tasks need to be reversed.\n                if dest.flavor == Flavor::Fifo {\n                    for i in 0..batch_size / 2 {\n                        unsafe {\n                            let i1 = dest_b.wrapping_sub(batch_size - i);\n                            let i2 = dest_b.wrapping_sub(i + 1);\n                            let t1 = dest_buffer.read(i1);\n                            let t2 = dest_buffer.read(i2);\n                            dest_buffer.write(i1, t2);\n                            dest_buffer.write(i2, t1);\n                        }\n                    }\n                }\n            }\n        }\n\n        atomic::fence(Ordering::Release);\n\n        // Update the back index in the destination queue.\n        //\n        // This ordering could be `Relaxed`, but then thread sanitizer would falsely report data\n        // races because it doesn't understand fences.\n        dest.inner.back.store(dest_b, Ordering::Release);\n\n        // Return with success.\n        Steal::Success(task)\n    }\n}","impl<T> fmt::Debug for Stealer<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Stealer { .. }\")\n    }\n}","unsafe impl<T: Send> Send for Stealer<T> {}","unsafe impl<T: Send> Sync for Stealer<T> {}"],"deque::Worker":["impl<T> Worker<T> {\n    /// Creates a FIFO worker queue.\n    ///\n    /// Tasks are pushed and popped from opposite ends.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::<i32>::new_fifo();\n    /// ```\n    pub fn new_fifo() -> Worker<T> {\n        let buffer = Buffer::alloc(MIN_CAP);\n\n        let inner = Arc::new(CachePadded::new(Inner {\n            front: AtomicIsize::new(0),\n            back: AtomicIsize::new(0),\n            buffer: CachePadded::new(Atomic::new(buffer)),\n        }));\n\n        Worker {\n            inner,\n            buffer: Cell::new(buffer),\n            flavor: Flavor::Fifo,\n            _marker: PhantomData,\n        }\n    }\n\n    /// Creates a LIFO worker queue.\n    ///\n    /// Tasks are pushed and popped from the same end.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::<i32>::new_lifo();\n    /// ```\n    pub fn new_lifo() -> Worker<T> {\n        let buffer = Buffer::alloc(MIN_CAP);\n\n        let inner = Arc::new(CachePadded::new(Inner {\n            front: AtomicIsize::new(0),\n            back: AtomicIsize::new(0),\n            buffer: CachePadded::new(Atomic::new(buffer)),\n        }));\n\n        Worker {\n            inner,\n            buffer: Cell::new(buffer),\n            flavor: Flavor::Lifo,\n            _marker: PhantomData,\n        }\n    }\n\n    /// Creates a stealer for this queue.\n    ///\n    /// The returned stealer can be shared among threads and cloned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::<i32>::new_lifo();\n    /// let s = w.stealer();\n    /// ```\n    pub fn stealer(&self) -> Stealer<T> {\n        Stealer {\n            inner: self.inner.clone(),\n            flavor: self.flavor,\n        }\n    }\n\n    /// Resizes the internal buffer to the new capacity of `new_cap`.\n    #[cold]\n    unsafe fn resize(&self, new_cap: usize) {\n        // Load the back index, front index, and buffer.\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::Relaxed);\n        let buffer = self.buffer.get();\n\n        // Allocate a new buffer and copy data from the old buffer to the new one.\n        let new = Buffer::alloc(new_cap);\n        let mut i = f;\n        while i != b {\n            ptr::copy_nonoverlapping(buffer.at(i), new.at(i), 1);\n            i = i.wrapping_add(1);\n        }\n\n        let guard = &epoch::pin();\n\n        // Replace the old buffer with the new one.\n        self.buffer.replace(new);\n        let old =\n            self.inner\n                .buffer\n                .swap(Owned::new(new).into_shared(guard), Ordering::Release, guard);\n\n        // Destroy the old buffer later.\n        guard.defer_unchecked(move || old.into_owned().into_box().dealloc());\n\n        // If the buffer is very large, then flush the thread-local garbage in order to deallocate\n        // it as soon as possible.\n        if mem::size_of::<T>() * new_cap >= FLUSH_THRESHOLD_BYTES {\n            guard.flush();\n        }\n    }\n\n    /// Reserves enough capacity so that `reserve_cap` tasks can be pushed without growing the\n    /// buffer.\n    fn reserve(&self, reserve_cap: usize) {\n        if reserve_cap > 0 {\n            // Compute the current length.\n            let b = self.inner.back.load(Ordering::Relaxed);\n            let f = self.inner.front.load(Ordering::SeqCst);\n            let len = b.wrapping_sub(f) as usize;\n\n            // The current capacity.\n            let cap = self.buffer.get().cap;\n\n            // Is there enough capacity to push `reserve_cap` tasks?\n            if cap - len < reserve_cap {\n                // Keep doubling the capacity as much as is needed.\n                let mut new_cap = cap * 2;\n                while new_cap - len < reserve_cap {\n                    new_cap *= 2;\n                }\n\n                // Resize the buffer.\n                unsafe {\n                    self.resize(new_cap);\n                }\n            }\n        }\n    }\n\n    /// Returns `true` if the queue is empty.\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::new_lifo();\n    ///\n    /// assert!(w.is_empty());\n    /// w.push(1);\n    /// assert!(!w.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::SeqCst);\n        b.wrapping_sub(f) <= 0\n    }\n\n    /// Returns the number of tasks in the deque.\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::new_lifo();\n    ///\n    /// assert_eq!(w.len(), 0);\n    /// w.push(1);\n    /// assert_eq!(w.len(), 1);\n    /// w.push(1);\n    /// assert_eq!(w.len(), 2);\n    /// ```\n    pub fn len(&self) -> usize {\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::SeqCst);\n        b.wrapping_sub(f).max(0) as usize\n    }\n\n    /// Pushes a task into the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::new_lifo();\n    /// w.push(1);\n    /// w.push(2);\n    /// ```\n    pub fn push(&self, task: T) {\n        // Load the back index, front index, and buffer.\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::Acquire);\n        let mut buffer = self.buffer.get();\n\n        // Calculate the length of the queue.\n        let len = b.wrapping_sub(f);\n\n        // Is the queue full?\n        if len >= buffer.cap as isize {\n            // Yes. Grow the underlying buffer.\n            unsafe {\n                self.resize(2 * buffer.cap);\n            }\n            buffer = self.buffer.get();\n        }\n\n        // Write `task` into the slot.\n        unsafe {\n            buffer.write(b, task);\n        }\n\n        atomic::fence(Ordering::Release);\n\n        // Increment the back index.\n        //\n        // This ordering could be `Relaxed`, but then thread sanitizer would falsely report data\n        // races because it doesn't understand fences.\n        self.inner.back.store(b.wrapping_add(1), Ordering::Release);\n    }\n\n    /// Pops a task from the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_deque::Worker;\n    ///\n    /// let w = Worker::new_fifo();\n    /// w.push(1);\n    /// w.push(2);\n    ///\n    /// assert_eq!(w.pop(), Some(1));\n    /// assert_eq!(w.pop(), Some(2));\n    /// assert_eq!(w.pop(), None);\n    /// ```\n    pub fn pop(&self) -> Option<T> {\n        // Load the back and front index.\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::Relaxed);\n\n        // Calculate the length of the queue.\n        let len = b.wrapping_sub(f);\n\n        // Is the queue empty?\n        if len <= 0 {\n            return None;\n        }\n\n        match self.flavor {\n            // Pop from the front of the queue.\n            Flavor::Fifo => {\n                // Try incrementing the front index to pop the task.\n                let f = self.inner.front.fetch_add(1, Ordering::SeqCst);\n                let new_f = f.wrapping_add(1);\n\n                if b.wrapping_sub(new_f) < 0 {\n                    self.inner.front.store(f, Ordering::Relaxed);\n                    return None;\n                }\n\n                unsafe {\n                    // Read the popped task.\n                    let buffer = self.buffer.get();\n                    let task = buffer.read(f);\n\n                    // Shrink the buffer if `len - 1` is less than one fourth of the capacity.\n                    if buffer.cap > MIN_CAP && len <= buffer.cap as isize / 4 {\n                        self.resize(buffer.cap / 2);\n                    }\n\n                    Some(task)\n                }\n            }\n\n            // Pop from the back of the queue.\n            Flavor::Lifo => {\n                // Decrement the back index.\n                let b = b.wrapping_sub(1);\n                self.inner.back.store(b, Ordering::Relaxed);\n\n                atomic::fence(Ordering::SeqCst);\n\n                // Load the front index.\n                let f = self.inner.front.load(Ordering::Relaxed);\n\n                // Compute the length after the back index was decremented.\n                let len = b.wrapping_sub(f);\n\n                if len < 0 {\n                    // The queue is empty. Restore the back index to the original task.\n                    self.inner.back.store(b.wrapping_add(1), Ordering::Relaxed);\n                    None\n                } else {\n                    // Read the task to be popped.\n                    let buffer = self.buffer.get();\n                    let mut task = unsafe { Some(buffer.read(b)) };\n\n                    // Are we popping the last task from the queue?\n                    if len == 0 {\n                        // Try incrementing the front index.\n                        if self\n                            .inner\n                            .front\n                            .compare_exchange(\n                                f,\n                                f.wrapping_add(1),\n                                Ordering::SeqCst,\n                                Ordering::Relaxed,\n                            )\n                            .is_err()\n                        {\n                            // Failed. We didn't pop anything.\n                            mem::forget(task.take());\n                        }\n\n                        // Restore the back index to the original task.\n                        self.inner.back.store(b.wrapping_add(1), Ordering::Relaxed);\n                    } else {\n                        // Shrink the buffer if `len` is less than one fourth of the capacity.\n                        if buffer.cap > MIN_CAP && len < buffer.cap as isize / 4 {\n                            unsafe {\n                                self.resize(buffer.cap / 2);\n                            }\n                        }\n                    }\n\n                    task\n                }\n            }\n        }\n    }\n}","impl<T> fmt::Debug for Worker<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Worker { .. }\")\n    }\n}","unsafe impl<T: Send> Send for Worker<T> {}"]},"single_path_import":{"deque::Injector":"Injector","deque::Steal":"Steal","deque::Stealer":"Stealer","deque::Worker":"Worker"},"srcs":{"<deque::Buffer<T> as std::clone::Clone>::clone":["fn clone(&self) -> Buffer<T>{\n        Buffer {\n            ptr: self.ptr,\n            cap: self.cap,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Injector<T> as std::default::Default>::default":["fn default() -> Self{\n        let block = Box::into_raw(Box::new(Block::<T>::new()));\n        Self {\n            head: CachePadded::new(Position {\n                block: AtomicPtr::new(block),\n                index: AtomicUsize::new(0),\n            }),\n            tail: CachePadded::new(Position {\n                block: AtomicPtr::new(block),\n                index: AtomicUsize::new(0),\n            }),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Injector<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Worker { .. }\")\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Injector<T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        let mut head = self.head.index.load(Ordering::Relaxed);\n        let mut tail = self.tail.index.load(Ordering::Relaxed);\n        let mut block = self.head.block.load(Ordering::Relaxed);\n\n        // Erase the lower bits.\n        head &= !((1 << SHIFT) - 1);\n        tail &= !((1 << SHIFT) - 1);\n\n        unsafe {\n            // Drop all values between `head` and `tail` and deallocate the heap-allocated blocks.\n            while head != tail {\n                let offset = (head >> SHIFT) % LAP;\n\n                if offset < BLOCK_CAP {\n                    // Drop the task in the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    let p = &mut *slot.task.get();\n                    p.as_mut_ptr().drop_in_place();\n                } else {\n                    // Deallocate the block and move to the next one.\n                    let next = (*block).next.load(Ordering::Relaxed);\n                    drop(Box::from_raw(block));\n                    block = next;\n                }\n\n                head = head.wrapping_add(1 << SHIFT);\n            }\n\n            // Deallocate the last remaining block.\n            drop(Box::from_raw(block));\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Inner<T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        // Load the back index, front index, and buffer.\n        let b = self.back.load(Ordering::Relaxed);\n        let f = self.front.load(Ordering::Relaxed);\n\n        unsafe {\n            let buffer = self.buffer.load(Ordering::Relaxed, epoch::unprotected());\n\n            // Go through the buffer from front to back and drop all tasks in the queue.\n            let mut i = f;\n            while i != b {\n                buffer.deref().at(i).drop_in_place();\n                i = i.wrapping_add(1);\n            }\n\n            // Free the memory allocated by the buffer.\n            buffer.into_owned().into_box().dealloc();\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Steal<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        match self {\n            Steal::Empty => f.pad(\"Empty\"),\n            Steal::Success(_) => f.pad(\"Success(..)\"),\n            Steal::Retry => f.pad(\"Retry\"),\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Steal<T> as std::iter::FromIterator<deque::Steal<T>>>::from_iter":["/// Consumes items until a `Success` is found and returns it.\n///\n/// If no `Success` was found, but there was at least one `Retry`, then returns `Retry`.\n/// Otherwise, `Empty` is returned.\nfn from_iter<I>(iter: I) -> Steal<T>\n    where\n        I: IntoIterator<Item = Steal<T>>,{\n        let mut retry = false;\n        for s in iter {\n            match &s {\n                Steal::Empty => {}\n                Steal::Success(_) => return s,\n                Steal::Retry => retry = true,\n            }\n        }\n\n        if retry {\n            Steal::Retry\n        } else {\n            Steal::Empty\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Stealer<T> as std::clone::Clone>::clone":["fn clone(&self) -> Stealer<T>{\n        Stealer {\n            inner: self.inner.clone(),\n            flavor: self.flavor,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Stealer<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Stealer { .. }\")\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"<deque::Worker<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Worker { .. }\")\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Block":["/// A block in a linked list.\n///\n/// Each block in the list can hold up to `BLOCK_CAP` values.\nstruct Block<T> {\n    /// The next block in the linked list.\n    next: AtomicPtr<Block<T>>,\n\n    /// Slots for values.\n    slots: [Slot<T>; BLOCK_CAP],\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Block::<T>::destroy":["/// Sets the `DESTROY` bit in slots starting from `start` and destroys the block.\nunsafe fn destroy(this: *mut Block<T>, count: usize){\n        // It is not necessary to set the `DESTROY` bit in the last slot because that slot has\n        // begun destruction of the block.\n        for i in (0..count).rev() {\n            let slot = (*this).slots.get_unchecked(i);\n\n            // Mark the `DESTROY` bit if a thread is still using the slot.\n            if slot.state.load(Ordering::Acquire) & READ == 0\n                && slot.state.fetch_or(DESTROY, Ordering::AcqRel) & READ == 0\n            {\n                // If a thread is still using the slot, it will continue destruction of the block.\n                return;\n            }\n        }\n\n        // No thread is using the block, now it is safe to destroy it.\n        drop(Box::from_raw(this));\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Block::<T>::new":["/// Creates an empty block that starts at `start_index`.\nfn new() -> Block<T>{\n        // SAFETY: This is safe because:\n        //  [1] `Block::next` (AtomicPtr) may be safely zero initialized.\n        //  [2] `Block::slots` (Array) may be safely zero initialized because of [3, 4].\n        //  [3] `Slot::task` (UnsafeCell) may be safely zero initialized because it\n        //       holds a MaybeUninit.\n        //  [4] `Slot::state` (AtomicUsize) may be safely zero initialized.\n        unsafe { MaybeUninit::zeroed().assume_init() }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Block::<T>::wait_next":["/// Waits until the next pointer is set.\nfn wait_next(&self) -> *mut Block<T>{\n        let backoff = Backoff::new();\n        loop {\n            let next = self.next.load(Ordering::Acquire);\n            if !next.is_null() {\n                return next;\n            }\n            backoff.snooze();\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Buffer":["/// A buffer that holds tasks in a worker queue.\n///\n/// This is just a pointer to the buffer and its length - dropping an instance of this struct will\n/// *not* deallocate the buffer.\nstruct Buffer<T> {\n    /// Pointer to the allocated memory.\n    ptr: *mut T,\n\n    /// Capacity of the buffer. Always a power of two.\n    cap: usize,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Buffer::<T>::alloc":["/// Allocates a new buffer with the specified capacity.\nfn alloc(cap: usize) -> Buffer<T>{\n        debug_assert_eq!(cap, cap.next_power_of_two());\n\n        let mut v = Vec::with_capacity(cap);\n        let ptr = v.as_mut_ptr();\n        mem::forget(v);\n\n        Buffer { ptr, cap }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Buffer::<T>::at":["/// Returns a pointer to the task at the specified `index`.\nunsafe fn at(&self, index: isize) -> *mut T{\n        // `self.cap` is always a power of two.\n        self.ptr.offset(index & (self.cap - 1) as isize)\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Buffer::<T>::dealloc":["/// Deallocates the buffer.\nunsafe fn dealloc(self){\n        drop(Vec::from_raw_parts(self.ptr, 0, self.cap));\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Buffer::<T>::read":["/// Reads a task from the specified `index`.\n///\n/// This method might be concurrently called with another `write` at the same index, which is\n/// technically speaking a data race and therefore UB. We should use an atomic load here, but\n/// that would be more expensive and difficult to implement generically for all types `T`.\n/// Hence, as a hack, we use a volatile write instead.\nunsafe fn read(&self, index: isize) -> T{\n        ptr::read_volatile(self.at(index))\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Buffer::<T>::write":["/// Writes `task` into the specified `index`.\n///\n/// This method might be concurrently called with another `read` at the same index, which is\n/// technically speaking a data race and therefore UB. We should use an atomic store here, but\n/// that would be more expensive and difficult to implement generically for all types `T`.\n/// Hence, as a hack, we use a volatile write instead.\nunsafe fn write(&self, index: isize, task: T){\n        ptr::write_volatile(self.at(index), task)\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Flavor":["/// Worker queue flavor: FIFO or LIFO.\nenum Flavor {\n    /// The first-in first-out flavor.\n    Fifo,\n\n    /// The last-in first-out flavor.\n    Lifo,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector":["/// An injector queue.\n///\n/// This is a FIFO queue that can be shared among multiple threads. Task schedulers typically have\n/// a single injector queue, which is the entry point for new tasks.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Injector, Steal};\n///\n/// let q = Injector::new();\n/// q.push(1);\n/// q.push(2);\n///\n/// assert_eq!(q.steal(), Steal::Success(1));\n/// assert_eq!(q.steal(), Steal::Success(2));\n/// assert_eq!(q.steal(), Steal::Empty);\n/// ```\npub struct Injector<T> {\n    /// The head of the queue.\n    head: CachePadded<Position<T>>,\n\n    /// The tail of the queue.\n    tail: CachePadded<Position<T>>,\n\n    /// Indicates that dropping a `Injector<T>` may drop values of type `T`.\n    _marker: PhantomData<T>,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::is_empty":["/// Returns `true` if the queue is empty.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Injector;\n///\n/// let q = Injector::new();\n///\n/// assert!(q.is_empty());\n/// q.push(1);\n/// assert!(!q.is_empty());\n/// ```\npub fn is_empty(&self) -> bool{\n        let head = self.head.index.load(Ordering::SeqCst);\n        let tail = self.tail.index.load(Ordering::SeqCst);\n        head >> SHIFT == tail >> SHIFT\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::len":["/// Returns the number of tasks in the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Injector;\n///\n/// let q = Injector::new();\n///\n/// assert_eq!(q.len(), 0);\n/// q.push(1);\n/// assert_eq!(q.len(), 1);\n/// q.push(1);\n/// assert_eq!(q.len(), 2);\n/// ```\npub fn len(&self) -> usize{\n        loop {\n            // Load the tail index, then load the head index.\n            let mut tail = self.tail.index.load(Ordering::SeqCst);\n            let mut head = self.head.index.load(Ordering::SeqCst);\n\n            // If the tail index didn't change, we've got consistent indices to work with.\n            if self.tail.index.load(Ordering::SeqCst) == tail {\n                // Erase the lower bits.\n                tail &= !((1 << SHIFT) - 1);\n                head &= !((1 << SHIFT) - 1);\n\n                // Fix up indices if they fall onto block ends.\n                if (tail >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    tail = tail.wrapping_add(1 << SHIFT);\n                }\n                if (head >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    head = head.wrapping_add(1 << SHIFT);\n                }\n\n                // Rotate indices so that head falls into the first block.\n                let lap = (head >> SHIFT) / LAP;\n                tail = tail.wrapping_sub((lap * LAP) << SHIFT);\n                head = head.wrapping_sub((lap * LAP) << SHIFT);\n\n                // Remove the lower bits.\n                tail >>= SHIFT;\n                head >>= SHIFT;\n\n                // Return the difference minus the number of blocks between tail and head.\n                return tail - head - tail / LAP;\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::new":["/// Creates a new injector queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Injector;\n///\n/// let q = Injector::<i32>::new();\n/// ```\npub fn new() -> Injector<T>{\n        Self::default()\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::push":["/// Pushes a task into the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Injector;\n///\n/// let w = Injector::new();\n/// w.push(1);\n/// w.push(2);\n/// ```\npub fn push(&self, task: T){\n        let backoff = Backoff::new();\n        let mut tail = self.tail.index.load(Ordering::Acquire);\n        let mut block = self.tail.block.load(Ordering::Acquire);\n        let mut next_block = None;\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (tail >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n                tail = self.tail.index.load(Ordering::Acquire);\n                block = self.tail.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // If we're going to have to install the next block, allocate it in advance in order to\n            // make the wait for other threads as short as possible.\n            if offset + 1 == BLOCK_CAP && next_block.is_none() {\n                next_block = Some(Box::new(Block::<T>::new()));\n            }\n\n            let new_tail = tail + (1 << SHIFT);\n\n            // Try advancing the tail forward.\n            match self.tail.index.compare_exchange_weak(\n                tail,\n                new_tail,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, install the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next_block = Box::into_raw(next_block.unwrap());\n                        let next_index = new_tail.wrapping_add(1 << SHIFT);\n\n                        self.tail.block.store(next_block, Ordering::Release);\n                        self.tail.index.store(next_index, Ordering::Release);\n                        (*block).next.store(next_block, Ordering::Release);\n                    }\n\n                    // Write the task into the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.task.get().write(MaybeUninit::new(task));\n                    slot.state.fetch_or(WRITE, Ordering::Release);\n\n                    return;\n                },\n                Err(t) => {\n                    tail = t;\n                    block = self.tail.block.load(Ordering::Acquire);\n                    backoff.spin();\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::steal":["/// Steals a task from the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Injector, Steal};\n///\n/// let q = Injector::new();\n/// q.push(1);\n/// q.push(2);\n///\n/// assert_eq!(q.steal(), Steal::Success(1));\n/// assert_eq!(q.steal(), Steal::Success(2));\n/// assert_eq!(q.steal(), Steal::Empty);\n/// ```\npub fn steal(&self) -> Steal<T>{\n        let mut head;\n        let mut block;\n        let mut offset;\n\n        let backoff = Backoff::new();\n        loop {\n            head = self.head.index.load(Ordering::Acquire);\n            block = self.head.block.load(Ordering::Acquire);\n\n            // Calculate the offset of the index into the block.\n            offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n            } else {\n                break;\n            }\n        }\n\n        let mut new_head = head + (1 << SHIFT);\n\n        if new_head & HAS_NEXT == 0 {\n            atomic::fence(Ordering::SeqCst);\n            let tail = self.tail.index.load(Ordering::Relaxed);\n\n            // If the tail equals the head, that means the queue is empty.\n            if head >> SHIFT == tail >> SHIFT {\n                return Steal::Empty;\n            }\n\n            // If head and tail are not in the same block, set `HAS_NEXT` in head.\n            if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                new_head |= HAS_NEXT;\n            }\n        }\n\n        // Try moving the head index forward.\n        if self\n            .head\n            .index\n            .compare_exchange_weak(head, new_head, Ordering::SeqCst, Ordering::Acquire)\n            .is_err()\n        {\n            return Steal::Retry;\n        }\n\n        unsafe {\n            // If we've reached the end of the block, move to the next one.\n            if offset + 1 == BLOCK_CAP {\n                let next = (*block).wait_next();\n                let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                if !(*next).next.load(Ordering::Relaxed).is_null() {\n                    next_index |= HAS_NEXT;\n                }\n\n                self.head.block.store(next, Ordering::Release);\n                self.head.index.store(next_index, Ordering::Release);\n            }\n\n            // Read the task.\n            let slot = (*block).slots.get_unchecked(offset);\n            slot.wait_write();\n            let task = slot.task.get().read().assume_init();\n\n            // Destroy the block if we've reached the end, or if another thread wanted to destroy\n            // but couldn't because we were busy reading from the slot.\n            if (offset + 1 == BLOCK_CAP) || (slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0) {\n                Block::destroy(block, offset);\n            }\n\n            Steal::Success(task)\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::steal_batch":["/// Steals a batch of tasks and pushes them into a worker.\n///\n/// How many tasks exactly will be stolen is not specified. That said, this method will try to\n/// steal around half of the tasks in the queue, but also not more than some constant limit.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Injector, Worker};\n///\n/// let q = Injector::new();\n/// q.push(1);\n/// q.push(2);\n/// q.push(3);\n/// q.push(4);\n///\n/// let w = Worker::new_fifo();\n/// let _ = q.steal_batch(&w);\n/// assert_eq!(w.pop(), Some(1));\n/// assert_eq!(w.pop(), Some(2));\n/// ```\npub fn steal_batch(&self, dest: &Worker<T>) -> Steal<()>{\n        let mut head;\n        let mut block;\n        let mut offset;\n\n        let backoff = Backoff::new();\n        loop {\n            head = self.head.index.load(Ordering::Acquire);\n            block = self.head.block.load(Ordering::Acquire);\n\n            // Calculate the offset of the index into the block.\n            offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n            } else {\n                break;\n            }\n        }\n\n        let mut new_head = head;\n        let advance;\n\n        if new_head & HAS_NEXT == 0 {\n            atomic::fence(Ordering::SeqCst);\n            let tail = self.tail.index.load(Ordering::Relaxed);\n\n            // If the tail equals the head, that means the queue is empty.\n            if head >> SHIFT == tail >> SHIFT {\n                return Steal::Empty;\n            }\n\n            // If head and tail are not in the same block, set `HAS_NEXT` in head. Also, calculate\n            // the right batch size to steal.\n            if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                new_head |= HAS_NEXT;\n                // We can steal all tasks till the end of the block.\n                advance = (BLOCK_CAP - offset).min(MAX_BATCH);\n            } else {\n                let len = (tail - head) >> SHIFT;\n                // Steal half of the available tasks.\n                advance = ((len + 1) / 2).min(MAX_BATCH);\n            }\n        } else {\n            // We can steal all tasks till the end of the block.\n            advance = (BLOCK_CAP - offset).min(MAX_BATCH);\n        }\n\n        new_head += advance << SHIFT;\n        let new_offset = offset + advance;\n\n        // Try moving the head index forward.\n        if self\n            .head\n            .index\n            .compare_exchange_weak(head, new_head, Ordering::SeqCst, Ordering::Acquire)\n            .is_err()\n        {\n            return Steal::Retry;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = new_offset - offset;\n        dest.reserve(batch_size);\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        unsafe {\n            // If we've reached the end of the block, move to the next one.\n            if new_offset == BLOCK_CAP {\n                let next = (*block).wait_next();\n                let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                if !(*next).next.load(Ordering::Relaxed).is_null() {\n                    next_index |= HAS_NEXT;\n                }\n\n                self.head.block.store(next, Ordering::Release);\n                self.head.index.store(next_index, Ordering::Release);\n            }\n\n            // Copy values from the injector into the destination queue.\n            match dest.flavor {\n                Flavor::Fifo => {\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add(i as isize), task);\n                    }\n                }\n\n                Flavor::Lifo => {\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add((batch_size - 1 - i) as isize), task);\n                    }\n                }\n            }\n\n            atomic::fence(Ordering::Release);\n\n            // Update the back index in the destination queue.\n            //\n            // This ordering could be `Relaxed`, but then thread sanitizer would falsely report\n            // data races because it doesn't understand fences.\n            dest.inner\n                .back\n                .store(dest_b.wrapping_add(batch_size as isize), Ordering::Release);\n\n            // Destroy the block if we've reached the end, or if another thread wanted to destroy\n            // but couldn't because we were busy reading from the slot.\n            if new_offset == BLOCK_CAP {\n                Block::destroy(block, offset);\n            } else {\n                for i in offset..new_offset {\n                    let slot = (*block).slots.get_unchecked(i);\n\n                    if slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0 {\n                        Block::destroy(block, offset);\n                        break;\n                    }\n                }\n            }\n\n            Steal::Success(())\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Injector::<T>::steal_batch_and_pop":["/// Steals a batch of tasks, pushes them into a worker, and pops a task from that worker.\n///\n/// How many tasks exactly will be stolen is not specified. That said, this method will try to\n/// steal around half of the tasks in the queue, but also not more than some constant limit.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Injector, Steal, Worker};\n///\n/// let q = Injector::new();\n/// q.push(1);\n/// q.push(2);\n/// q.push(3);\n/// q.push(4);\n///\n/// let w = Worker::new_fifo();\n/// assert_eq!(q.steal_batch_and_pop(&w), Steal::Success(1));\n/// assert_eq!(w.pop(), Some(2));\n/// ```\npub fn steal_batch_and_pop(&self, dest: &Worker<T>) -> Steal<T>{\n        let mut head;\n        let mut block;\n        let mut offset;\n\n        let backoff = Backoff::new();\n        loop {\n            head = self.head.index.load(Ordering::Acquire);\n            block = self.head.block.load(Ordering::Acquire);\n\n            // Calculate the offset of the index into the block.\n            offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n            } else {\n                break;\n            }\n        }\n\n        let mut new_head = head;\n        let advance;\n\n        if new_head & HAS_NEXT == 0 {\n            atomic::fence(Ordering::SeqCst);\n            let tail = self.tail.index.load(Ordering::Relaxed);\n\n            // If the tail equals the head, that means the queue is empty.\n            if head >> SHIFT == tail >> SHIFT {\n                return Steal::Empty;\n            }\n\n            // If head and tail are not in the same block, set `HAS_NEXT` in head.\n            if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                new_head |= HAS_NEXT;\n                // We can steal all tasks till the end of the block.\n                advance = (BLOCK_CAP - offset).min(MAX_BATCH + 1);\n            } else {\n                let len = (tail - head) >> SHIFT;\n                // Steal half of the available tasks.\n                advance = ((len + 1) / 2).min(MAX_BATCH + 1);\n            }\n        } else {\n            // We can steal all tasks till the end of the block.\n            advance = (BLOCK_CAP - offset).min(MAX_BATCH + 1);\n        }\n\n        new_head += advance << SHIFT;\n        let new_offset = offset + advance;\n\n        // Try moving the head index forward.\n        if self\n            .head\n            .index\n            .compare_exchange_weak(head, new_head, Ordering::SeqCst, Ordering::Acquire)\n            .is_err()\n        {\n            return Steal::Retry;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = new_offset - offset - 1;\n        dest.reserve(batch_size);\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        unsafe {\n            // If we've reached the end of the block, move to the next one.\n            if new_offset == BLOCK_CAP {\n                let next = (*block).wait_next();\n                let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                if !(*next).next.load(Ordering::Relaxed).is_null() {\n                    next_index |= HAS_NEXT;\n                }\n\n                self.head.block.store(next, Ordering::Release);\n                self.head.index.store(next_index, Ordering::Release);\n            }\n\n            // Read the task.\n            let slot = (*block).slots.get_unchecked(offset);\n            slot.wait_write();\n            let task = slot.task.get().read().assume_init();\n\n            match dest.flavor {\n                Flavor::Fifo => {\n                    // Copy values from the injector into the destination queue.\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i + 1);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add(i as isize), task);\n                    }\n                }\n\n                Flavor::Lifo => {\n                    // Copy values from the injector into the destination queue.\n                    for i in 0..batch_size {\n                        // Read the task.\n                        let slot = (*block).slots.get_unchecked(offset + i + 1);\n                        slot.wait_write();\n                        let task = slot.task.get().read().assume_init();\n\n                        // Write it into the destination queue.\n                        dest_buffer.write(dest_b.wrapping_add((batch_size - 1 - i) as isize), task);\n                    }\n                }\n            }\n\n            atomic::fence(Ordering::Release);\n\n            // Update the back index in the destination queue.\n            //\n            // This ordering could be `Relaxed`, but then thread sanitizer would falsely report\n            // data races because it doesn't understand fences.\n            dest.inner\n                .back\n                .store(dest_b.wrapping_add(batch_size as isize), Ordering::Release);\n\n            // Destroy the block if we've reached the end, or if another thread wanted to destroy\n            // but couldn't because we were busy reading from the slot.\n            if new_offset == BLOCK_CAP {\n                Block::destroy(block, offset);\n            } else {\n                for i in offset..new_offset {\n                    let slot = (*block).slots.get_unchecked(i);\n\n                    if slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0 {\n                        Block::destroy(block, offset);\n                        break;\n                    }\n                }\n            }\n\n            Steal::Success(task)\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Inner":["/// Internal queue data shared between the worker and stealers.\n///\n/// The implementation is based on the following work:\n///\n/// 1. [Chase and Lev. Dynamic circular work-stealing deque. SPAA 2005.][chase-lev]\n/// 2. [Le, Pop, Cohen, and Nardelli. Correct and efficient work-stealing for weak memory models.\n///    PPoPP 2013.][weak-mem]\n/// 3. [Norris and Demsky. CDSchecker: checking concurrent data structures written with C/C++\n///    atomics. OOPSLA 2013.][checker]\n///\n/// [chase-lev]: https://dl.acm.org/citation.cfm?id=1073974\n/// [weak-mem]: https://dl.acm.org/citation.cfm?id=2442524\n/// [checker]: https://dl.acm.org/citation.cfm?id=2509514\nstruct Inner<T> {\n    /// The front index.\n    front: AtomicIsize,\n\n    /// The back index.\n    back: AtomicIsize,\n\n    /// The underlying buffer.\n    buffer: CachePadded<Atomic<Buffer<T>>>,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Position":["/// A position in a queue.\nstruct Position<T> {\n    /// The index in the queue.\n    index: AtomicUsize,\n\n    /// The block in the linked list.\n    block: AtomicPtr<Block<T>>,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Slot":["/// A slot in a block.\nstruct Slot<T> {\n    /// The task.\n    task: UnsafeCell<MaybeUninit<T>>,\n\n    /// The state of the slot.\n    state: AtomicUsize,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Slot::<T>::wait_write":["/// Waits until a task is written into the slot.\nfn wait_write(&self){\n        let backoff = Backoff::new();\n        while self.state.load(Ordering::Acquire) & WRITE == 0 {\n            backoff.snooze();\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Steal":["/// Possible outcomes of a steal operation.\n///\n/// # Examples\n///\n/// There are lots of ways to chain results of steal operations together:\n///\n/// ```\n/// use crossbeam_deque::Steal::{self, Empty, Retry, Success};\n///\n/// let collect = |v: Vec<Steal<i32>>| v.into_iter().collect::<Steal<i32>>();\n///\n/// assert_eq!(collect(vec![Empty, Empty, Empty]), Empty);\n/// assert_eq!(collect(vec![Empty, Retry, Empty]), Retry);\n/// assert_eq!(collect(vec![Retry, Success(1), Empty]), Success(1));\n///\n/// assert_eq!(collect(vec![Empty, Empty]).or_else(|| Retry), Retry);\n/// assert_eq!(collect(vec![Retry, Empty]).or_else(|| Success(1)), Success(1));\n/// ```\n#[must_use]\npub enum Steal<T> {\n    /// The queue was empty at the time of stealing.\n    Empty,\n\n    /// At least one task was successfully stolen.\n    Success(T),\n\n    /// The steal operation needs to be retried.\n    Retry,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Steal::<T>::is_empty":["/// Returns `true` if the queue was empty at the time of stealing.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Steal::{Empty, Retry, Success};\n///\n/// assert!(!Success(7).is_empty());\n/// assert!(!Retry::<i32>.is_empty());\n///\n/// assert!(Empty::<i32>.is_empty());\n/// ```\npub fn is_empty(&self) -> bool{\n        match self {\n            Steal::Empty => true,\n            _ => false,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Steal::<T>::is_retry":["/// Returns `true` if the steal operation needs to be retried.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Steal::{Empty, Retry, Success};\n///\n/// assert!(!Empty::<i32>.is_retry());\n/// assert!(!Success(7).is_retry());\n///\n/// assert!(Retry::<i32>.is_retry());\n/// ```\npub fn is_retry(&self) -> bool{\n        match self {\n            Steal::Retry => true,\n            _ => false,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Steal::<T>::is_success":["/// Returns `true` if at least one task was stolen.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Steal::{Empty, Retry, Success};\n///\n/// assert!(!Empty::<i32>.is_success());\n/// assert!(!Retry::<i32>.is_success());\n///\n/// assert!(Success(7).is_success());\n/// ```\npub fn is_success(&self) -> bool{\n        match self {\n            Steal::Success(_) => true,\n            _ => false,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Steal::<T>::or_else":["/// If no task was stolen, attempts another steal operation.\n///\n/// Returns this steal result if it is `Success`. Otherwise, closure `f` is invoked and then:\n///\n/// * If the second steal resulted in `Success`, it is returned.\n/// * If both steals were unsuccessful but any resulted in `Retry`, then `Retry` is returned.\n/// * If both resulted in `None`, then `None` is returned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Steal::{Empty, Retry, Success};\n///\n/// assert_eq!(Success(1).or_else(|| Success(2)), Success(1));\n/// assert_eq!(Retry.or_else(|| Success(2)), Success(2));\n///\n/// assert_eq!(Retry.or_else(|| Empty), Retry::<i32>);\n/// assert_eq!(Empty.or_else(|| Retry), Retry::<i32>);\n///\n/// assert_eq!(Empty.or_else(|| Empty), Empty::<i32>);\n/// ```\npub fn or_else<F>(self, f: F) -> Steal<T>\n    where\n        F: FnOnce() -> Steal<T>,{\n        match self {\n            Steal::Empty => f(),\n            Steal::Success(_) => self,\n            Steal::Retry => {\n                if let Steal::Success(res) = f() {\n                    Steal::Success(res)\n                } else {\n                    Steal::Retry\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Steal::<T>::success":["/// Returns the result of the operation, if successful.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Steal::{Empty, Retry, Success};\n///\n/// assert_eq!(Empty::<i32>.success(), None);\n/// assert_eq!(Retry::<i32>.success(), None);\n///\n/// assert_eq!(Success(7).success(), Some(7));\n/// ```\npub fn success(self) -> Option<T>{\n        match self {\n            Steal::Success(res) => Some(res),\n            _ => None,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Stealer":["/// A stealer handle of a worker queue.\n///\n/// Stealers can be shared among threads.\n///\n/// Task schedulers typically have a single worker queue per worker thread.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Steal, Worker};\n///\n/// let w = Worker::new_lifo();\n/// w.push(1);\n/// w.push(2);\n///\n/// let s = w.stealer();\n/// assert_eq!(s.steal(), Steal::Success(1));\n/// assert_eq!(s.steal(), Steal::Success(2));\n/// assert_eq!(s.steal(), Steal::Empty);\n/// ```\npub struct Stealer<T> {\n    /// A reference to the inner representation of the queue.\n    inner: Arc<CachePadded<Inner<T>>>,\n\n    /// The flavor of the queue.\n    flavor: Flavor,\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Stealer::<T>::is_empty":["/// Returns `true` if the queue is empty.\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::new_lifo();\n/// let s = w.stealer();\n///\n/// assert!(s.is_empty());\n/// w.push(1);\n/// assert!(!s.is_empty());\n/// ```\npub fn is_empty(&self) -> bool{\n        let f = self.inner.front.load(Ordering::Acquire);\n        atomic::fence(Ordering::SeqCst);\n        let b = self.inner.back.load(Ordering::Acquire);\n        b.wrapping_sub(f) <= 0\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Stealer::<T>::steal":["/// Steals a task from the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Steal, Worker};\n///\n/// let w = Worker::new_lifo();\n/// w.push(1);\n/// w.push(2);\n///\n/// let s = w.stealer();\n/// assert_eq!(s.steal(), Steal::Success(1));\n/// assert_eq!(s.steal(), Steal::Success(2));\n/// ```\npub fn steal(&self) -> Steal<T>{\n        // Load the front index.\n        let f = self.inner.front.load(Ordering::Acquire);\n\n        // A SeqCst fence is needed here.\n        //\n        // If the current thread is already pinned (reentrantly), we must manually issue the\n        // fence. Otherwise, the following pinning will issue the fence anyway, so we don't\n        // have to.\n        if epoch::is_pinned() {\n            atomic::fence(Ordering::SeqCst);\n        }\n\n        let guard = &epoch::pin();\n\n        // Load the back index.\n        let b = self.inner.back.load(Ordering::Acquire);\n\n        // Is the queue empty?\n        if b.wrapping_sub(f) <= 0 {\n            return Steal::Empty;\n        }\n\n        // Load the buffer and read the task at the front.\n        let buffer = self.inner.buffer.load(Ordering::Acquire, guard);\n        let task = unsafe { buffer.deref().read(f) };\n\n        // Try incrementing the front index to steal the task.\n        if self\n            .inner\n            .front\n            .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n            .is_err()\n        {\n            // We didn't steal this task, forget it.\n            mem::forget(task);\n            return Steal::Retry;\n        }\n\n        // Return the stolen task.\n        Steal::Success(task)\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Stealer::<T>::steal_batch":["/// Steals a batch of tasks and pushes them into another worker.\n///\n/// How many tasks exactly will be stolen is not specified. That said, this method will try to\n/// steal around half of the tasks in the queue, but also not more than some constant limit.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w1 = Worker::new_fifo();\n/// w1.push(1);\n/// w1.push(2);\n/// w1.push(3);\n/// w1.push(4);\n///\n/// let s = w1.stealer();\n/// let w2 = Worker::new_fifo();\n///\n/// let _ = s.steal_batch(&w2);\n/// assert_eq!(w2.pop(), Some(1));\n/// assert_eq!(w2.pop(), Some(2));\n/// ```\npub fn steal_batch(&self, dest: &Worker<T>) -> Steal<()>{\n        if Arc::ptr_eq(&self.inner, &dest.inner) {\n            if dest.is_empty() {\n                return Steal::Empty;\n            } else {\n                return Steal::Success(());\n            }\n        }\n\n        // Load the front index.\n        let mut f = self.inner.front.load(Ordering::Acquire);\n\n        // A SeqCst fence is needed here.\n        //\n        // If the current thread is already pinned (reentrantly), we must manually issue the\n        // fence. Otherwise, the following pinning will issue the fence anyway, so we don't\n        // have to.\n        if epoch::is_pinned() {\n            atomic::fence(Ordering::SeqCst);\n        }\n\n        let guard = &epoch::pin();\n\n        // Load the back index.\n        let b = self.inner.back.load(Ordering::Acquire);\n\n        // Is the queue empty?\n        let len = b.wrapping_sub(f);\n        if len <= 0 {\n            return Steal::Empty;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = cmp::min((len as usize + 1) / 2, MAX_BATCH);\n        dest.reserve(batch_size);\n        let mut batch_size = batch_size as isize;\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let mut dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        // Load the buffer.\n        let buffer = self.inner.buffer.load(Ordering::Acquire, guard);\n\n        match self.flavor {\n            // Steal a batch of tasks from the front at once.\n            Flavor::Fifo => {\n                // Copy the batch from the source to the destination buffer.\n                match dest.flavor {\n                    Flavor::Fifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i));\n                                dest_buffer.write(dest_b.wrapping_add(i), task);\n                            }\n                        }\n                    }\n                    Flavor::Lifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i));\n                                dest_buffer.write(dest_b.wrapping_add(batch_size - 1 - i), task);\n                            }\n                        }\n                    }\n                }\n\n                // Try incrementing the front index to steal the batch.\n                if self\n                    .inner\n                    .front\n                    .compare_exchange(\n                        f,\n                        f.wrapping_add(batch_size),\n                        Ordering::SeqCst,\n                        Ordering::Relaxed,\n                    )\n                    .is_err()\n                {\n                    return Steal::Retry;\n                }\n\n                dest_b = dest_b.wrapping_add(batch_size);\n            }\n\n            // Steal a batch of tasks from the front one by one.\n            Flavor::Lifo => {\n                for i in 0..batch_size {\n                    // If this is not the first steal, check whether the queue is empty.\n                    if i > 0 {\n                        // We've already got the current front index. Now execute the fence to\n                        // synchronize with other threads.\n                        atomic::fence(Ordering::SeqCst);\n\n                        // Load the back index.\n                        let b = self.inner.back.load(Ordering::Acquire);\n\n                        // Is the queue empty?\n                        if b.wrapping_sub(f) <= 0 {\n                            batch_size = i;\n                            break;\n                        }\n                    }\n\n                    // Read the task at the front.\n                    let task = unsafe { buffer.deref().read(f) };\n\n                    // Try incrementing the front index to steal the task.\n                    if self\n                        .inner\n                        .front\n                        .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n                        .is_err()\n                    {\n                        // We didn't steal this task, forget it and break from the loop.\n                        mem::forget(task);\n                        batch_size = i;\n                        break;\n                    }\n\n                    // Write the stolen task into the destination buffer.\n                    unsafe {\n                        dest_buffer.write(dest_b, task);\n                    }\n\n                    // Move the source front index and the destination back index one step forward.\n                    f = f.wrapping_add(1);\n                    dest_b = dest_b.wrapping_add(1);\n                }\n\n                // If we didn't steal anything, the operation needs to be retried.\n                if batch_size == 0 {\n                    return Steal::Retry;\n                }\n\n                // If stealing into a FIFO queue, stolen tasks need to be reversed.\n                if dest.flavor == Flavor::Fifo {\n                    for i in 0..batch_size / 2 {\n                        unsafe {\n                            let i1 = dest_b.wrapping_sub(batch_size - i);\n                            let i2 = dest_b.wrapping_sub(i + 1);\n                            let t1 = dest_buffer.read(i1);\n                            let t2 = dest_buffer.read(i2);\n                            dest_buffer.write(i1, t2);\n                            dest_buffer.write(i2, t1);\n                        }\n                    }\n                }\n            }\n        }\n\n        atomic::fence(Ordering::Release);\n\n        // Update the back index in the destination queue.\n        //\n        // This ordering could be `Relaxed`, but then thread sanitizer would falsely report data\n        // races because it doesn't understand fences.\n        dest.inner.back.store(dest_b, Ordering::Release);\n\n        // Return with success.\n        Steal::Success(())\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Stealer::<T>::steal_batch_and_pop":["/// Steals a batch of tasks, pushes them into another worker, and pops a task from that worker.\n///\n/// How many tasks exactly will be stolen is not specified. That said, this method will try to\n/// steal around half of the tasks in the queue, but also not more than some constant limit.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::{Steal, Worker};\n///\n/// let w1 = Worker::new_fifo();\n/// w1.push(1);\n/// w1.push(2);\n/// w1.push(3);\n/// w1.push(4);\n///\n/// let s = w1.stealer();\n/// let w2 = Worker::new_fifo();\n///\n/// assert_eq!(s.steal_batch_and_pop(&w2), Steal::Success(1));\n/// assert_eq!(w2.pop(), Some(2));\n/// ```\npub fn steal_batch_and_pop(&self, dest: &Worker<T>) -> Steal<T>{\n        if Arc::ptr_eq(&self.inner, &dest.inner) {\n            match dest.pop() {\n                None => return Steal::Empty,\n                Some(task) => return Steal::Success(task),\n            }\n        }\n\n        // Load the front index.\n        let mut f = self.inner.front.load(Ordering::Acquire);\n\n        // A SeqCst fence is needed here.\n        //\n        // If the current thread is already pinned (reentrantly), we must manually issue the\n        // fence. Otherwise, the following pinning will issue the fence anyway, so we don't\n        // have to.\n        if epoch::is_pinned() {\n            atomic::fence(Ordering::SeqCst);\n        }\n\n        let guard = &epoch::pin();\n\n        // Load the back index.\n        let b = self.inner.back.load(Ordering::Acquire);\n\n        // Is the queue empty?\n        let len = b.wrapping_sub(f);\n        if len <= 0 {\n            return Steal::Empty;\n        }\n\n        // Reserve capacity for the stolen batch.\n        let batch_size = cmp::min((len as usize - 1) / 2, MAX_BATCH - 1);\n        dest.reserve(batch_size);\n        let mut batch_size = batch_size as isize;\n\n        // Get the destination buffer and back index.\n        let dest_buffer = dest.buffer.get();\n        let mut dest_b = dest.inner.back.load(Ordering::Relaxed);\n\n        // Load the buffer\n        let buffer = self.inner.buffer.load(Ordering::Acquire, guard);\n\n        // Read the task at the front.\n        let mut task = unsafe { buffer.deref().read(f) };\n\n        match self.flavor {\n            // Steal a batch of tasks from the front at once.\n            Flavor::Fifo => {\n                // Copy the batch from the source to the destination buffer.\n                match dest.flavor {\n                    Flavor::Fifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i + 1));\n                                dest_buffer.write(dest_b.wrapping_add(i), task);\n                            }\n                        }\n                    }\n                    Flavor::Lifo => {\n                        for i in 0..batch_size {\n                            unsafe {\n                                let task = buffer.deref().read(f.wrapping_add(i + 1));\n                                dest_buffer.write(dest_b.wrapping_add(batch_size - 1 - i), task);\n                            }\n                        }\n                    }\n                }\n\n                // Try incrementing the front index to steal the batch.\n                if self\n                    .inner\n                    .front\n                    .compare_exchange(\n                        f,\n                        f.wrapping_add(batch_size + 1),\n                        Ordering::SeqCst,\n                        Ordering::Relaxed,\n                    )\n                    .is_err()\n                {\n                    // We didn't steal this task, forget it.\n                    mem::forget(task);\n                    return Steal::Retry;\n                }\n\n                dest_b = dest_b.wrapping_add(batch_size);\n            }\n\n            // Steal a batch of tasks from the front one by one.\n            Flavor::Lifo => {\n                // Try incrementing the front index to steal the task.\n                if self\n                    .inner\n                    .front\n                    .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n                    .is_err()\n                {\n                    // We didn't steal this task, forget it.\n                    mem::forget(task);\n                    return Steal::Retry;\n                }\n\n                // Move the front index one step forward.\n                f = f.wrapping_add(1);\n\n                // Repeat the same procedure for the batch steals.\n                for i in 0..batch_size {\n                    // We've already got the current front index. Now execute the fence to\n                    // synchronize with other threads.\n                    atomic::fence(Ordering::SeqCst);\n\n                    // Load the back index.\n                    let b = self.inner.back.load(Ordering::Acquire);\n\n                    // Is the queue empty?\n                    if b.wrapping_sub(f) <= 0 {\n                        batch_size = i;\n                        break;\n                    }\n\n                    // Read the task at the front.\n                    let tmp = unsafe { buffer.deref().read(f) };\n\n                    // Try incrementing the front index to steal the task.\n                    if self\n                        .inner\n                        .front\n                        .compare_exchange(f, f.wrapping_add(1), Ordering::SeqCst, Ordering::Relaxed)\n                        .is_err()\n                    {\n                        // We didn't steal this task, forget it and break from the loop.\n                        mem::forget(tmp);\n                        batch_size = i;\n                        break;\n                    }\n\n                    // Write the previously stolen task into the destination buffer.\n                    unsafe {\n                        dest_buffer.write(dest_b, mem::replace(&mut task, tmp));\n                    }\n\n                    // Move the source front index and the destination back index one step forward.\n                    f = f.wrapping_add(1);\n                    dest_b = dest_b.wrapping_add(1);\n                }\n\n                // If stealing into a FIFO queue, stolen tasks need to be reversed.\n                if dest.flavor == Flavor::Fifo {\n                    for i in 0..batch_size / 2 {\n                        unsafe {\n                            let i1 = dest_b.wrapping_sub(batch_size - i);\n                            let i2 = dest_b.wrapping_sub(i + 1);\n                            let t1 = dest_buffer.read(i1);\n                            let t2 = dest_buffer.read(i2);\n                            dest_buffer.write(i1, t2);\n                            dest_buffer.write(i2, t1);\n                        }\n                    }\n                }\n            }\n        }\n\n        atomic::fence(Ordering::Release);\n\n        // Update the back index in the destination queue.\n        //\n        // This ordering could be `Relaxed`, but then thread sanitizer would falsely report data\n        // races because it doesn't understand fences.\n        dest.inner.back.store(dest_b, Ordering::Release);\n\n        // Return with success.\n        Steal::Success(task)\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker":["/// A worker queue.\n///\n/// This is a FIFO or LIFO queue that is owned by a single thread, but other threads may steal\n/// tasks from it. Task schedulers typically create a single worker queue per thread.\n///\n/// # Examples\n///\n/// A FIFO worker:\n///\n/// ```\n/// use crossbeam_deque::{Steal, Worker};\n///\n/// let w = Worker::new_fifo();\n/// let s = w.stealer();\n///\n/// w.push(1);\n/// w.push(2);\n/// w.push(3);\n///\n/// assert_eq!(s.steal(), Steal::Success(1));\n/// assert_eq!(w.pop(), Some(2));\n/// assert_eq!(w.pop(), Some(3));\n/// ```\n///\n/// A LIFO worker:\n///\n/// ```\n/// use crossbeam_deque::{Steal, Worker};\n///\n/// let w = Worker::new_lifo();\n/// let s = w.stealer();\n///\n/// w.push(1);\n/// w.push(2);\n/// w.push(3);\n///\n/// assert_eq!(s.steal(), Steal::Success(1));\n/// assert_eq!(w.pop(), Some(3));\n/// assert_eq!(w.pop(), Some(2));\n/// ```\npub struct Worker<T> {\n    /// A reference to the inner representation of the queue.\n    inner: Arc<CachePadded<Inner<T>>>,\n\n    /// A copy of `inner.buffer` for quick access.\n    buffer: Cell<Buffer<T>>,\n\n    /// The flavor of the queue.\n    flavor: Flavor,\n\n    /// Indicates that the worker cannot be shared among threads.\n    _marker: PhantomData<*mut ()>, // !Send + !Sync\n}","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::is_empty":["/// Returns `true` if the queue is empty.\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::new_lifo();\n///\n/// assert!(w.is_empty());\n/// w.push(1);\n/// assert!(!w.is_empty());\n/// ```\npub fn is_empty(&self) -> bool{\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::SeqCst);\n        b.wrapping_sub(f) <= 0\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::len":["/// Returns the number of tasks in the deque.\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::new_lifo();\n///\n/// assert_eq!(w.len(), 0);\n/// w.push(1);\n/// assert_eq!(w.len(), 1);\n/// w.push(1);\n/// assert_eq!(w.len(), 2);\n/// ```\npub fn len(&self) -> usize{\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::SeqCst);\n        b.wrapping_sub(f).max(0) as usize\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::new_fifo":["/// Creates a FIFO worker queue.\n///\n/// Tasks are pushed and popped from opposite ends.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::<i32>::new_fifo();\n/// ```\npub fn new_fifo() -> Worker<T>{\n        let buffer = Buffer::alloc(MIN_CAP);\n\n        let inner = Arc::new(CachePadded::new(Inner {\n            front: AtomicIsize::new(0),\n            back: AtomicIsize::new(0),\n            buffer: CachePadded::new(Atomic::new(buffer)),\n        }));\n\n        Worker {\n            inner,\n            buffer: Cell::new(buffer),\n            flavor: Flavor::Fifo,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::new_lifo":["/// Creates a LIFO worker queue.\n///\n/// Tasks are pushed and popped from the same end.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::<i32>::new_lifo();\n/// ```\npub fn new_lifo() -> Worker<T>{\n        let buffer = Buffer::alloc(MIN_CAP);\n\n        let inner = Arc::new(CachePadded::new(Inner {\n            front: AtomicIsize::new(0),\n            back: AtomicIsize::new(0),\n            buffer: CachePadded::new(Atomic::new(buffer)),\n        }));\n\n        Worker {\n            inner,\n            buffer: Cell::new(buffer),\n            flavor: Flavor::Lifo,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::pop":["/// Pops a task from the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::new_fifo();\n/// w.push(1);\n/// w.push(2);\n///\n/// assert_eq!(w.pop(), Some(1));\n/// assert_eq!(w.pop(), Some(2));\n/// assert_eq!(w.pop(), None);\n/// ```\npub fn pop(&self) -> Option<T>{\n        // Load the back and front index.\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::Relaxed);\n\n        // Calculate the length of the queue.\n        let len = b.wrapping_sub(f);\n\n        // Is the queue empty?\n        if len <= 0 {\n            return None;\n        }\n\n        match self.flavor {\n            // Pop from the front of the queue.\n            Flavor::Fifo => {\n                // Try incrementing the front index to pop the task.\n                let f = self.inner.front.fetch_add(1, Ordering::SeqCst);\n                let new_f = f.wrapping_add(1);\n\n                if b.wrapping_sub(new_f) < 0 {\n                    self.inner.front.store(f, Ordering::Relaxed);\n                    return None;\n                }\n\n                unsafe {\n                    // Read the popped task.\n                    let buffer = self.buffer.get();\n                    let task = buffer.read(f);\n\n                    // Shrink the buffer if `len - 1` is less than one fourth of the capacity.\n                    if buffer.cap > MIN_CAP && len <= buffer.cap as isize / 4 {\n                        self.resize(buffer.cap / 2);\n                    }\n\n                    Some(task)\n                }\n            }\n\n            // Pop from the back of the queue.\n            Flavor::Lifo => {\n                // Decrement the back index.\n                let b = b.wrapping_sub(1);\n                self.inner.back.store(b, Ordering::Relaxed);\n\n                atomic::fence(Ordering::SeqCst);\n\n                // Load the front index.\n                let f = self.inner.front.load(Ordering::Relaxed);\n\n                // Compute the length after the back index was decremented.\n                let len = b.wrapping_sub(f);\n\n                if len < 0 {\n                    // The queue is empty. Restore the back index to the original task.\n                    self.inner.back.store(b.wrapping_add(1), Ordering::Relaxed);\n                    None\n                } else {\n                    // Read the task to be popped.\n                    let buffer = self.buffer.get();\n                    let mut task = unsafe { Some(buffer.read(b)) };\n\n                    // Are we popping the last task from the queue?\n                    if len == 0 {\n                        // Try incrementing the front index.\n                        if self\n                            .inner\n                            .front\n                            .compare_exchange(\n                                f,\n                                f.wrapping_add(1),\n                                Ordering::SeqCst,\n                                Ordering::Relaxed,\n                            )\n                            .is_err()\n                        {\n                            // Failed. We didn't pop anything.\n                            mem::forget(task.take());\n                        }\n\n                        // Restore the back index to the original task.\n                        self.inner.back.store(b.wrapping_add(1), Ordering::Relaxed);\n                    } else {\n                        // Shrink the buffer if `len` is less than one fourth of the capacity.\n                        if buffer.cap > MIN_CAP && len < buffer.cap as isize / 4 {\n                            unsafe {\n                                self.resize(buffer.cap / 2);\n                            }\n                        }\n                    }\n\n                    task\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::push":["/// Pushes a task into the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::new_lifo();\n/// w.push(1);\n/// w.push(2);\n/// ```\npub fn push(&self, task: T){\n        // Load the back index, front index, and buffer.\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::Acquire);\n        let mut buffer = self.buffer.get();\n\n        // Calculate the length of the queue.\n        let len = b.wrapping_sub(f);\n\n        // Is the queue full?\n        if len >= buffer.cap as isize {\n            // Yes. Grow the underlying buffer.\n            unsafe {\n                self.resize(2 * buffer.cap);\n            }\n            buffer = self.buffer.get();\n        }\n\n        // Write `task` into the slot.\n        unsafe {\n            buffer.write(b, task);\n        }\n\n        atomic::fence(Ordering::Release);\n\n        // Increment the back index.\n        //\n        // This ordering could be `Relaxed`, but then thread sanitizer would falsely report data\n        // races because it doesn't understand fences.\n        self.inner.back.store(b.wrapping_add(1), Ordering::Release);\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::reserve":["/// Reserves enough capacity so that `reserve_cap` tasks can be pushed without growing the\n/// buffer.\nfn reserve(&self, reserve_cap: usize){\n        if reserve_cap > 0 {\n            // Compute the current length.\n            let b = self.inner.back.load(Ordering::Relaxed);\n            let f = self.inner.front.load(Ordering::SeqCst);\n            let len = b.wrapping_sub(f) as usize;\n\n            // The current capacity.\n            let cap = self.buffer.get().cap;\n\n            // Is there enough capacity to push `reserve_cap` tasks?\n            if cap - len < reserve_cap {\n                // Keep doubling the capacity as much as is needed.\n                let mut new_cap = cap * 2;\n                while new_cap - len < reserve_cap {\n                    new_cap *= 2;\n                }\n\n                // Resize the buffer.\n                unsafe {\n                    self.resize(new_cap);\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::resize":["/// Resizes the internal buffer to the new capacity of `new_cap`.\n#[cold]\nunsafe fn resize(&self, new_cap: usize){\n        // Load the back index, front index, and buffer.\n        let b = self.inner.back.load(Ordering::Relaxed);\n        let f = self.inner.front.load(Ordering::Relaxed);\n        let buffer = self.buffer.get();\n\n        // Allocate a new buffer and copy data from the old buffer to the new one.\n        let new = Buffer::alloc(new_cap);\n        let mut i = f;\n        while i != b {\n            ptr::copy_nonoverlapping(buffer.at(i), new.at(i), 1);\n            i = i.wrapping_add(1);\n        }\n\n        let guard = &epoch::pin();\n\n        // Replace the old buffer with the new one.\n        self.buffer.replace(new);\n        let old =\n            self.inner\n                .buffer\n                .swap(Owned::new(new).into_shared(guard), Ordering::Release, guard);\n\n        // Destroy the old buffer later.\n        guard.defer_unchecked(move || old.into_owned().into_box().dealloc());\n\n        // If the buffer is very large, then flush the thread-local garbage in order to deallocate\n        // it as soon as possible.\n        if mem::size_of::<T>() * new_cap >= FLUSH_THRESHOLD_BYTES {\n            guard.flush();\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"],"deque::Worker::<T>::stealer":["/// Creates a stealer for this queue.\n///\n/// The returned stealer can be shared among threads and cloned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_deque::Worker;\n///\n/// let w = Worker::<i32>::new_lifo();\n/// let s = w.stealer();\n/// ```\npub fn stealer(&self) -> Stealer<T>{\n        Stealer {\n            inner: self.inner.clone(),\n            flavor: self.flavor,\n        }\n    }","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))"]},"struct_constructor":{"bool":["eq","is_empty","is_retry","is_success"],"deque::Block":["new"],"deque::Buffer":["alloc","clone"],"deque::Flavor":["clone"],"deque::Injector":["default","new"],"deque::Steal":["clone","from_iter","steal","steal_batch","steal_batch_and_pop"],"deque::Stealer":["clone","stealer"],"deque::Worker":["new_fifo","new_lifo"],"usize":["len"]},"struct_to_trait":{"deque::Buffer":["std::clone::Clone","std::marker::Copy","std::marker::Send"],"deque::Flavor":["std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::fmt::Debug","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq"],"deque::Injector":["std::default::Default","std::fmt::Debug","std::marker::Send","std::marker::Sync","std::ops::Drop"],"deque::Inner":["std::ops::Drop"],"deque::Steal":["std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::fmt::Debug","std::iter::FromIterator","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq"],"deque::Stealer":["std::clone::Clone","std::fmt::Debug","std::marker::Send","std::marker::Sync"],"deque::Worker":["std::fmt::Debug","std::marker::Send"]},"targets":{"<deque::Buffer<T> as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::clone::Clone"],"<deque::Injector<T> as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::default::Default"],"<deque::Injector<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::fmt::Debug"],"<deque::Injector<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::ops::Drop"],"<deque::Inner<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::ops::Drop"],"<deque::Steal<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::fmt::Debug"],"<deque::Steal<T> as std::iter::FromIterator<deque::Steal<T>>>::from_iter":["from_iter","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::iter::FromIterator"],"<deque::Stealer<T> as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::clone::Clone"],"<deque::Stealer<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::fmt::Debug"],"<deque::Worker<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))","std::fmt::Debug"],"deque::Block::<T>::destroy":["destroy","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Block::<T>::new":["new","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Block::<T>::wait_next":["wait_next","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Buffer::<T>::alloc":["alloc","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Buffer::<T>::at":["at","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Buffer::<T>::dealloc":["dealloc","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Buffer::<T>::read":["read","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Buffer::<T>::write":["write","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::is_empty":["is_empty","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::len":["len","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::new":["new","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::push":["push","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::steal":["steal","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::steal_batch":["steal_batch","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Injector::<T>::steal_batch_and_pop":["steal_batch_and_pop","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Slot::<T>::wait_write":["wait_write","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Steal::<T>::is_empty":["is_empty","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Steal::<T>::is_retry":["is_retry","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Steal::<T>::is_success":["is_success","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Steal::<T>::or_else":["or_else","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Steal::<T>::success":["success","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Stealer::<T>::is_empty":["is_empty","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Stealer::<T>::steal":["steal","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Stealer::<T>::steal_batch":["steal_batch","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Stealer::<T>::steal_batch_and_pop":["steal_batch_and_pop","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::is_empty":["is_empty","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::len":["len","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::new_fifo":["new_fifo","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::new_lifo":["new_lifo","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::pop":["pop","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::push":["push","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::reserve":["reserve","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::resize":["resize","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""],"deque::Worker::<T>::stealer":["stealer","Real(LocalPath(\"crossbeam-deque/src/deque.rs\"))",""]},"trait_to_struct":{"std::clone::Clone":["deque::Buffer","deque::Flavor","deque::Steal","deque::Stealer"],"std::cmp::Eq":["deque::Flavor","deque::Steal"],"std::cmp::PartialEq":["deque::Flavor","deque::Steal"],"std::default::Default":["deque::Injector"],"std::fmt::Debug":["deque::Flavor","deque::Injector","deque::Steal","deque::Stealer","deque::Worker"],"std::iter::FromIterator":["deque::Steal"],"std::marker::Copy":["deque::Buffer","deque::Flavor","deque::Steal"],"std::marker::Send":["deque::Buffer","deque::Injector","deque::Stealer","deque::Worker"],"std::marker::StructuralEq":["deque::Flavor","deque::Steal"],"std::marker::StructuralPartialEq":["deque::Flavor","deque::Steal"],"std::marker::Sync":["deque::Injector","deque::Stealer"],"std::ops::Drop":["deque::Injector","deque::Inner"]},"type_to_def_path":{"deque::Block<T>":"deque::Block","deque::Buffer<T>":"deque::Buffer","deque::Flavor":"deque::Flavor","deque::Injector<T>":"deque::Injector","deque::Inner<T>":"deque::Inner","deque::Position<T>":"deque::Position","deque::Slot<T>":"deque::Slot","deque::Steal<T>":"deque::Steal","deque::Stealer<T>":"deque::Stealer","deque::Worker<T>":"deque::Worker"}}