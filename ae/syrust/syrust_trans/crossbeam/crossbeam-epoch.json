{"dependencies":{"<(std::sync::atomic::Ordering, std::sync::atomic::Ordering) as atomic::CompareAndSetOrdering>::failure":["std::sync::atomic::Ordering"],"<(std::sync::atomic::Ordering, std::sync::atomic::Ordering) as atomic::CompareAndSetOrdering>::success":["std::sync::atomic::Ordering"],"<T as atomic::Pointable>::deref":[],"<T as atomic::Pointable>::deref_mut":[],"<T as atomic::Pointable>::drop":[],"<T as atomic::Pointable>::init":[],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::deref":["std::marker::Sized","std::mem::MaybeUninit"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::deref_mut":["std::marker::Sized","std::mem::MaybeUninit"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::drop":[],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::init":[],"<atomic::Atomic<T> as std::clone::Clone>::clone":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::convert::From<*const T>>::from":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::convert::From<T>>::from":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::convert::From<atomic::Owned<T>>>::from":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Owned","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::convert::From<atomic::Shared<'g, T>>>::from":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::convert::From<std::boxed::Box<T>>>::from":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::alloc::Allocator","std::boxed::Box","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::default::Default>::default":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize"],"<atomic::Atomic<T> as std::fmt::Pointer>::fmt":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize"],"<atomic::CompareAndSetError<'g, T, P> as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::CompareAndSetError","atomic::Owned","atomic::Pointable","atomic::Pointer","atomic::Shared","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<atomic::Owned<T> as atomic::Pointer<T>>::from_usize":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as atomic::Pointer<T>>::into_usize":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as lazy_static::__Deref>::deref":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::borrow::Borrow<T>>::borrow":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::borrow::BorrowMut<T>>::borrow_mut":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::clone::Clone>::clone":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::convert::AsMut<T>>::as_mut":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::convert::AsRef<T>>::as_ref":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::convert::From<T>>::from":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::convert::From<std::boxed::Box<T>>>::from":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::alloc::Allocator","std::boxed::Box","std::marker::PhantomData","std::marker::Sized"],"<atomic::Owned<T> as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<atomic::Owned<T> as std::ops::DerefMut>::deref_mut":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Owned<T> as std::ops::Drop>::drop":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"<atomic::Shared<'_, T> as atomic::Pointer<T>>::from_usize":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"<atomic::Shared<'_, T> as atomic::Pointer<T>>::into_usize":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"<atomic::Shared<'_, T> as std::clone::Clone>::clone":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"<atomic::Shared<'_, T> as std::cmp::Ord>::cmp":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::cmp::Ordering","std::marker::PhantomData"],"<atomic::Shared<'_, T> as std::convert::From<*const T>>::from":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"<atomic::Shared<'_, T> as std::default::Default>::default":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"<atomic::Shared<'_, T> as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<atomic::Shared<'_, T> as std::fmt::Pointer>::fmt":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<atomic::Shared<'g, T> as std::cmp::PartialEq>::eq":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"<atomic::Shared<'g, T> as std::cmp::PartialOrd>::partial_cmp":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData","std::marker::Sized","std::option::Option"],"<collector::Collector as std::clone::Clone>::clone":["collector::Collector","std::sync::Arc"],"<collector::Collector as std::cmp::PartialEq>::eq":["collector::Collector","std::sync::Arc"],"<collector::Collector as std::default::Default>::default":["collector::Collector","std::sync::Arc"],"<collector::Collector as std::fmt::Debug>::fmt":["collector::Collector","std::fmt::Formatter","std::marker::Sized","std::result::Result","std::sync::Arc"],"<collector::LocalHandle as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<collector::LocalHandle as std::ops::Drop>::drop":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<default::COLLECTOR as lazy_static::LazyStatic>::initialize":["default::COLLECTOR"],"<default::COLLECTOR as lazy_static::__Deref>::deref":["collector::Collector","default::COLLECTOR","std::sync::Arc"],"<default::COLLECTOR as lazy_static::__Deref>::deref::__stability":["collector::Collector","std::sync::Arc"],"<default::COLLECTOR as lazy_static::__Deref>::deref::__static_ref_initialize":["collector::Collector","std::sync::Arc"],"<deferred::Deferred as std::fmt::Debug>::fmt":["deferred::Deferred","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<epoch::AtomicEpoch as std::default::Default>::default":["epoch::AtomicEpoch","std::sync::atomic::AtomicUsize"],"<epoch::AtomicEpoch as std::fmt::Debug>::fmt":["epoch::AtomicEpoch","std::fmt::Formatter","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize"],"<epoch::Epoch as std::clone::Clone>::clone":["epoch::Epoch"],"<epoch::Epoch as std::cmp::Eq>::assert_receiver_is_total_eq":["epoch::Epoch"],"<epoch::Epoch as std::cmp::PartialEq>::eq":["epoch::Epoch"],"<epoch::Epoch as std::default::Default>::default":["epoch::Epoch"],"<epoch::Epoch as std::fmt::Debug>::fmt":["epoch::Epoch","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<guard::Guard as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<guard::Guard as std::ops::Drop>::drop":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<internal::Bag as std::default::Default>::default":["deferred::Deferred","internal::Bag","std::marker::PhantomData"],"<internal::Bag as std::fmt::Debug>::fmt":["deferred::Deferred","internal::Bag","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<internal::Bag as std::ops::Drop>::drop":["deferred::Deferred","internal::Bag","std::marker::PhantomData"],"<internal::Local as sync::list::IsElement<internal::Local>>::element_of":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<internal::Local as sync::list::IsElement<internal::Local>>::entry_of":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<internal::Local as sync::list::IsElement<internal::Local>>::finalize":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<internal::SealedBag as std::default::Default>::default":["deferred::Deferred","epoch::Epoch","internal::Bag","internal::SealedBag","std::marker::PhantomData"],"<internal::SealedBag as std::fmt::Debug>::fmt":["deferred::Deferred","epoch::Epoch","internal::Bag","internal::SealedBag","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<std::sync::atomic::Ordering as atomic::CompareAndSetOrdering>::failure":["std::sync::atomic::Ordering"],"<std::sync::atomic::Ordering as atomic::CompareAndSetOrdering>::success":["std::sync::atomic::Ordering"],"<sync::list::Entry as std::default::Default>::default":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<sync::list::Entry as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","sync::list::Entry"],"<sync::list::Iter<'g, T, C> as std::iter::Iterator>::next":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::Iter"],"<sync::list::IterError as std::cmp::PartialEq>::eq":["sync::list::IterError"],"<sync::list::IterError as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::Sized","std::result::Result","sync::list::IterError"],"<sync::list::List<T, C> as std::fmt::Debug>::fmt":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List"],"<sync::list::List<T, C> as std::ops::Drop>::drop":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List"],"<sync::queue::Queue<T> as std::fmt::Debug>::fmt":["crossbeam_utils::CachePadded","std::fmt::Formatter","std::marker::Sized","std::result::Result","sync::queue::Queue"],"<sync::queue::Queue<T> as std::ops::Drop>::drop":["crossbeam_utils::CachePadded","std::marker::Sized","sync::queue::Queue"],"atomic::Array":["atomic::Array","std::marker::Sized","std::mem::MaybeUninit"],"atomic::Atomic":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"atomic::Atomic::<T>::compare_and_set":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::CompareAndSetOrdering","atomic::Owned","atomic::Pointable","atomic::Pointer","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::Atomic::<T>::compare_and_set_weak":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::CompareAndSetOrdering","atomic::Owned","atomic::Pointable","atomic::Pointer","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::Atomic::<T>::fetch_and":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::Atomic::<T>::fetch_or":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::Atomic::<T>::fetch_xor":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::Atomic::<T>::from_usize":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"atomic::Atomic::<T>::init":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"atomic::Atomic::<T>::into_owned":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Owned","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"atomic::Atomic::<T>::load":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::Atomic::<T>::load_consume":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"atomic::Atomic::<T>::new":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"atomic::Atomic::<T>::null":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize"],"atomic::Atomic::<T>::store":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Owned","atomic::Pointable","atomic::Pointer","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering"],"atomic::Atomic::<T>::swap":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Owned","atomic::Pointable","atomic::Pointer","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering","sync::list::Entry"],"atomic::CompareAndSetError":["<T as atomic::Pointable>::T","atomic::CompareAndSetError","atomic::Owned","atomic::Pointable","atomic::Pointer","atomic::Shared","std::marker::PhantomData","std::marker::Sized"],"atomic::CompareAndSetOrdering::failure":["std::sync::atomic::Ordering"],"atomic::CompareAndSetOrdering::success":["std::sync::atomic::Ordering"],"atomic::Owned":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"atomic::Owned::<T>::from_raw":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"atomic::Owned::<T>::init":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"atomic::Owned::<T>::into_box":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::alloc::Allocator","std::boxed::Box","std::marker::PhantomData","std::marker::Sized"],"atomic::Owned::<T>::into_shared":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Owned","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"atomic::Owned::<T>::new":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"atomic::Owned::<T>::tag":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"atomic::Owned::<T>::with_tag":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","std::marker::PhantomData"],"atomic::Pointable::deref":[],"atomic::Pointable::deref_mut":[],"atomic::Pointable::drop":[],"atomic::Pointable::init":[],"atomic::Pointer::from_usize":[],"atomic::Pointer::into_usize":[],"atomic::Shared":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::as_raw":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::as_ref":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData","std::marker::Sized","std::option::Option"],"atomic::Shared::<'g, T>::deref":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::deref_mut":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::into_owned":["<T as atomic::Pointable>::T","atomic::Owned","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::is_null":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::null":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::tag":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::Shared::<'g, T>::with_tag":["<T as atomic::Pointable>::T","atomic::Pointable","atomic::Shared","std::marker::PhantomData"],"atomic::compose_tag":["<T as atomic::Pointable>::T","atomic::Pointable"],"atomic::decompose_tag":["<T as atomic::Pointable>::T","atomic::Pointable"],"atomic::ensure_aligned":["<T as atomic::Pointable>::T","atomic::Pointable"],"atomic::low_bits":["<T as atomic::Pointable>::T","atomic::Pointable"],"atomic::strongest_failure_ordering":["std::sync::atomic::Ordering"],"collector::Collector":["collector::Collector","std::sync::Arc"],"collector::Collector::new":["collector::Collector","std::sync::Arc"],"collector::Collector::register":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::Collector","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::Arc","std::sync::atomic::AtomicUsize","sync::list::Entry"],"collector::LocalHandle":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"collector::LocalHandle::collector":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::Collector","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::Arc","std::sync::atomic::AtomicUsize","sync::list::Entry"],"collector::LocalHandle::is_pinned":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"collector::LocalHandle::pin":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::LocalHandle","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"default::COLLECTOR":["default::COLLECTOR"],"default::HANDLE::__getit":["std::marker::Sized","std::option::Option"],"default::HANDLE::__init":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"default::default_collector":["collector::Collector","std::sync::Arc"],"default::is_pinned":[],"default::pin":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"default::with_handle":["std::marker::Sized","std::ops::FnMut"],"deferred::Deferred":["deferred::Deferred","std::marker::PhantomData"],"deferred::Deferred::call":["deferred::Deferred","std::marker::PhantomData"],"deferred::Deferred::new":["deferred::Deferred","std::marker::PhantomData","std::marker::Sized","std::ops::FnOnce"],"deferred::Deferred::new::call":["std::marker::Sized","std::ops::FnOnce"],"epoch::AtomicEpoch":["epoch::AtomicEpoch","std::sync::atomic::AtomicUsize"],"epoch::AtomicEpoch::compare_and_swap":["epoch::AtomicEpoch","epoch::Epoch","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering"],"epoch::AtomicEpoch::load":["epoch::AtomicEpoch","epoch::Epoch","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering"],"epoch::AtomicEpoch::new":["epoch::AtomicEpoch","epoch::Epoch","std::sync::atomic::AtomicUsize"],"epoch::AtomicEpoch::store":["epoch::AtomicEpoch","epoch::Epoch","std::sync::atomic::AtomicUsize","std::sync::atomic::Ordering"],"epoch::Epoch":["epoch::Epoch"],"epoch::Epoch::is_pinned":["epoch::Epoch"],"epoch::Epoch::pinned":["epoch::Epoch"],"epoch::Epoch::starting":["epoch::Epoch"],"epoch::Epoch::successor":["epoch::Epoch"],"epoch::Epoch::unpinned":["epoch::Epoch"],"epoch::Epoch::wrapping_sub":["epoch::Epoch"],"guard::Guard":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::collector":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::defer":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Send","std::marker::Sized","std::ops::FnOnce","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::defer_destroy":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::defer_unchecked":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::ops::FnOnce","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::flush":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::repin":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::Guard::repin_after":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::ops::FnOnce","std::sync::atomic::AtomicUsize","sync::list::Entry"],"guard::unprotected":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Bag":["deferred::Deferred","internal::Bag","std::marker::PhantomData"],"internal::Bag::is_empty":["deferred::Deferred","internal::Bag","std::marker::PhantomData"],"internal::Bag::new":["deferred::Deferred","internal::Bag","std::marker::PhantomData"],"internal::Bag::seal":["deferred::Deferred","epoch::Epoch","internal::Bag","internal::SealedBag","std::marker::PhantomData"],"internal::Bag::try_push":["deferred::Deferred","internal::Bag","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"internal::Global":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","internal::Global","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List","sync::queue::Queue"],"internal::Global::collect":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Global","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List","sync::queue::Queue"],"internal::Global::new":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","internal::Global","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List","sync::queue::Queue"],"internal::Global::push_bag":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","deferred::Deferred","epoch::AtomicEpoch","guard::Guard","internal::Bag","internal::Global","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List","sync::queue::Queue"],"internal::Global::try_advance":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","epoch::Epoch","guard::Guard","internal::Global","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List","sync::queue::Queue"],"internal::Local":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::acquire_handle":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::collector":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::Collector","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::Arc","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::defer":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","deferred::Deferred","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::finalize":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::flush":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::global":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","internal::Global","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List","sync::queue::Queue"],"internal::Local::is_pinned":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::pin":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::register":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","collector::Collector","collector::LocalHandle","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::Arc","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::release_handle":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::repin":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::Local::unpin":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"internal::SealedBag":["deferred::Deferred","epoch::Epoch","internal::Bag","internal::SealedBag","std::marker::PhantomData"],"internal::SealedBag::is_expired":["deferred::Deferred","epoch::Epoch","internal::Bag","internal::SealedBag","std::marker::PhantomData"],"internal::no_op_func":[],"sync::list::Entry":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"sync::list::Entry::delete":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"sync::list::IsElement::element_of":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"sync::list::IsElement::entry_of":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"sync::list::IsElement::finalize":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::sync::atomic::AtomicUsize","sync::list::Entry"],"sync::list::Iter":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::Iter"],"sync::list::IterError":["sync::list::IterError"],"sync::list::List":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List"],"sync::list::List::<T, C>::insert":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List"],"sync::list::List::<T, C>::iter":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::Iter","sync::list::List"],"sync::list::List::<T, C>::new":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","epoch::AtomicEpoch","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::list::IsElement","sync::list::List"],"sync::queue::Node":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","std::marker::PhantomData","std::marker::Sized","std::mem::MaybeUninit","std::sync::atomic::AtomicUsize","sync::queue::Node"],"sync::queue::Queue":["crossbeam_utils::CachePadded","std::marker::Sized","sync::queue::Queue"],"sync::queue::Queue::<T>::new":["crossbeam_utils::CachePadded","std::marker::Sized","sync::queue::Queue"],"sync::queue::Queue::<T>::pop_if_internal":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::marker::Sync","std::ops::Fn","std::result::Result","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::queue::Queue"],"sync::queue::Queue::<T>::pop_internal":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::queue::Queue"],"sync::queue::Queue::<T>::push":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::queue::Queue"],"sync::queue::Queue::<T>::push_internal":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","atomic::Shared","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::queue::Queue"],"sync::queue::Queue::<T>::try_pop":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::queue::Queue"],"sync::queue::Queue::<T>::try_pop_if":["<T as atomic::Pointable>::T","atomic::Atomic","atomic::Pointable","crossbeam_utils::CachePadded","epoch::AtomicEpoch","guard::Guard","internal::Local","std::cell::Cell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::marker::Sync","std::ops::Fn","std::option::Option","std::sync::atomic::AtomicUsize","sync::list::Entry","sync::queue::Queue"]},"glob_path_import":{},"self_to_fn":{"<T as atomic::Pointable>::T":["impl<T> Pointable for T {\n    const ALIGN: usize = mem::align_of::<T>();\n\n    type Init = T;\n\n    unsafe fn init(init: Self::Init) -> usize {\n        Box::into_raw(Box::new(init)) as usize\n    }\n\n    unsafe fn deref<'a>(ptr: usize) -> &'a Self {\n        &*(ptr as *const T)\n    }\n\n    unsafe fn deref_mut<'a>(ptr: usize) -> &'a mut Self {\n        &mut *(ptr as *mut T)\n    }\n\n    unsafe fn drop(ptr: usize) {\n        drop(Box::from_raw(ptr as *mut T));\n    }\n}"],"atomic::Atomic":["impl<'g, T: ?Sized + Pointable> From<Shared<'g, T>> for Atomic<T> {\n    /// Returns a new atomic pointer pointing to `ptr`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{Atomic, Shared};\n    ///\n    /// let a = Atomic::<i32>::from(Shared::<i32>::null());\n    /// ```\n    fn from(ptr: Shared<'g, T>) -> Self {\n        Self::from_usize(ptr.data)\n    }\n}","impl<T: ?Sized + Pointable> Atomic<T> {\n    /// Allocates `value` on the heap and returns a new atomic pointer pointing to it.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Atomic;\n    ///\n    /// let a = Atomic::<i32>::init(1234);\n    /// ```\n    pub fn init(init: T::Init) -> Atomic<T> {\n        Self::from(Owned::init(init))\n    }\n\n    /// Returns a new atomic pointer pointing to the tagged pointer `data`.\n    fn from_usize(data: usize) -> Self {\n        Self {\n            data: AtomicUsize::new(data),\n            _marker: PhantomData,\n        }\n    }\n\n    /// Returns a new null atomic pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Atomic;\n    ///\n    /// let a = Atomic::<i32>::null();\n    /// ```\n    #[cfg(feature = \"nightly\")]\n    pub const fn null() -> Atomic<T> {\n        Self {\n            data: AtomicUsize::new(0),\n            _marker: PhantomData,\n        }\n    }\n\n    /// Returns a new null atomic pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Atomic;\n    ///\n    /// let a = Atomic::<i32>::null();\n    /// ```\n    #[cfg(not(feature = \"nightly\"))]\n    pub fn null() -> Atomic<T> {\n        Self {\n            data: AtomicUsize::new(0),\n            _marker: PhantomData,\n        }\n    }\n\n    /// Loads a `Shared` from the atomic pointer.\n    ///\n    /// This method takes an [`Ordering`] argument which describes the memory ordering of this\n    /// operation.\n    ///\n    /// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// let guard = &epoch::pin();\n    /// let p = a.load(SeqCst, guard);\n    /// ```\n    pub fn load<'g>(&self, ord: Ordering, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.data.load(ord)) }\n    }\n\n    /// Loads a `Shared` from the atomic pointer using a \"consume\" memory ordering.\n    ///\n    /// This is similar to the \"acquire\" ordering, except that an ordering is\n    /// only guaranteed with operations that \"depend on\" the result of the load.\n    /// However consume loads are usually much faster than acquire loads on\n    /// architectures with a weak memory model since they don't require memory\n    /// fence instructions.\n    ///\n    /// The exact definition of \"depend on\" is a bit vague, but it works as you\n    /// would expect in practice since a lot of software, especially the Linux\n    /// kernel, rely on this behavior.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    ///\n    /// let a = Atomic::new(1234);\n    /// let guard = &epoch::pin();\n    /// let p = a.load_consume(guard);\n    /// ```\n    pub fn load_consume<'g>(&self, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.data.load_consume()) }\n    }\n\n    /// Stores a `Shared` or `Owned` pointer into the atomic pointer.\n    ///\n    /// This method takes an [`Ordering`] argument which describes the memory ordering of this\n    /// operation.\n    ///\n    /// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{Atomic, Owned, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// a.store(Shared::null(), SeqCst);\n    /// a.store(Owned::new(1234), SeqCst);\n    /// ```\n    pub fn store<P: Pointer<T>>(&self, new: P, ord: Ordering) {\n        self.data.store(new.into_usize(), ord);\n    }\n\n    /// Stores a `Shared` or `Owned` pointer into the atomic pointer, returning the previous\n    /// `Shared`.\n    ///\n    /// This method takes an [`Ordering`] argument which describes the memory ordering of this\n    /// operation.\n    ///\n    /// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// let guard = &epoch::pin();\n    /// let p = a.swap(Shared::null(), SeqCst, guard);\n    /// ```\n    pub fn swap<'g, P: Pointer<T>>(&self, new: P, ord: Ordering, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.data.swap(new.into_usize(), ord)) }\n    }\n\n    /// Stores the pointer `new` (either `Shared` or `Owned`) into the atomic pointer if the current\n    /// value is the same as `current`. The tag is also taken into account, so two pointers to the\n    /// same object, but with different tags, will not be considered equal.\n    ///\n    /// The return value is a result indicating whether the new pointer was written. On success the\n    /// pointer that was written is returned. On failure the actual current value and `new` are\n    /// returned.\n    ///\n    /// This method takes a [`CompareAndSetOrdering`] argument which describes the memory\n    /// ordering of this operation.\n    ///\n    /// [`CompareAndSetOrdering`]: trait.CompareAndSetOrdering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    ///\n    /// let guard = &epoch::pin();\n    /// let curr = a.load(SeqCst, guard);\n    /// let res1 = a.compare_and_set(curr, Shared::null(), SeqCst, guard);\n    /// let res2 = a.compare_and_set(curr, Owned::new(5678), SeqCst, guard);\n    /// ```\n    pub fn compare_and_set<'g, O, P>(\n        &self,\n        current: Shared<'_, T>,\n        new: P,\n        ord: O,\n        _: &'g Guard,\n    ) -> Result<Shared<'g, T>, CompareAndSetError<'g, T, P>>\n    where\n        O: CompareAndSetOrdering,\n        P: Pointer<T>,\n    {\n        let new = new.into_usize();\n        self.data\n            .compare_exchange(current.into_usize(), new, ord.success(), ord.failure())\n            .map(|_| unsafe { Shared::from_usize(new) })\n            .map_err(|current| unsafe {\n                CompareAndSetError {\n                    current: Shared::from_usize(current),\n                    new: P::from_usize(new),\n                }\n            })\n    }\n\n    /// Stores the pointer `new` (either `Shared` or `Owned`) into the atomic pointer if the current\n    /// value is the same as `current`. The tag is also taken into account, so two pointers to the\n    /// same object, but with different tags, will not be considered equal.\n    ///\n    /// Unlike [`compare_and_set`], this method is allowed to spuriously fail even when comparison\n    /// succeeds, which can result in more efficient code on some platforms.  The return value is a\n    /// result indicating whether the new pointer was written. On success the pointer that was\n    /// written is returned. On failure the actual current value and `new` are returned.\n    ///\n    /// This method takes a [`CompareAndSetOrdering`] argument which describes the memory\n    /// ordering of this operation.\n    ///\n    /// [`compare_and_set`]: struct.Atomic.html#method.compare_and_set\n    /// [`CompareAndSetOrdering`]: trait.CompareAndSetOrdering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// let guard = &epoch::pin();\n    ///\n    /// let mut new = Owned::new(5678);\n    /// let mut ptr = a.load(SeqCst, guard);\n    /// loop {\n    ///     match a.compare_and_set_weak(ptr, new, SeqCst, guard) {\n    ///         Ok(p) => {\n    ///             ptr = p;\n    ///             break;\n    ///         }\n    ///         Err(err) => {\n    ///             ptr = err.current;\n    ///             new = err.new;\n    ///         }\n    ///     }\n    /// }\n    ///\n    /// let mut curr = a.load(SeqCst, guard);\n    /// loop {\n    ///     match a.compare_and_set_weak(curr, Shared::null(), SeqCst, guard) {\n    ///         Ok(_) => break,\n    ///         Err(err) => curr = err.current,\n    ///     }\n    /// }\n    /// ```\n    pub fn compare_and_set_weak<'g, O, P>(\n        &self,\n        current: Shared<'_, T>,\n        new: P,\n        ord: O,\n        _: &'g Guard,\n    ) -> Result<Shared<'g, T>, CompareAndSetError<'g, T, P>>\n    where\n        O: CompareAndSetOrdering,\n        P: Pointer<T>,\n    {\n        let new = new.into_usize();\n        self.data\n            .compare_exchange_weak(current.into_usize(), new, ord.success(), ord.failure())\n            .map(|_| unsafe { Shared::from_usize(new) })\n            .map_err(|current| unsafe {\n                CompareAndSetError {\n                    current: Shared::from_usize(current),\n                    new: P::from_usize(new),\n                }\n            })\n    }\n\n    /// Bitwise \"and\" with the current tag.\n    ///\n    /// Performs a bitwise \"and\" operation on the current tag and the argument `val`, and sets the\n    /// new tag to the result. Returns the previous pointer.\n    ///\n    /// This method takes an [`Ordering`] argument which describes the memory ordering of this\n    /// operation.\n    ///\n    /// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::<i32>::from(Shared::null().with_tag(3));\n    /// let guard = &epoch::pin();\n    /// assert_eq!(a.fetch_and(2, SeqCst, guard).tag(), 3);\n    /// assert_eq!(a.load(SeqCst, guard).tag(), 2);\n    /// ```\n    pub fn fetch_and<'g>(&self, val: usize, ord: Ordering, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.data.fetch_and(val | !low_bits::<T>(), ord)) }\n    }\n\n    /// Bitwise \"or\" with the current tag.\n    ///\n    /// Performs a bitwise \"or\" operation on the current tag and the argument `val`, and sets the\n    /// new tag to the result. Returns the previous pointer.\n    ///\n    /// This method takes an [`Ordering`] argument which describes the memory ordering of this\n    /// operation.\n    ///\n    /// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::<i32>::from(Shared::null().with_tag(1));\n    /// let guard = &epoch::pin();\n    /// assert_eq!(a.fetch_or(2, SeqCst, guard).tag(), 1);\n    /// assert_eq!(a.load(SeqCst, guard).tag(), 3);\n    /// ```\n    pub fn fetch_or<'g>(&self, val: usize, ord: Ordering, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.data.fetch_or(val & low_bits::<T>(), ord)) }\n    }\n\n    /// Bitwise \"xor\" with the current tag.\n    ///\n    /// Performs a bitwise \"xor\" operation on the current tag and the argument `val`, and sets the\n    /// new tag to the result. Returns the previous pointer.\n    ///\n    /// This method takes an [`Ordering`] argument which describes the memory ordering of this\n    /// operation.\n    ///\n    /// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::<i32>::from(Shared::null().with_tag(1));\n    /// let guard = &epoch::pin();\n    /// assert_eq!(a.fetch_xor(3, SeqCst, guard).tag(), 1);\n    /// assert_eq!(a.load(SeqCst, guard).tag(), 2);\n    /// ```\n    pub fn fetch_xor<'g>(&self, val: usize, ord: Ordering, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.data.fetch_xor(val & low_bits::<T>(), ord)) }\n    }\n\n    /// Takes ownership of the pointee.\n    ///\n    /// This consumes the atomic and converts it into [`Owned`]. As [`Atomic`] doesn't have a\n    /// destructor and doesn't drop the pointee while [`Owned`] does, this is suitable for\n    /// destructors of data structures.\n    ///\n    /// # Panics\n    ///\n    /// Panics if this pointer is null, but only in debug mode.\n    ///\n    /// # Safety\n    ///\n    /// This method may be called only if the pointer is valid and nobody else is holding a\n    /// reference to the same object.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// # use std::mem;\n    /// # use crossbeam_epoch::Atomic;\n    /// struct DataStructure {\n    ///     ptr: Atomic<usize>,\n    /// }\n    ///\n    /// impl Drop for DataStructure {\n    ///     fn drop(&mut self) {\n    ///         // By now the DataStructure lives only in our thread and we are sure we don't hold\n    ///         // any Shared or & to it ourselves.\n    ///         unsafe {\n    ///             drop(mem::replace(&mut self.ptr, Atomic::null()).into_owned());\n    ///         }\n    ///     }\n    /// }\n    /// ```\n    pub unsafe fn into_owned(self) -> Owned<T> {\n        Owned::from_usize(self.data.into_inner())\n    }\n}","impl<T: ?Sized + Pointable> Clone for Atomic<T> {\n    /// Returns a copy of the atomic value.\n    ///\n    /// Note that a `Relaxed` load is used here. If you need synchronization, use it with other\n    /// atomics or fences.\n    fn clone(&self) -> Self {\n        let data = self.data.load(Ordering::Relaxed);\n        Atomic::from_usize(data)\n    }\n}","impl<T: ?Sized + Pointable> Default for Atomic<T> {\n    fn default() -> Self {\n        Atomic::null()\n    }\n}","impl<T: ?Sized + Pointable> From<Owned<T>> for Atomic<T> {\n    /// Returns a new atomic pointer pointing to `owned`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{Atomic, Owned};\n    ///\n    /// let a = Atomic::<i32>::from(Owned::new(1234));\n    /// ```\n    fn from(owned: Owned<T>) -> Self {\n        let data = owned.data;\n        mem::forget(owned);\n        Self::from_usize(data)\n    }\n}","impl<T: ?Sized + Pointable> fmt::Debug for Atomic<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        let data = self.data.load(Ordering::SeqCst);\n        let (raw, tag) = decompose_tag::<T>(data);\n\n        f.debug_struct(\"Atomic\")\n            .field(\"raw\", &raw)\n            .field(\"tag\", &tag)\n            .finish()\n    }\n}","impl<T: ?Sized + Pointable> fmt::Pointer for Atomic<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        let data = self.data.load(Ordering::SeqCst);\n        let (raw, _) = decompose_tag::<T>(data);\n        fmt::Pointer::fmt(&(unsafe { T::deref(raw) as *const _ }), f)\n    }\n}","impl<T> Atomic<T> {\n    /// Allocates `value` on the heap and returns a new atomic pointer pointing to it.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Atomic;\n    ///\n    /// let a = Atomic::new(1234);\n    /// ```\n    pub fn new(init: T) -> Atomic<T> {\n        Self::init(init)\n    }\n}","impl<T> From<*const T> for Atomic<T> {\n    /// Returns a new atomic pointer pointing to `raw`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use std::ptr;\n    /// use crossbeam_epoch::Atomic;\n    ///\n    /// let a = Atomic::<i32>::from(ptr::null::<i32>());\n    /// ```\n    fn from(raw: *const T) -> Self {\n        Self::from_usize(raw as usize)\n    }\n}","impl<T> From<Box<T>> for Atomic<T> {\n    fn from(b: Box<T>) -> Self {\n        Self::from(Owned::from(b))\n    }\n}","impl<T> From<T> for Atomic<T> {\n    fn from(t: T) -> Self {\n        Self::new(t)\n    }\n}","unsafe impl<T: ?Sized + Pointable + Send + Sync> Send for Atomic<T> {}","unsafe impl<T: ?Sized + Pointable + Send + Sync> Sync for Atomic<T> {}"],"atomic::CompareAndSetError":["impl<'g, T: 'g, P: Pointer<T> + fmt::Debug> fmt::Debug for CompareAndSetError<'g, T, P> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"CompareAndSetError\")\n            .field(\"current\", &self.current)\n            .field(\"new\", &self.new)\n            .finish()\n    }\n}"],"atomic::Owned":["impl<T: ?Sized + Pointable> AsMut<T> for Owned<T> {\n    fn as_mut(&mut self) -> &mut T {\n        self.deref_mut()\n    }\n}","impl<T: ?Sized + Pointable> AsRef<T> for Owned<T> {\n    fn as_ref(&self) -> &T {\n        self.deref()\n    }\n}","impl<T: ?Sized + Pointable> Borrow<T> for Owned<T> {\n    fn borrow(&self) -> &T {\n        self.deref()\n    }\n}","impl<T: ?Sized + Pointable> BorrowMut<T> for Owned<T> {\n    fn borrow_mut(&mut self) -> &mut T {\n        self.deref_mut()\n    }\n}","impl<T: ?Sized + Pointable> Deref for Owned<T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        unsafe { T::deref(raw) }\n    }\n}","impl<T: ?Sized + Pointable> DerefMut for Owned<T> {\n    fn deref_mut(&mut self) -> &mut T {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        unsafe { T::deref_mut(raw) }\n    }\n}","impl<T: ?Sized + Pointable> Drop for Owned<T> {\n    fn drop(&mut self) {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        unsafe {\n            T::drop(raw);\n        }\n    }\n}","impl<T: ?Sized + Pointable> Owned<T> {\n    /// Allocates `value` on the heap and returns a new owned pointer pointing to it.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// let o = Owned::<i32>::init(1234);\n    /// ```\n    pub fn init(init: T::Init) -> Owned<T> {\n        unsafe { Self::from_usize(T::init(init)) }\n    }\n\n    /// Converts the owned pointer into a [`Shared`].\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Owned};\n    ///\n    /// let o = Owned::new(1234);\n    /// let guard = &epoch::pin();\n    /// let p = o.into_shared(guard);\n    /// ```\n    ///\n    /// [`Shared`]: struct.Shared.html\n    #[allow(clippy::needless_lifetimes)]\n    pub fn into_shared<'g>(self, _: &'g Guard) -> Shared<'g, T> {\n        unsafe { Shared::from_usize(self.into_usize()) }\n    }\n\n    /// Returns the tag stored within the pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// assert_eq!(Owned::new(1234).tag(), 0);\n    /// ```\n    pub fn tag(&self) -> usize {\n        let (_, tag) = decompose_tag::<T>(self.data);\n        tag\n    }\n\n    /// Returns the same pointer, but tagged with `tag`. `tag` is truncated to be fit into the\n    /// unused bits of the pointer to `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// let o = Owned::new(0u64);\n    /// assert_eq!(o.tag(), 0);\n    /// let o = o.with_tag(2);\n    /// assert_eq!(o.tag(), 2);\n    /// ```\n    pub fn with_tag(self, tag: usize) -> Owned<T> {\n        let data = self.into_usize();\n        unsafe { Self::from_usize(compose_tag::<T>(data, tag)) }\n    }\n}","impl<T: ?Sized + Pointable> Pointer<T> for Owned<T> {\n    #[inline]\n    fn into_usize(self) -> usize {\n        let data = self.data;\n        mem::forget(self);\n        data\n    }\n\n    /// Returns a new pointer pointing to the tagged pointer `data`.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the data is zero in debug mode.\n    #[inline]\n    unsafe fn from_usize(data: usize) -> Self {\n        debug_assert!(data != 0, \"converting zero into `Owned`\");\n        Owned {\n            data,\n            _marker: PhantomData,\n        }\n    }\n}","impl<T: ?Sized + Pointable> fmt::Debug for Owned<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        let (raw, tag) = decompose_tag::<T>(self.data);\n\n        f.debug_struct(\"Owned\")\n            .field(\"raw\", &raw)\n            .field(\"tag\", &tag)\n            .finish()\n    }\n}","impl<T: Clone> Clone for Owned<T> {\n    fn clone(&self) -> Self {\n        Owned::new((**self).clone()).with_tag(self.tag())\n    }\n}","impl<T> From<Box<T>> for Owned<T> {\n    /// Returns a new owned pointer pointing to `b`.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the pointer (the `Box`) is not properly aligned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// let o = unsafe { Owned::from_raw(Box::into_raw(Box::new(1234))) };\n    /// ```\n    fn from(b: Box<T>) -> Self {\n        unsafe { Self::from_raw(Box::into_raw(b)) }\n    }\n}","impl<T> From<T> for Owned<T> {\n    fn from(t: T) -> Self {\n        Owned::new(t)\n    }\n}","impl<T> Owned<T> {\n    /// Returns a new owned pointer pointing to `raw`.\n    ///\n    /// This function is unsafe because improper use may lead to memory problems. Argument `raw`\n    /// must be a valid pointer. Also, a double-free may occur if the function is called twice on\n    /// the same raw pointer.\n    ///\n    /// # Panics\n    ///\n    /// Panics if `raw` is not properly aligned.\n    ///\n    /// # Safety\n    ///\n    /// The given `raw` should have been derived from `Owned`, and one `raw` should not be converted\n    /// back by `Owned::from_raw()` mutliple times.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// let o = unsafe { Owned::from_raw(Box::into_raw(Box::new(1234))) };\n    /// ```\n    pub unsafe fn from_raw(raw: *mut T) -> Owned<T> {\n        let raw = raw as usize;\n        ensure_aligned::<T>(raw);\n        Self::from_usize(raw)\n    }\n\n    /// Converts the owned pointer into a `Box`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// let o = Owned::new(1234);\n    /// let b: Box<i32> = o.into_box();\n    /// assert_eq!(*b, 1234);\n    /// ```\n    pub fn into_box(self) -> Box<T> {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        mem::forget(self);\n        unsafe { Box::from_raw(raw as *mut _) }\n    }\n\n    /// Allocates `value` on the heap and returns a new owned pointer pointing to it.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Owned;\n    ///\n    /// let o = Owned::new(1234);\n    /// ```\n    pub fn new(init: T) -> Owned<T> {\n        Self::init(init)\n    }\n}"],"atomic::Shared":["impl<'g, T: ?Sized + Pointable> PartialEq<Shared<'g, T>> for Shared<'g, T> {\n    fn eq(&self, other: &Self) -> bool {\n        self.data == other.data\n    }\n}","impl<'g, T: ?Sized + Pointable> PartialOrd<Shared<'g, T>> for Shared<'g, T> {\n    fn partial_cmp(&self, other: &Self) -> Option<cmp::Ordering> {\n        self.data.partial_cmp(&other.data)\n    }\n}","impl<'g, T: ?Sized + Pointable> Shared<'g, T> {\n    /// Returns a new null pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Shared;\n    ///\n    /// let p = Shared::<i32>::null();\n    /// assert!(p.is_null());\n    /// ```\n    pub fn null() -> Shared<'g, T> {\n        Shared {\n            data: 0,\n            _marker: PhantomData,\n        }\n    }\n\n    /// Returns `true` if the pointer is null.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::null();\n    /// let guard = &epoch::pin();\n    /// assert!(a.load(SeqCst, guard).is_null());\n    /// a.store(Owned::new(1234), SeqCst);\n    /// assert!(!a.load(SeqCst, guard).is_null());\n    /// ```\n    #[allow(clippy::trivially_copy_pass_by_ref)]\n    pub fn is_null(&self) -> bool {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        raw == 0\n    }\n\n    /// Dereferences the pointer.\n    ///\n    /// Returns a reference to the pointee that is valid during the lifetime `'g`.\n    ///\n    /// # Safety\n    ///\n    /// Dereferencing a pointer is unsafe because it could be pointing to invalid memory.\n    ///\n    /// Another concern is the possiblity of data races due to lack of proper synchronization.\n    /// For example, consider the following scenario:\n    ///\n    /// 1. A thread creates a new object: `a.store(Owned::new(10), Relaxed)`\n    /// 2. Another thread reads it: `*a.load(Relaxed, guard).as_ref().unwrap()`\n    ///\n    /// The problem is that relaxed orderings don't synchronize initialization of the object with\n    /// the read from the second thread. This is a data race. A possible solution would be to use\n    /// `Release` and `Acquire` orderings.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// let guard = &epoch::pin();\n    /// let p = a.load(SeqCst, guard);\n    /// unsafe {\n    ///     assert_eq!(p.deref(), &1234);\n    /// }\n    /// ```\n    #[allow(clippy::trivially_copy_pass_by_ref)]\n    #[allow(clippy::should_implement_trait)]\n    pub unsafe fn deref(&self) -> &'g T {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        T::deref(raw)\n    }\n\n    /// Dereferences the pointer.\n    ///\n    /// Returns a mutable reference to the pointee that is valid during the lifetime `'g`.\n    ///\n    /// # Safety\n    ///\n    /// * There is no guarantee that there are no more threads attempting to read/write from/to the\n    ///   actual object at the same time.\n    ///\n    ///   The user must know that there are no concurrent accesses towards the object itself.\n    ///\n    /// * Other than the above, all safety concerns of `deref()` applies here.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(vec![1, 2, 3, 4]);\n    /// let guard = &epoch::pin();\n    ///\n    /// let mut p = a.load(SeqCst, guard);\n    /// unsafe {\n    ///     assert!(!p.is_null());\n    ///     let b = p.deref_mut();\n    ///     assert_eq!(b, &vec![1, 2, 3, 4]);\n    ///     b.push(5);\n    ///     assert_eq!(b, &vec![1, 2, 3, 4, 5]);\n    /// }\n    ///\n    /// let p = a.load(SeqCst, guard);\n    /// unsafe {\n    ///     assert_eq!(p.deref(), &vec![1, 2, 3, 4, 5]);\n    /// }\n    /// ```\n    #[allow(clippy::should_implement_trait)]\n    pub unsafe fn deref_mut(&mut self) -> &'g mut T {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        T::deref_mut(raw)\n    }\n\n    /// Converts the pointer to a reference.\n    ///\n    /// Returns `None` if the pointer is null, or else a reference to the object wrapped in `Some`.\n    ///\n    /// # Safety\n    ///\n    /// Dereferencing a pointer is unsafe because it could be pointing to invalid memory.\n    ///\n    /// Another concern is the possiblity of data races due to lack of proper synchronization.\n    /// For example, consider the following scenario:\n    ///\n    /// 1. A thread creates a new object: `a.store(Owned::new(10), Relaxed)`\n    /// 2. Another thread reads it: `*a.load(Relaxed, guard).as_ref().unwrap()`\n    ///\n    /// The problem is that relaxed orderings don't synchronize initialization of the object with\n    /// the read from the second thread. This is a data race. A possible solution would be to use\n    /// `Release` and `Acquire` orderings.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// let guard = &epoch::pin();\n    /// let p = a.load(SeqCst, guard);\n    /// unsafe {\n    ///     assert_eq!(p.as_ref(), Some(&1234));\n    /// }\n    /// ```\n    #[allow(clippy::trivially_copy_pass_by_ref)]\n    pub unsafe fn as_ref(&self) -> Option<&'g T> {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        if raw == 0 {\n            None\n        } else {\n            Some(T::deref(raw))\n        }\n    }\n\n    /// Takes ownership of the pointee.\n    ///\n    /// # Panics\n    ///\n    /// Panics if this pointer is null, but only in debug mode.\n    ///\n    /// # Safety\n    ///\n    /// This method may be called only if the pointer is valid and nobody else is holding a\n    /// reference to the same object.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(1234);\n    /// unsafe {\n    ///     let guard = &epoch::unprotected();\n    ///     let p = a.load(SeqCst, guard);\n    ///     drop(p.into_owned());\n    /// }\n    /// ```\n    pub unsafe fn into_owned(self) -> Owned<T> {\n        debug_assert!(!self.is_null(), \"converting a null `Shared` into `Owned`\");\n        Owned::from_usize(self.data)\n    }\n\n    /// Returns the tag stored within the pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::<u64>::from(Owned::new(0u64).with_tag(2));\n    /// let guard = &epoch::pin();\n    /// let p = a.load(SeqCst, guard);\n    /// assert_eq!(p.tag(), 2);\n    /// ```\n    #[allow(clippy::trivially_copy_pass_by_ref)]\n    pub fn tag(&self) -> usize {\n        let (_, tag) = decompose_tag::<T>(self.data);\n        tag\n    }\n\n    /// Returns the same pointer, but tagged with `tag`. `tag` is truncated to be fit into the\n    /// unused bits of the pointer to `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(0u64);\n    /// let guard = &epoch::pin();\n    /// let p1 = a.load(SeqCst, guard);\n    /// let p2 = p1.with_tag(2);\n    ///\n    /// assert_eq!(p1.tag(), 0);\n    /// assert_eq!(p2.tag(), 2);\n    /// assert_eq!(p1.as_raw(), p2.as_raw());\n    /// ```\n    #[allow(clippy::trivially_copy_pass_by_ref)]\n    pub fn with_tag(&self, tag: usize) -> Shared<'g, T> {\n        unsafe { Self::from_usize(compose_tag::<T>(self.data, tag)) }\n    }\n}","impl<'g, T> Shared<'g, T> {\n    /// Converts the pointer to a raw pointer (without the tag).\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let o = Owned::new(1234);\n    /// let raw = &*o as *const _;\n    /// let a = Atomic::from(o);\n    ///\n    /// let guard = &epoch::pin();\n    /// let p = a.load(SeqCst, guard);\n    /// assert_eq!(p.as_raw(), raw);\n    /// ```\n    #[allow(clippy::trivially_copy_pass_by_ref)]\n    pub fn as_raw(&self) -> *const T {\n        let (raw, _) = decompose_tag::<T>(self.data);\n        raw as *const _\n    }\n}","impl<T: ?Sized + Pointable> Clone for Shared<'_, T> {\n    fn clone(&self) -> Self {\n        Self {\n            data: self.data,\n            _marker: PhantomData,\n        }\n    }\n}","impl<T: ?Sized + Pointable> Copy for Shared<'_, T> {}","impl<T: ?Sized + Pointable> Default for Shared<'_, T> {\n    fn default() -> Self {\n        Shared::null()\n    }\n}","impl<T: ?Sized + Pointable> Eq for Shared<'_, T> {}","impl<T: ?Sized + Pointable> Ord for Shared<'_, T> {\n    fn cmp(&self, other: &Self) -> cmp::Ordering {\n        self.data.cmp(&other.data)\n    }\n}","impl<T: ?Sized + Pointable> Pointer<T> for Shared<'_, T> {\n    #[inline]\n    fn into_usize(self) -> usize {\n        self.data\n    }\n\n    #[inline]\n    unsafe fn from_usize(data: usize) -> Self {\n        Shared {\n            data,\n            _marker: PhantomData,\n        }\n    }\n}","impl<T: ?Sized + Pointable> fmt::Debug for Shared<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        let (raw, tag) = decompose_tag::<T>(self.data);\n\n        f.debug_struct(\"Shared\")\n            .field(\"raw\", &raw)\n            .field(\"tag\", &tag)\n            .finish()\n    }\n}","impl<T: ?Sized + Pointable> fmt::Pointer for Shared<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        fmt::Pointer::fmt(&(unsafe { self.deref() as *const _ }), f)\n    }\n}","impl<T> From<*const T> for Shared<'_, T> {\n    /// Returns a new pointer pointing to `raw`.\n    ///\n    /// # Panics\n    ///\n    /// Panics if `raw` is not properly aligned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::Shared;\n    ///\n    /// let p = Shared::from(Box::into_raw(Box::new(1234)) as *const _);\n    /// assert!(!p.is_null());\n    /// ```\n    fn from(raw: *const T) -> Self {\n        let raw = raw as usize;\n        ensure_aligned::<T>(raw);\n        unsafe { Self::from_usize(raw) }\n    }\n}"],"collector::Collector":["impl Clone for Collector {\n    /// Creates another reference to the same garbage collector.\n    fn clone(&self) -> Self {\n        Collector {\n            global: self.global.clone(),\n        }\n    }\n}","impl Collector {\n    /// Creates a new collector.\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Registers a new handle for the collector.\n    pub fn register(&self) -> LocalHandle {\n        Local::register(self)\n    }\n}","impl Default for Collector {\n    fn default() -> Self {\n        Self {\n            global: Arc::new(Global::new()),\n        }\n    }\n}","impl Eq for Collector {}","impl PartialEq for Collector {\n    /// Checks if both handles point to the same collector.\n    fn eq(&self, rhs: &Collector) -> bool {\n        Arc::ptr_eq(&self.global, &rhs.global)\n    }\n}","impl fmt::Debug for Collector {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Collector { .. }\")\n    }\n}","unsafe impl Send for Collector {}","unsafe impl Sync for Collector {}"],"collector::LocalHandle":["impl Drop for LocalHandle {\n    #[inline]\n    fn drop(&mut self) {\n        unsafe {\n            Local::release_handle(&*self.local);\n        }\n    }\n}","impl LocalHandle {\n    /// Pins the handle.\n    #[inline]\n    pub fn pin(&self) -> Guard {\n        unsafe { (*self.local).pin() }\n    }\n\n    /// Returns `true` if the handle is pinned.\n    #[inline]\n    pub fn is_pinned(&self) -> bool {\n        unsafe { (*self.local).is_pinned() }\n    }\n\n    /// Returns the `Collector` associated with this handle.\n    #[inline]\n    pub fn collector(&self) -> &Collector {\n        unsafe { (*self.local).collector() }\n    }\n}","impl fmt::Debug for LocalHandle {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"LocalHandle { .. }\")\n    }\n}"],"default::COLLECTOR":["impl $crate::LazyStatic for $N {\n            fn initialize(lazy: &Self) {\n                let _ = &**lazy;\n            }\n        }","impl $crate::__Deref for $N {\n            type Target = $T;\n            fn deref(&self) -> &$T {\n                #[inline(always)]\n                fn __static_ref_initialize() -> $T { $e }\n\n                #[inline(always)]\n                fn __stability() -> &'static $T {\n                    __lazy_static_create!(LAZY, $T);\n                    LAZY.get(__static_ref_initialize)\n                }\n                __stability()\n            }\n        }"],"deferred::Deferred":["impl Deferred {\n    /// Constructs a new `Deferred` from a `FnOnce()`.\n    pub fn new<F: FnOnce()>(f: F) -> Self {\n        let size = mem::size_of::<F>();\n        let align = mem::align_of::<F>();\n\n        unsafe {\n            if size <= mem::size_of::<Data>() && align <= mem::align_of::<Data>() {\n                let mut data = MaybeUninit::<Data>::uninit();\n                ptr::write(data.as_mut_ptr() as *mut F, f);\n\n                unsafe fn call<F: FnOnce()>(raw: *mut u8) {\n                    let f: F = ptr::read(raw as *mut F);\n                    f();\n                }\n\n                Deferred {\n                    call: call::<F>,\n                    data: data.assume_init(),\n                    _marker: PhantomData,\n                }\n            } else {\n                let b: Box<F> = Box::new(f);\n                let mut data = MaybeUninit::<Data>::uninit();\n                ptr::write(data.as_mut_ptr() as *mut Box<F>, b);\n\n                unsafe fn call<F: FnOnce()>(raw: *mut u8) {\n                    // It's safe to cast `raw` from `*mut u8` to `*mut Box<F>`, because `raw` is\n                    // originally derived from `*mut Box<F>`.\n                    #[allow(clippy::cast_ptr_alignment)]\n                    let b: Box<F> = ptr::read(raw as *mut Box<F>);\n                    (*b)();\n                }\n\n                Deferred {\n                    call: call::<F>,\n                    data: data.assume_init(),\n                    _marker: PhantomData,\n                }\n            }\n        }\n    }\n\n    /// Calls the function.\n    #[inline]\n    pub fn call(mut self) {\n        let call = self.call;\n        unsafe { call(&mut self.data as *mut Data as *mut u8) };\n    }\n}","impl fmt::Debug for Deferred {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> Result<(), fmt::Error> {\n        f.pad(\"Deferred { .. }\")\n    }\n}"],"epoch::AtomicEpoch":["Debug","Default","impl AtomicEpoch {\n    /// Creates a new atomic epoch.\n    #[inline]\n    pub fn new(epoch: Epoch) -> Self {\n        let data = AtomicUsize::new(epoch.data);\n        AtomicEpoch { data }\n    }\n\n    /// Loads a value from the atomic epoch.\n    #[inline]\n    pub fn load(&self, ord: Ordering) -> Epoch {\n        Epoch {\n            data: self.data.load(ord),\n        }\n    }\n\n    /// Stores a value into the atomic epoch.\n    #[inline]\n    pub fn store(&self, epoch: Epoch, ord: Ordering) {\n        self.data.store(epoch.data, ord);\n    }\n\n    /// Stores a value into the atomic epoch if the current value is the same as `current`.\n    ///\n    /// The return value is always the previous value. If it is equal to `current`, then the value\n    /// is updated.\n    ///\n    /// The `Ordering` argument describes the memory ordering of this operation.\n    #[inline]\n    pub fn compare_and_swap(&self, current: Epoch, new: Epoch, ord: Ordering) -> Epoch {\n        let data = self.data.compare_and_swap(current.data, new.data, ord);\n        Epoch { data }\n    }\n}"],"epoch::Epoch":["Clone","Copy","Debug","Default","Eq","PartialEq","impl Epoch {\n    /// Returns the starting epoch in unpinned state.\n    #[inline]\n    pub fn starting() -> Self {\n        Self::default()\n    }\n\n    /// Returns the number of epochs `self` is ahead of `rhs`.\n    ///\n    /// Internally, epochs are represented as numbers in the range `(isize::MIN / 2) .. (isize::MAX\n    /// / 2)`, so the returned distance will be in the same interval.\n    pub fn wrapping_sub(self, rhs: Self) -> isize {\n        // The result is the same with `(self.data & !1).wrapping_sub(rhs.data & !1) as isize >> 1`,\n        // because the possible difference of LSB in `(self.data & !1).wrapping_sub(rhs.data & !1)`\n        // will be ignored in the shift operation.\n        self.data.wrapping_sub(rhs.data & !1) as isize >> 1\n    }\n\n    /// Returns `true` if the epoch is marked as pinned.\n    #[inline]\n    pub fn is_pinned(self) -> bool {\n        (self.data & 1) == 1\n    }\n\n    /// Returns the same epoch, but marked as pinned.\n    #[inline]\n    pub fn pinned(self) -> Epoch {\n        Epoch {\n            data: self.data | 1,\n        }\n    }\n\n    /// Returns the same epoch, but marked as unpinned.\n    #[inline]\n    pub fn unpinned(self) -> Epoch {\n        Epoch {\n            data: self.data & !1,\n        }\n    }\n\n    /// Returns the successor epoch.\n    ///\n    /// The returned epoch will be marked as pinned only if the previous one was as well.\n    #[inline]\n    pub fn successor(self) -> Epoch {\n        Epoch {\n            data: self.data.wrapping_add(2),\n        }\n    }\n}"],"guard::Guard":["impl Drop for Guard {\n    #[inline]\n    fn drop(&mut self) {\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.unpin();\n        }\n    }\n}","impl Guard {\n    /// Stores a function so that it can be executed at some point after all currently pinned\n    /// threads get unpinned.\n    ///\n    /// This method first stores `f` into the thread-local (or handle-local) cache. If this cache\n    /// becomes full, some functions are moved into the global cache. At the same time, some\n    /// functions from both local and global caches may get executed in order to incrementally\n    /// clean up the caches as they fill up.\n    ///\n    /// There is no guarantee when exactly `f` will be executed. The only guarantee is that it\n    /// won't be executed until all currently pinned threads get unpinned. In theory, `f` might\n    /// never run, but the epoch-based garbage collection will make an effort to execute it\n    /// reasonably soon.\n    ///\n    /// If this method is called from an [`unprotected`] guard, the function will simply be\n    /// executed immediately.\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub fn defer<F, R>(&self, f: F)\n    where\n        F: FnOnce() -> R,\n        F: Send + 'static,\n    {\n        unsafe {\n            self.defer_unchecked(f);\n        }\n    }\n\n    /// Stores a function so that it can be executed at some point after all currently pinned\n    /// threads get unpinned.\n    ///\n    /// This method first stores `f` into the thread-local (or handle-local) cache. If this cache\n    /// becomes full, some functions are moved into the global cache. At the same time, some\n    /// functions from both local and global caches may get executed in order to incrementally\n    /// clean up the caches as they fill up.\n    ///\n    /// There is no guarantee when exactly `f` will be executed. The only guarantee is that it\n    /// won't be executed until all currently pinned threads get unpinned. In theory, `f` might\n    /// never run, but the epoch-based garbage collection will make an effort to execute it\n    /// reasonably soon.\n    ///\n    /// If this method is called from an [`unprotected`] guard, the function will simply be\n    /// executed immediately.\n    ///\n    /// # Safety\n    ///\n    /// The given function must not hold reference onto the stack. It is highly recommended that\n    /// the passed function is **always** marked with `move` in order to prevent accidental\n    /// borrows.\n    ///\n    /// ```\n    /// use crossbeam_epoch as epoch;\n    ///\n    /// let guard = &epoch::pin();\n    /// let message = \"Hello!\";\n    /// unsafe {\n    ///     // ALWAYS use `move` when sending a closure into `defer_unchecked`.\n    ///     guard.defer_unchecked(move || {\n    ///         println!(\"{}\", message);\n    ///     });\n    /// }\n    /// ```\n    ///\n    /// Apart from that, keep in mind that another thread may execute `f`, so anything accessed by\n    /// the closure must be `Send`.\n    ///\n    /// We intentionally didn't require `F: Send`, because Rust's type systems usually cannot prove\n    /// `F: Send` for typical use cases. For example, consider the following code snippet, which\n    /// exemplifies the typical use case of deferring the deallocation of a shared reference:\n    ///\n    /// ```ignore\n    /// let shared = Owned::new(7i32).into_shared(guard);\n    /// guard.defer_unchecked(move || shared.into_owned()); // `Shared` is not `Send`!\n    /// ```\n    ///\n    /// While `Shared` is not `Send`, it's safe for another thread to call the deferred function,\n    /// because it's called only after the grace period and `shared` is no longer shared with other\n    /// threads. But we don't expect type systems to prove this.\n    ///\n    /// # Examples\n    ///\n    /// When a heap-allocated object in a data structure becomes unreachable, it has to be\n    /// deallocated. However, the current thread and other threads may be still holding references\n    /// on the stack to that same object. Therefore it cannot be deallocated before those references\n    /// get dropped. This method can defer deallocation until all those threads get unpinned and\n    /// consequently drop all their references on the stack.\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(\"foo\");\n    ///\n    /// // Now suppose that `a` is shared among multiple threads and concurrently\n    /// // accessed and modified...\n    ///\n    /// // Pin the current thread.\n    /// let guard = &epoch::pin();\n    ///\n    /// // Steal the object currently stored in `a` and swap it with another one.\n    /// let p = a.swap(Owned::new(\"bar\").into_shared(guard), SeqCst, guard);\n    ///\n    /// if !p.is_null() {\n    ///     // The object `p` is pointing to is now unreachable.\n    ///     // Defer its deallocation until all currently pinned threads get unpinned.\n    ///     unsafe {\n    ///         // ALWAYS use `move` when sending a closure into `defer_unchecked`.\n    ///         guard.defer_unchecked(move || {\n    ///             println!(\"{} is now being deallocated.\", p.deref());\n    ///             // Now we have unique access to the object pointed to by `p` and can turn it\n    ///             // into an `Owned`. Dropping the `Owned` will deallocate the object.\n    ///             drop(p.into_owned());\n    ///         });\n    ///     }\n    /// }\n    /// ```\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub unsafe fn defer_unchecked<F, R>(&self, f: F)\n    where\n        F: FnOnce() -> R,\n    {\n        if let Some(local) = self.local.as_ref() {\n            local.defer(Deferred::new(move || drop(f())), self);\n        } else {\n            drop(f());\n        }\n    }\n\n    /// Stores a destructor for an object so that it can be deallocated and dropped at some point\n    /// after all currently pinned threads get unpinned.\n    ///\n    /// This method first stores the destructor into the thread-local (or handle-local) cache. If\n    /// this cache becomes full, some destructors are moved into the global cache. At the same\n    /// time, some destructors from both local and global caches may get executed in order to\n    /// incrementally clean up the caches as they fill up.\n    ///\n    /// There is no guarantee when exactly the destructor will be executed. The only guarantee is\n    /// that it won't be executed until all currently pinned threads get unpinned. In theory, the\n    /// destructor might never run, but the epoch-based garbage collection will make an effort to\n    /// execute it reasonably soon.\n    ///\n    /// If this method is called from an [`unprotected`] guard, the destructor will simply be\n    /// executed immediately.\n    ///\n    /// # Safety\n    ///\n    /// The object must not be reachable by other threads anymore, otherwise it might be still in\n    /// use when the destructor runs.\n    ///\n    /// Apart from that, keep in mind that another thread may execute the destructor, so the object\n    /// must be sendable to other threads.\n    ///\n    /// We intentionally didn't require `T: Send`, because Rust's type systems usually cannot prove\n    /// `T: Send` for typical use cases. For example, consider the following code snippet, which\n    /// exemplifies the typical use case of deferring the deallocation of a shared reference:\n    ///\n    /// ```ignore\n    /// let shared = Owned::new(7i32).into_shared(guard);\n    /// guard.defer_destroy(shared); // `Shared` is not `Send`!\n    /// ```\n    ///\n    /// While `Shared` is not `Send`, it's safe for another thread to call the destructor, because\n    /// it's called only after the grace period and `shared` is no longer shared with other\n    /// threads. But we don't expect type systems to prove this.\n    ///\n    /// # Examples\n    ///\n    /// When a heap-allocated object in a data structure becomes unreachable, it has to be\n    /// deallocated. However, the current thread and other threads may be still holding references\n    /// on the stack to that same object. Therefore it cannot be deallocated before those references\n    /// get dropped. This method can defer deallocation until all those threads get unpinned and\n    /// consequently drop all their references on the stack.\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(\"foo\");\n    ///\n    /// // Now suppose that `a` is shared among multiple threads and concurrently\n    /// // accessed and modified...\n    ///\n    /// // Pin the current thread.\n    /// let guard = &epoch::pin();\n    ///\n    /// // Steal the object currently stored in `a` and swap it with another one.\n    /// let p = a.swap(Owned::new(\"bar\").into_shared(guard), SeqCst, guard);\n    ///\n    /// if !p.is_null() {\n    ///     // The object `p` is pointing to is now unreachable.\n    ///     // Defer its deallocation until all currently pinned threads get unpinned.\n    ///     unsafe {\n    ///         guard.defer_destroy(p);\n    ///     }\n    /// }\n    /// ```\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub unsafe fn defer_destroy<T>(&self, ptr: Shared<'_, T>) {\n        self.defer_unchecked(move || ptr.into_owned());\n    }\n\n    /// Clears up the thread-local cache of deferred functions by executing them or moving into the\n    /// global cache.\n    ///\n    /// Call this method after deferring execution of a function if you want to get it executed as\n    /// soon as possible. Flushing will make sure it is residing in in the global cache, so that\n    /// any thread has a chance of taking the function and executing it.\n    ///\n    /// If this method is called from an [`unprotected`] guard, it is a no-op (nothing happens).\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch as epoch;\n    ///\n    /// let guard = &epoch::pin();\n    /// guard.defer(move || {\n    ///     println!(\"This better be printed as soon as possible!\");\n    /// });\n    /// guard.flush();\n    /// ```\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub fn flush(&self) {\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.flush(self);\n        }\n    }\n\n    /// Unpins and then immediately re-pins the thread.\n    ///\n    /// This method is useful when you don't want delay the advancement of the global epoch by\n    /// holding an old epoch. For safety, you should not maintain any guard-based reference across\n    /// the call (the latter is enforced by `&mut self`). The thread will only be repinned if this\n    /// is the only active guard for the current thread.\n    ///\n    /// If this method is called from an [`unprotected`] guard, then the call will be just no-op.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// let a = Atomic::new(777);\n    /// let mut guard = epoch::pin();\n    /// {\n    ///     let p = a.load(SeqCst, &guard);\n    ///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n    /// }\n    /// guard.repin();\n    /// {\n    ///     let p = a.load(SeqCst, &guard);\n    ///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n    /// }\n    /// ```\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub fn repin(&mut self) {\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.repin();\n        }\n    }\n\n    /// Temporarily unpins the thread, executes the given function and then re-pins the thread.\n    ///\n    /// This method is useful when you need to perform a long-running operation (e.g. sleeping)\n    /// and don't need to maintain any guard-based reference across the call (the latter is enforced\n    /// by `&mut self`). The thread will only be unpinned if this is the only active guard for the\n    /// current thread.\n    ///\n    /// If this method is called from an [`unprotected`] guard, then the passed function is called\n    /// directly without unpinning the thread.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch::{self as epoch, Atomic};\n    /// use std::sync::atomic::Ordering::SeqCst;\n    /// use std::thread;\n    /// use std::time::Duration;\n    ///\n    /// let a = Atomic::new(777);\n    /// let mut guard = epoch::pin();\n    /// {\n    ///     let p = a.load(SeqCst, &guard);\n    ///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n    /// }\n    /// guard.repin_after(|| thread::sleep(Duration::from_millis(50)));\n    /// {\n    ///     let p = a.load(SeqCst, &guard);\n    ///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n    /// }\n    /// ```\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub fn repin_after<F, R>(&mut self, f: F) -> R\n    where\n        F: FnOnce() -> R,\n    {\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            // We need to acquire a handle here to ensure the Local doesn't\n            // disappear from under us.\n            local.acquire_handle();\n            local.unpin();\n        }\n\n        // Ensure the Guard is re-pinned even if the function panics\n        defer! {\n            if let Some(local) = unsafe { self.local.as_ref() } {\n                mem::forget(local.pin());\n                local.release_handle();\n            }\n        }\n\n        f()\n    }\n\n    /// Returns the `Collector` associated with this guard.\n    ///\n    /// This method is useful when you need to ensure that all guards used with\n    /// a data structure come from the same collector.\n    ///\n    /// If this method is called from an [`unprotected`] guard, then `None` is returned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_epoch as epoch;\n    ///\n    /// let guard1 = epoch::pin();\n    /// let guard2 = epoch::pin();\n    /// assert!(guard1.collector() == guard2.collector());\n    /// ```\n    ///\n    /// [`unprotected`]: fn.unprotected.html\n    pub fn collector(&self) -> Option<&Collector> {\n        unsafe { self.local.as_ref().map(|local| local.collector()) }\n    }\n}","impl fmt::Debug for Guard {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Guard { .. }\")\n    }\n}"],"internal::Bag":["impl Bag {\n    /// Returns a new, empty bag.\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Returns `true` if the bag is empty.\n    pub fn is_empty(&self) -> bool {\n        self.len == 0\n    }\n\n    /// Attempts to insert a deferred function into the bag.\n    ///\n    /// Returns `Ok(())` if successful, and `Err(deferred)` for the given `deferred` if the bag is\n    /// full.\n    ///\n    /// # Safety\n    ///\n    /// It should be safe for another thread to execute the given function.\n    pub unsafe fn try_push(&mut self, deferred: Deferred) -> Result<(), Deferred> {\n        if self.len < MAX_OBJECTS {\n            self.deferreds[self.len] = deferred;\n            self.len += 1;\n            Ok(())\n        } else {\n            Err(deferred)\n        }\n    }\n\n    /// Seals the bag with the given epoch.\n    fn seal(self, epoch: Epoch) -> SealedBag {\n        SealedBag { epoch, bag: self }\n    }\n}","impl Default for Bag {\n    #[rustfmt::skip]\n    fn default() -> Self {\n        // TODO: [no_op; MAX_OBJECTS] syntax blocked by https://github.com/rust-lang/rust/issues/49147\n        #[cfg(not(feature = \"sanitize\"))]\n        return Bag {\n            len: 0,\n            deferreds: [\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n            ],\n        };\n        #[cfg(feature = \"sanitize\")]\n        return Bag {\n            len: 0,\n            deferreds: [\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n            ],\n        };\n    }\n}","impl Drop for Bag {\n    fn drop(&mut self) {\n        // Call all deferred functions.\n        for deferred in &mut self.deferreds[..self.len] {\n            let no_op = Deferred::new(no_op_func);\n            let owned_deferred = mem::replace(deferred, no_op);\n            owned_deferred.call();\n        }\n    }\n}","impl fmt::Debug for Bag {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"Bag\")\n            .field(\"deferreds\", &&self.deferreds[..self.len])\n            .finish()\n    }\n}","unsafe impl Send for Bag {}"],"internal::Global":["impl Global {\n    /// Number of bags to destroy.\n    const COLLECT_STEPS: usize = 8;\n\n    /// Creates a new global data for garbage collection.\n    #[inline]\n    pub fn new() -> Self {\n        Self {\n            locals: List::new(),\n            queue: Queue::new(),\n            epoch: CachePadded::new(AtomicEpoch::new(Epoch::starting())),\n        }\n    }\n\n    /// Pushes the bag into the global queue and replaces the bag with a new empty bag.\n    pub fn push_bag(&self, bag: &mut Bag, guard: &Guard) {\n        let bag = mem::replace(bag, Bag::new());\n\n        atomic::fence(Ordering::SeqCst);\n\n        let epoch = self.epoch.load(Ordering::Relaxed);\n        self.queue.push(bag.seal(epoch), guard);\n    }\n\n    /// Collects several bags from the global queue and executes deferred functions in them.\n    ///\n    /// Note: This may itself produce garbage and in turn allocate new bags.\n    ///\n    /// `pin()` rarely calls `collect()`, so we want the compiler to place that call on a cold\n    /// path. In other words, we want the compiler to optimize branching for the case when\n    /// `collect()` is not called.\n    #[cold]\n    pub fn collect(&self, guard: &Guard) {\n        let global_epoch = self.try_advance(guard);\n\n        let steps = if cfg!(feature = \"sanitize\") {\n            usize::max_value()\n        } else {\n            Self::COLLECT_STEPS\n        };\n\n        for _ in 0..steps {\n            match self.queue.try_pop_if(\n                &|sealed_bag: &SealedBag| sealed_bag.is_expired(global_epoch),\n                guard,\n            ) {\n                None => break,\n                Some(sealed_bag) => drop(sealed_bag),\n            }\n        }\n    }\n\n    /// Attempts to advance the global epoch.\n    ///\n    /// The global epoch can advance only if all currently pinned participants have been pinned in\n    /// the current epoch.\n    ///\n    /// Returns the current global epoch.\n    ///\n    /// `try_advance()` is annotated `#[cold]` because it is rarely called.\n    #[cold]\n    pub fn try_advance(&self, guard: &Guard) -> Epoch {\n        let global_epoch = self.epoch.load(Ordering::Relaxed);\n        atomic::fence(Ordering::SeqCst);\n\n        // TODO(stjepang): `Local`s are stored in a linked list because linked lists are fairly\n        // easy to implement in a lock-free manner. However, traversal can be slow due to cache\n        // misses and data dependencies. We should experiment with other data structures as well.\n        for local in self.locals.iter(&guard) {\n            match local {\n                Err(IterError::Stalled) => {\n                    // A concurrent thread stalled this iteration. That thread might also try to\n                    // advance the epoch, in which case we leave the job to it. Otherwise, the\n                    // epoch will not be advanced.\n                    return global_epoch;\n                }\n                Ok(local) => {\n                    let local_epoch = local.epoch.load(Ordering::Relaxed);\n\n                    // If the participant was pinned in a different epoch, we cannot advance the\n                    // global epoch just yet.\n                    if local_epoch.is_pinned() && local_epoch.unpinned() != global_epoch {\n                        return global_epoch;\n                    }\n                }\n            }\n        }\n        atomic::fence(Ordering::Acquire);\n\n        // All pinned participants were pinned in the current global epoch.\n        // Now let's advance the global epoch...\n        //\n        // Note that if another thread already advanced it before us, this store will simply\n        // overwrite the global epoch with the same value. This is true because `try_advance` was\n        // called from a thread that was pinned in `global_epoch`, and the global epoch cannot be\n        // advanced two steps ahead of it.\n        let new_epoch = global_epoch.successor();\n        self.epoch.store(new_epoch, Ordering::Release);\n        new_epoch\n    }\n}"],"internal::Local":["impl IsElement<Local> for Local {\n    fn entry_of(local: &Local) -> &Entry {\n        let entry_ptr = (local as *const Local as usize + offset_of!(Local, entry)) as *const Entry;\n        unsafe { &*entry_ptr }\n    }\n\n    unsafe fn element_of(entry: &Entry) -> &Local {\n        // offset_of! macro uses unsafe, but it's unnecessary in this context.\n        #[allow(unused_unsafe)]\n        let local_ptr = (entry as *const Entry as usize - offset_of!(Local, entry)) as *const Local;\n        &*local_ptr\n    }\n\n    unsafe fn finalize(entry: &Entry, guard: &Guard) {\n        guard.defer_destroy(Shared::from(Self::element_of(entry) as *const _));\n    }\n}","impl Local {\n    /// Number of pinnings after which a participant will execute some deferred functions from the\n    /// global queue.\n    const PINNINGS_BETWEEN_COLLECT: usize = 128;\n\n    /// Registers a new `Local` in the provided `Global`.\n    pub fn register(collector: &Collector) -> LocalHandle {\n        unsafe {\n            // Since we dereference no pointers in this block, it is safe to use `unprotected`.\n\n            let local = Owned::new(Local {\n                entry: Entry::default(),\n                epoch: AtomicEpoch::new(Epoch::starting()),\n                collector: UnsafeCell::new(ManuallyDrop::new(collector.clone())),\n                bag: UnsafeCell::new(Bag::new()),\n                guard_count: Cell::new(0),\n                handle_count: Cell::new(1),\n                pin_count: Cell::new(Wrapping(0)),\n            })\n            .into_shared(unprotected());\n            collector.global.locals.insert(local, unprotected());\n            LocalHandle {\n                local: local.as_raw(),\n            }\n        }\n    }\n\n    /// Returns a reference to the `Global` in which this `Local` resides.\n    #[inline]\n    pub fn global(&self) -> &Global {\n        &self.collector().global\n    }\n\n    /// Returns a reference to the `Collector` in which this `Local` resides.\n    #[inline]\n    pub fn collector(&self) -> &Collector {\n        unsafe { &**self.collector.get() }\n    }\n\n    /// Returns `true` if the current participant is pinned.\n    #[inline]\n    pub fn is_pinned(&self) -> bool {\n        self.guard_count.get() > 0\n    }\n\n    /// Adds `deferred` to the thread-local bag.\n    ///\n    /// # Safety\n    ///\n    /// It should be safe for another thread to execute the given function.\n    pub unsafe fn defer(&self, mut deferred: Deferred, guard: &Guard) {\n        let bag = &mut *self.bag.get();\n\n        while let Err(d) = bag.try_push(deferred) {\n            self.global().push_bag(bag, guard);\n            deferred = d;\n        }\n    }\n\n    pub fn flush(&self, guard: &Guard) {\n        let bag = unsafe { &mut *self.bag.get() };\n\n        if !bag.is_empty() {\n            self.global().push_bag(bag, guard);\n        }\n\n        self.global().collect(guard);\n    }\n\n    /// Pins the `Local`.\n    #[inline]\n    pub fn pin(&self) -> Guard {\n        let guard = Guard { local: self };\n\n        let guard_count = self.guard_count.get();\n        self.guard_count.set(guard_count.checked_add(1).unwrap());\n\n        if guard_count == 0 {\n            let global_epoch = self.global().epoch.load(Ordering::Relaxed);\n            let new_epoch = global_epoch.pinned();\n\n            // Now we must store `new_epoch` into `self.epoch` and execute a `SeqCst` fence.\n            // The fence makes sure that any future loads from `Atomic`s will not happen before\n            // this store.\n            if cfg!(any(target_arch = \"x86\", target_arch = \"x86_64\")) {\n                // HACK(stjepang): On x86 architectures there are two different ways of executing\n                // a `SeqCst` fence.\n                //\n                // 1. `atomic::fence(SeqCst)`, which compiles into a `mfence` instruction.\n                // 2. `_.compare_and_swap(_, _, SeqCst)`, which compiles into a `lock cmpxchg`\n                //    instruction.\n                //\n                // Both instructions have the effect of a full barrier, but benchmarks have shown\n                // that the second one makes pinning faster in this particular case.  It is not\n                // clear that this is permitted by the C++ memory model (SC fences work very\n                // differently from SC accesses), but experimental evidence suggests that this\n                // works fine.  Using inline assembly would be a viable (and correct) alternative,\n                // but alas, that is not possible on stable Rust.\n                let current = Epoch::starting();\n                let previous = self\n                    .epoch\n                    .compare_and_swap(current, new_epoch, Ordering::SeqCst);\n                debug_assert_eq!(current, previous, \"participant was expected to be unpinned\");\n                // We add a compiler fence to make it less likely for LLVM to do something wrong\n                // here.  Formally, this is not enough to get rid of data races; practically,\n                // it should go a long way.\n                atomic::compiler_fence(Ordering::SeqCst);\n            } else {\n                self.epoch.store(new_epoch, Ordering::Relaxed);\n                atomic::fence(Ordering::SeqCst);\n            }\n\n            // Increment the pin counter.\n            let count = self.pin_count.get();\n            self.pin_count.set(count + Wrapping(1));\n\n            // After every `PINNINGS_BETWEEN_COLLECT` try advancing the epoch and collecting\n            // some garbage.\n            if count.0 % Self::PINNINGS_BETWEEN_COLLECT == 0 {\n                self.global().collect(&guard);\n            }\n        }\n\n        guard\n    }\n\n    /// Unpins the `Local`.\n    #[inline]\n    pub fn unpin(&self) {\n        let guard_count = self.guard_count.get();\n        self.guard_count.set(guard_count - 1);\n\n        if guard_count == 1 {\n            self.epoch.store(Epoch::starting(), Ordering::Release);\n\n            if self.handle_count.get() == 0 {\n                self.finalize();\n            }\n        }\n    }\n\n    /// Unpins and then pins the `Local`.\n    #[inline]\n    pub fn repin(&self) {\n        let guard_count = self.guard_count.get();\n\n        // Update the local epoch only if there's only one guard.\n        if guard_count == 1 {\n            let epoch = self.epoch.load(Ordering::Relaxed);\n            let global_epoch = self.global().epoch.load(Ordering::Relaxed).pinned();\n\n            // Update the local epoch only if the global epoch is greater than the local epoch.\n            if epoch != global_epoch {\n                // We store the new epoch with `Release` because we need to ensure any memory\n                // accesses from the previous epoch do not leak into the new one.\n                self.epoch.store(global_epoch, Ordering::Release);\n\n                // However, we don't need a following `SeqCst` fence, because it is safe for memory\n                // accesses from the new epoch to be executed before updating the local epoch. At\n                // worse, other threads will see the new epoch late and delay GC slightly.\n            }\n        }\n    }\n\n    /// Increments the handle count.\n    #[inline]\n    pub fn acquire_handle(&self) {\n        let handle_count = self.handle_count.get();\n        debug_assert!(handle_count >= 1);\n        self.handle_count.set(handle_count + 1);\n    }\n\n    /// Decrements the handle count.\n    #[inline]\n    pub fn release_handle(&self) {\n        let guard_count = self.guard_count.get();\n        let handle_count = self.handle_count.get();\n        debug_assert!(handle_count >= 1);\n        self.handle_count.set(handle_count - 1);\n\n        if guard_count == 0 && handle_count == 1 {\n            self.finalize();\n        }\n    }\n\n    /// Removes the `Local` from the global linked list.\n    #[cold]\n    fn finalize(&self) {\n        debug_assert_eq!(self.guard_count.get(), 0);\n        debug_assert_eq!(self.handle_count.get(), 0);\n\n        // Temporarily increment handle count. This is required so that the following call to `pin`\n        // doesn't call `finalize` again.\n        self.handle_count.set(1);\n        unsafe {\n            // Pin and move the local bag into the global queue. It's important that `push_bag`\n            // doesn't defer destruction on any new garbage.\n            let guard = &self.pin();\n            self.global().push_bag(&mut *self.bag.get(), guard);\n        }\n        // Revert the handle count back to zero.\n        self.handle_count.set(0);\n\n        unsafe {\n            // Take the reference to the `Global` out of this `Local`. Since we're not protected\n            // by a guard at this time, it's crucial that the reference is read before marking the\n            // `Local` as deleted.\n            let collector: Collector = ptr::read(&*(*self.collector.get()));\n\n            // Mark this node in the linked list as deleted.\n            self.entry.delete(unprotected());\n\n            // Finally, drop the reference to the global. Note that this might be the last reference\n            // to the `Global`. If so, the global data will be destroyed and all deferred functions\n            // in its queue will be executed.\n            drop(collector);\n        }\n    }\n}"],"internal::SealedBag":["Debug","Default","impl SealedBag {\n    /// Checks if it is safe to drop the bag w.r.t. the given global epoch.\n    fn is_expired(&self, global_epoch: Epoch) -> bool {\n        // A pinned participant can witness at most one epoch advancement. Therefore, any bag that\n        // is within one epoch of the current one cannot be destroyed yet.\n        global_epoch.wrapping_sub(self.epoch) >= 2\n    }\n}","unsafe impl Sync for SealedBag {}"],"std::sync::atomic::Ordering":["impl CompareAndSetOrdering for Ordering {\n    #[inline]\n    fn success(&self) -> Ordering {\n        *self\n    }\n\n    #[inline]\n    fn failure(&self) -> Ordering {\n        strongest_failure_ordering(*self)\n    }\n}"],"sync::list::Entry":["Debug","impl Default for Entry {\n    /// Returns the empty entry.\n    fn default() -> Self {\n        Self {\n            next: Atomic::null(),\n        }\n    }\n}","impl Entry {\n    /// Marks this entry as deleted, deferring the actual deallocation to a later iteration.\n    ///\n    /// # Safety\n    ///\n    /// The entry should be a member of a linked list, and it should not have been deleted.\n    /// It should be safe to call `C::finalize` on the entry after the `guard` is dropped, where `C`\n    /// is the associated helper for the linked list.\n    pub unsafe fn delete(&self, guard: &Guard) {\n        self.next.fetch_or(1, Release, guard);\n    }\n}"],"sync::list::Iter":["impl<'g, T: 'g, C: IsElement<T>> Iterator for Iter<'g, T, C> {\n    type Item = Result<&'g T, IterError>;\n\n    fn next(&mut self) -> Option<Self::Item> {\n        while let Some(c) = unsafe { self.curr.as_ref() } {\n            let succ = c.next.load(Acquire, self.guard);\n\n            if succ.tag() == 1 {\n                // This entry was removed. Try unlinking it from the list.\n                let succ = succ.with_tag(0);\n\n                // The tag should always be zero, because removing a node after a logically deleted\n                // node leaves the list in an invalid state.\n                debug_assert!(self.curr.tag() == 0);\n\n                // Try to unlink `curr` from the list, and get the new value of `self.pred`.\n                let succ = match self\n                    .pred\n                    .compare_and_set(self.curr, succ, Acquire, self.guard)\n                {\n                    Ok(_) => {\n                        // We succeeded in unlinking `curr`, so we have to schedule\n                        // deallocation. Deferred drop is okay, because `list.delete()` can only be\n                        // called if `T: 'static`.\n                        unsafe {\n                            C::finalize(self.curr.deref(), self.guard);\n                        }\n\n                        // `succ` is the new value of `self.pred`.\n                        succ\n                    }\n                    Err(e) => {\n                        // `e.current` is the current value of `self.pred`.\n                        e.current\n                    }\n                };\n\n                // If the predecessor node is already marked as deleted, we need to restart from\n                // `head`.\n                if succ.tag() != 0 {\n                    self.pred = self.head;\n                    self.curr = self.head.load(Acquire, self.guard);\n\n                    return Some(Err(IterError::Stalled));\n                }\n\n                // Move over the removed by only advancing `curr`, not `pred`.\n                self.curr = succ;\n                continue;\n            }\n\n            // Move one step forward.\n            self.pred = &c.next;\n            self.curr = succ;\n\n            return Some(Ok(unsafe { C::element_of(c) }));\n        }\n\n        // We reached the end of the list.\n        None\n    }\n}"],"sync::list::IterError":["Debug","PartialEq"],"sync::list::List":["Debug","impl<T, C: IsElement<T>> Drop for List<T, C> {\n    fn drop(&mut self) {\n        unsafe {\n            let guard = unprotected();\n            let mut curr = self.head.load(Relaxed, guard);\n            while let Some(c) = curr.as_ref() {\n                let succ = c.next.load(Relaxed, guard);\n                // Verify that all elements have been removed from the list.\n                assert_eq!(succ.tag(), 1);\n\n                C::finalize(curr.deref(), guard);\n                curr = succ;\n            }\n        }\n    }\n}","impl<T, C: IsElement<T>> List<T, C> {\n    /// Returns a new, empty linked list.\n    pub fn new() -> Self {\n        Self {\n            head: Atomic::null(),\n            _marker: PhantomData,\n        }\n    }\n\n    /// Inserts `entry` into the head of the list.\n    ///\n    /// # Safety\n    ///\n    /// You should guarantee that:\n    ///\n    /// - `container` is not null\n    /// - `container` is immovable, e.g. inside an `Owned`\n    /// - the same `Entry` is not inserted more than once\n    /// - the inserted object will be removed before the list is dropped\n    pub unsafe fn insert<'g>(&'g self, container: Shared<'g, T>, guard: &'g Guard) {\n        // Insert right after head, i.e. at the beginning of the list.\n        let to = &self.head;\n        // Get the intrusively stored Entry of the new element to insert.\n        let entry: &Entry = C::entry_of(container.deref());\n        // Make a Shared ptr to that Entry.\n        let entry_ptr = Shared::from(entry as *const _);\n        // Read the current successor of where we want to insert.\n        let mut next = to.load(Relaxed, guard);\n\n        loop {\n            // Set the Entry of the to-be-inserted element to point to the previous successor of\n            // `to`.\n            entry.next.store(next, Relaxed);\n            match to.compare_and_set_weak(next, entry_ptr, Release, guard) {\n                Ok(_) => break,\n                // We lost the race or weak CAS failed spuriously. Update the successor and try\n                // again.\n                Err(err) => next = err.current,\n            }\n        }\n    }\n\n    /// Returns an iterator over all objects.\n    ///\n    /// # Caveat\n    ///\n    /// Every object that is inserted at the moment this function is called and persists at least\n    /// until the end of iteration will be returned. Since this iterator traverses a lock-free\n    /// linked list that may be concurrently modified, some additional caveats apply:\n    ///\n    /// 1. If a new object is inserted during iteration, it may or may not be returned.\n    /// 2. If an object is deleted during iteration, it may or may not be returned.\n    /// 3. The iteration may be aborted when it lost in a race condition. In this case, the winning\n    ///    thread will continue to iterate over the same list.\n    pub fn iter<'g>(&'g self, guard: &'g Guard) -> Iter<'g, T, C> {\n        Iter {\n            guard,\n            pred: &self.head,\n            curr: self.head.load(Acquire, guard),\n            head: &self.head,\n            _marker: PhantomData,\n        }\n    }\n}"],"sync::queue::Queue":["Debug","impl<T> Drop for Queue<T> {\n    fn drop(&mut self) {\n        unsafe {\n            let guard = unprotected();\n\n            while self.try_pop(guard).is_some() {}\n\n            // Destroy the remaining sentinel node.\n            let sentinel = self.head.load(Relaxed, guard);\n            drop(sentinel.into_owned());\n        }\n    }\n}","impl<T> Queue<T> {\n    /// Create a new, empty queue.\n    pub fn new() -> Queue<T> {\n        let q = Queue {\n            head: CachePadded::new(Atomic::null()),\n            tail: CachePadded::new(Atomic::null()),\n        };\n        let sentinel = Owned::new(Node {\n            data: MaybeUninit::uninit(),\n            next: Atomic::null(),\n        });\n        unsafe {\n            let guard = unprotected();\n            let sentinel = sentinel.into_shared(guard);\n            q.head.store(sentinel, Relaxed);\n            q.tail.store(sentinel, Relaxed);\n            q\n        }\n    }\n\n    /// Attempts to atomically place `n` into the `next` pointer of `onto`, and returns `true` on\n    /// success. The queue's `tail` pointer may be updated.\n    #[inline(always)]\n    fn push_internal(\n        &self,\n        onto: Shared<'_, Node<T>>,\n        new: Shared<'_, Node<T>>,\n        guard: &Guard,\n    ) -> bool {\n        // is `onto` the actual tail?\n        let o = unsafe { onto.deref() };\n        let next = o.next.load(Acquire, guard);\n        if unsafe { next.as_ref().is_some() } {\n            // if not, try to \"help\" by moving the tail pointer forward\n            let _ = self.tail.compare_and_set(onto, next, Release, guard);\n            false\n        } else {\n            // looks like the actual tail; attempt to link in `n`\n            let result = o\n                .next\n                .compare_and_set(Shared::null(), new, Release, guard)\n                .is_ok();\n            if result {\n                // try to move the tail pointer forward\n                let _ = self.tail.compare_and_set(onto, new, Release, guard);\n            }\n            result\n        }\n    }\n\n    /// Adds `t` to the back of the queue, possibly waking up threads blocked on `pop`.\n    pub fn push(&self, t: T, guard: &Guard) {\n        let new = Owned::new(Node {\n            data: MaybeUninit::new(t),\n            next: Atomic::null(),\n        });\n        let new = Owned::into_shared(new, guard);\n\n        loop {\n            // We push onto the tail, so we'll start optimistically by looking there first.\n            let tail = self.tail.load(Acquire, guard);\n\n            // Attempt to push onto the `tail` snapshot; fails if `tail.next` has changed.\n            if self.push_internal(tail, new, guard) {\n                break;\n            }\n        }\n    }\n\n    /// Attempts to pop a data node. `Ok(None)` if queue is empty; `Err(())` if lost race to pop.\n    #[inline(always)]\n    fn pop_internal(&self, guard: &Guard) -> Result<Option<T>, ()> {\n        let head = self.head.load(Acquire, guard);\n        let h = unsafe { head.deref() };\n        let next = h.next.load(Acquire, guard);\n        match unsafe { next.as_ref() } {\n            Some(n) => unsafe {\n                self.head\n                    .compare_and_set(head, next, Release, guard)\n                    .map(|_| {\n                        let tail = self.tail.load(Relaxed, guard);\n                        // Advance the tail so that we don't retire a pointer to a reachable node.\n                        if head == tail {\n                            let _ = self.tail.compare_and_set(tail, next, Release, guard);\n                        }\n                        guard.defer_destroy(head);\n                        // TODO: Replace with MaybeUninit::read when api is stable\n                        Some(n.data.as_ptr().read())\n                    })\n                    .map_err(|_| ())\n            },\n            None => Ok(None),\n        }\n    }\n\n    /// Attempts to pop a data node, if the data satisfies the given condition. `Ok(None)` if queue\n    /// is empty or the data does not satisfy the condition; `Err(())` if lost race to pop.\n    #[inline(always)]\n    fn pop_if_internal<F>(&self, condition: F, guard: &Guard) -> Result<Option<T>, ()>\n    where\n        T: Sync,\n        F: Fn(&T) -> bool,\n    {\n        let head = self.head.load(Acquire, guard);\n        let h = unsafe { head.deref() };\n        let next = h.next.load(Acquire, guard);\n        match unsafe { next.as_ref() } {\n            Some(n) if condition(unsafe { &*n.data.as_ptr() }) => unsafe {\n                self.head\n                    .compare_and_set(head, next, Release, guard)\n                    .map(|_| {\n                        let tail = self.tail.load(Relaxed, guard);\n                        // Advance the tail so that we don't retire a pointer to a reachable node.\n                        if head == tail {\n                            let _ = self.tail.compare_and_set(tail, next, Release, guard);\n                        }\n                        guard.defer_destroy(head);\n                        Some(n.data.as_ptr().read())\n                    })\n                    .map_err(|_| ())\n            },\n            None | Some(_) => Ok(None),\n        }\n    }\n\n    /// Attempts to dequeue from the front.\n    ///\n    /// Returns `None` if the queue is observed to be empty.\n    pub fn try_pop(&self, guard: &Guard) -> Option<T> {\n        loop {\n            if let Ok(head) = self.pop_internal(guard) {\n                return head;\n            }\n        }\n    }\n\n    /// Attempts to dequeue from the front, if the item satisfies the given condition.\n    ///\n    /// Returns `None` if the queue is observed to be empty, or the head does not satisfy the given\n    /// condition.\n    pub fn try_pop_if<F>(&self, condition: F, guard: &Guard) -> Option<T>\n    where\n        T: Sync,\n        F: Fn(&T) -> bool,\n    {\n        loop {\n            if let Ok(head) = self.pop_if_internal(&condition, guard) {\n                return head;\n            }\n        }\n    }\n}","unsafe impl<T: Send> Send for Queue<T> {}","unsafe impl<T: Send> Sync for Queue<T> {}"]},"single_path_import":{"atomic::Atomic":"Atomic","atomic::CompareAndSetError":"CompareAndSetError","atomic::CompareAndSetOrdering":"CompareAndSetOrdering","atomic::Owned":"Owned","atomic::Pointable":"Pointable","atomic::Pointer":"Pointer","atomic::Shared":"Shared","collector::Collector":"Collector","collector::LocalHandle":"LocalHandle","default::default_collector":"default_collector","default::is_pinned":"is_pinned","default::pin":"pin","guard::Guard":"Guard","guard::unprotected":"unprotected"},"srcs":{"<(std::sync::atomic::Ordering, std::sync::atomic::Ordering) as atomic::CompareAndSetOrdering>::failure":["#[inline]\nfn failure(&self) -> Ordering{\n        self.1\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<(std::sync::atomic::Ordering, std::sync::atomic::Ordering) as atomic::CompareAndSetOrdering>::success":["#[inline]\nfn success(&self) -> Ordering{\n        self.0\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<T as atomic::Pointable>::deref":["unsafe fn deref<'a>(ptr: usize) -> &'a Self{\n        &*(ptr as *const T)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<T as atomic::Pointable>::deref_mut":["unsafe fn deref_mut<'a>(ptr: usize) -> &'a mut Self{\n        &mut *(ptr as *mut T)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<T as atomic::Pointable>::drop":["unsafe fn drop(ptr: usize){\n        drop(Box::from_raw(ptr as *mut T));\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<T as atomic::Pointable>::init":["unsafe fn init(init: Self::Init) -> usize{\n        Box::into_raw(Box::new(init)) as usize\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::deref":["unsafe fn deref<'a>(ptr: usize) -> &'a Self{\n        let array = &*(ptr as *const Array<T>);\n        slice::from_raw_parts(array.elements.as_ptr() as *const _, array.size)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::deref_mut":["unsafe fn deref_mut<'a>(ptr: usize) -> &'a mut Self{\n        let array = &*(ptr as *mut Array<T>);\n        slice::from_raw_parts_mut(array.elements.as_ptr() as *mut _, array.size)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::drop":["unsafe fn drop(ptr: usize){\n        let array = &*(ptr as *mut Array<T>);\n        let size = mem::size_of::<Array<T>>() + mem::size_of::<MaybeUninit<T>>() * array.size;\n        let align = mem::align_of::<Array<T>>();\n        let layout = alloc::Layout::from_size_align(size, align).unwrap();\n        alloc::dealloc(ptr as *mut u8, layout);\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::init":["unsafe fn init(size: Self::Init) -> usize{\n        let size = mem::size_of::<Array<T>>() + mem::size_of::<MaybeUninit<T>>() * size;\n        let align = mem::align_of::<Array<T>>();\n        let layout = alloc::Layout::from_size_align(size, align).unwrap();\n        let ptr = alloc::alloc(layout) as *mut Array<T>;\n        (*ptr).size = size;\n        ptr as usize\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::clone::Clone>::clone":["/// Returns a copy of the atomic value.\n///\n/// Note that a `Relaxed` load is used here. If you need synchronization, use it with other\n/// atomics or fences.\nfn clone(&self) -> Self{\n        let data = self.data.load(Ordering::Relaxed);\n        Atomic::from_usize(data)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::convert::From<*const T>>::from":["/// Returns a new atomic pointer pointing to `raw`.\n///\n/// # Examples\n///\n/// ```\n/// use std::ptr;\n/// use crossbeam_epoch::Atomic;\n///\n/// let a = Atomic::<i32>::from(ptr::null::<i32>());\n/// ```\nfn from(raw: *const T) -> Self{\n        Self::from_usize(raw as usize)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::convert::From<T>>::from":["fn from(t: T) -> Self{\n        Self::new(t)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::convert::From<atomic::Owned<T>>>::from":["/// Returns a new atomic pointer pointing to `owned`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{Atomic, Owned};\n///\n/// let a = Atomic::<i32>::from(Owned::new(1234));\n/// ```\nfn from(owned: Owned<T>) -> Self{\n        let data = owned.data;\n        mem::forget(owned);\n        Self::from_usize(data)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::convert::From<atomic::Shared<'g, T>>>::from":["/// Returns a new atomic pointer pointing to `ptr`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{Atomic, Shared};\n///\n/// let a = Atomic::<i32>::from(Shared::<i32>::null());\n/// ```\nfn from(ptr: Shared<'g, T>) -> Self{\n        Self::from_usize(ptr.data)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::convert::From<std::boxed::Box<T>>>::from":["fn from(b: Box<T>) -> Self{\n        Self::from(Owned::from(b))\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::default::Default>::default":["fn default() -> Self{\n        Atomic::null()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        let data = self.data.load(Ordering::SeqCst);\n        let (raw, tag) = decompose_tag::<T>(data);\n\n        f.debug_struct(\"Atomic\")\n            .field(\"raw\", &raw)\n            .field(\"tag\", &tag)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Atomic<T> as std::fmt::Pointer>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        let data = self.data.load(Ordering::SeqCst);\n        let (raw, _) = decompose_tag::<T>(data);\n        fmt::Pointer::fmt(&(unsafe { T::deref(raw) as *const _ }), f)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::CompareAndSetError<'g, T, P> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"CompareAndSetError\")\n            .field(\"current\", &self.current)\n            .field(\"new\", &self.new)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as atomic::Pointer<T>>::from_usize":["/// Returns a new pointer pointing to the tagged pointer `data`.\n///\n/// # Panics\n///\n/// Panics if the data is zero in debug mode.\n#[inline]\nunsafe fn from_usize(data: usize) -> Self{\n        debug_assert!(data != 0, \"converting zero into `Owned`\");\n        Owned {\n            data,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as atomic::Pointer<T>>::into_usize":["#[inline]\nfn into_usize(self) -> usize{\n        let data = self.data;\n        mem::forget(self);\n        data\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as lazy_static::__Deref>::deref":["fn deref(&self) -> &T{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        unsafe { T::deref(raw) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::borrow::Borrow<T>>::borrow":["fn borrow(&self) -> &T{\n        self.deref()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::borrow::BorrowMut<T>>::borrow_mut":["fn borrow_mut(&mut self) -> &mut T{\n        self.deref_mut()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::clone::Clone>::clone":["fn clone(&self) -> Self{\n        Owned::new((**self).clone()).with_tag(self.tag())\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::convert::AsMut<T>>::as_mut":["fn as_mut(&mut self) -> &mut T{\n        self.deref_mut()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::convert::AsRef<T>>::as_ref":["fn as_ref(&self) -> &T{\n        self.deref()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::convert::From<T>>::from":["fn from(t: T) -> Self{\n        Owned::new(t)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::convert::From<std::boxed::Box<T>>>::from":["/// Returns a new owned pointer pointing to `b`.\n///\n/// # Panics\n///\n/// Panics if the pointer (the `Box`) is not properly aligned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// let o = unsafe { Owned::from_raw(Box::into_raw(Box::new(1234))) };\n/// ```\nfn from(b: Box<T>) -> Self{\n        unsafe { Self::from_raw(Box::into_raw(b)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        let (raw, tag) = decompose_tag::<T>(self.data);\n\n        f.debug_struct(\"Owned\")\n            .field(\"raw\", &raw)\n            .field(\"tag\", &tag)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::ops::DerefMut>::deref_mut":["fn deref_mut(&mut self) -> &mut T{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        unsafe { T::deref_mut(raw) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Owned<T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        let (raw, _) = decompose_tag::<T>(self.data);\n        unsafe {\n            T::drop(raw);\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as atomic::Pointer<T>>::from_usize":["#[inline]\nunsafe fn from_usize(data: usize) -> Self{\n        Shared {\n            data,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as atomic::Pointer<T>>::into_usize":["#[inline]\nfn into_usize(self) -> usize{\n        self.data\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as std::clone::Clone>::clone":["fn clone(&self) -> Self{\n        Self {\n            data: self.data,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as std::cmp::Ord>::cmp":["fn cmp(&self, other: &Self) -> cmp::Ordering{\n        self.data.cmp(&other.data)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as std::convert::From<*const T>>::from":["/// Returns a new pointer pointing to `raw`.\n///\n/// # Panics\n///\n/// Panics if `raw` is not properly aligned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Shared;\n///\n/// let p = Shared::from(Box::into_raw(Box::new(1234)) as *const _);\n/// assert!(!p.is_null());\n/// ```\nfn from(raw: *const T) -> Self{\n        let raw = raw as usize;\n        ensure_aligned::<T>(raw);\n        unsafe { Self::from_usize(raw) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as std::default::Default>::default":["fn default() -> Self{\n        Shared::null()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        let (raw, tag) = decompose_tag::<T>(self.data);\n\n        f.debug_struct(\"Shared\")\n            .field(\"raw\", &raw)\n            .field(\"tag\", &tag)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'_, T> as std::fmt::Pointer>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        fmt::Pointer::fmt(&(unsafe { self.deref() as *const _ }), f)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'g, T> as std::cmp::PartialEq>::eq":["fn eq(&self, other: &Self) -> bool{\n        self.data == other.data\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<atomic::Shared<'g, T> as std::cmp::PartialOrd>::partial_cmp":["fn partial_cmp(&self, other: &Self) -> Option<cmp::Ordering>{\n        self.data.partial_cmp(&other.data)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<collector::Collector as std::clone::Clone>::clone":["/// Creates another reference to the same garbage collector.\nfn clone(&self) -> Self{\n        Collector {\n            global: self.global.clone(),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"<collector::Collector as std::cmp::PartialEq>::eq":["/// Checks if both handles point to the same collector.\nfn eq(&self, rhs: &Collector) -> bool{\n        Arc::ptr_eq(&self.global, &rhs.global)\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"<collector::Collector as std::default::Default>::default":["fn default() -> Self{\n        Self {\n            global: Arc::new(Global::new()),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"<collector::Collector as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Collector { .. }\")\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"<collector::LocalHandle as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"LocalHandle { .. }\")\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"<collector::LocalHandle as std::ops::Drop>::drop":["#[inline]\nfn drop(&mut self){\n        unsafe {\n            Local::release_handle(&*self.local);\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"<default::COLLECTOR as lazy_static::LazyStatic>::initialize":["fn initialize(lazy: &Self){\n                let _ = &**lazy;\n            }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<default::COLLECTOR as lazy_static::__Deref>::deref":["fn deref(&self) -> &$T{\n                #[inline(always)]\n                fn __static_ref_initialize() -> $T { $e }\n\n                #[inline(always)]\n                fn __stability() -> &'static $T {\n                    __lazy_static_create!(LAZY, $T);\n                    LAZY.get(__static_ref_initialize)\n                }\n                __stability()\n            }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<default::COLLECTOR as lazy_static::__Deref>::deref::__stability":["#[inline(always)]\nfn __stability() -> &'static $T{\n                    __lazy_static_create!(LAZY, $T);\n                    LAZY.get(__static_ref_initialize)\n                }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<default::COLLECTOR as lazy_static::__Deref>::deref::__static_ref_initialize":["#[inline(always)]\nfn __static_ref_initialize() -> $T{ $e }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<deferred::Deferred as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> Result<(), fmt::Error>{\n        f.pad(\"Deferred { .. }\")\n    }","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))"],"<guard::Guard as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Guard { .. }\")\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"<guard::Guard as std::ops::Drop>::drop":["#[inline]\nfn drop(&mut self){\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.unpin();\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"<internal::Bag as std::default::Default>::default":["#[rustfmt::skip]\nfn default() -> Self{\n        // TODO: [no_op; MAX_OBJECTS] syntax blocked by https://github.com/rust-lang/rust/issues/49147\n        #[cfg(not(feature = \"sanitize\"))]\n        return Bag {\n            len: 0,\n            deferreds: [\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n            ],\n        };\n        #[cfg(feature = \"sanitize\")]\n        return Bag {\n            len: 0,\n            deferreds: [\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n                Deferred::new(no_op_func),\n            ],\n        };\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"<internal::Bag as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"Bag\")\n            .field(\"deferreds\", &&self.deferreds[..self.len])\n            .finish()\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"<internal::Bag as std::ops::Drop>::drop":["fn drop(&mut self){\n        // Call all deferred functions.\n        for deferred in &mut self.deferreds[..self.len] {\n            let no_op = Deferred::new(no_op_func);\n            let owned_deferred = mem::replace(deferred, no_op);\n            owned_deferred.call();\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"<internal::Local as sync::list::IsElement<internal::Local>>::element_of":["unsafe fn element_of(entry: &Entry) -> &Local{\n        // offset_of! macro uses unsafe, but it's unnecessary in this context.\n        #[allow(unused_unsafe)]\n        let local_ptr = (entry as *const Entry as usize - offset_of!(Local, entry)) as *const Local;\n        &*local_ptr\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"<internal::Local as sync::list::IsElement<internal::Local>>::entry_of":["fn entry_of(local: &Local) -> &Entry{\n        let entry_ptr = (local as *const Local as usize + offset_of!(Local, entry)) as *const Entry;\n        unsafe { &*entry_ptr }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"<internal::Local as sync::list::IsElement<internal::Local>>::finalize":["unsafe fn finalize(entry: &Entry, guard: &Guard){\n        guard.defer_destroy(Shared::from(Self::element_of(entry) as *const _));\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"<std::sync::atomic::Ordering as atomic::CompareAndSetOrdering>::failure":["#[inline]\nfn failure(&self) -> Ordering{\n        strongest_failure_ordering(*self)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<std::sync::atomic::Ordering as atomic::CompareAndSetOrdering>::success":["#[inline]\nfn success(&self) -> Ordering{\n        *self\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"<sync::list::Entry as std::default::Default>::default":["/// Returns the empty entry.\nfn default() -> Self{\n        Self {\n            next: Atomic::null(),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"<sync::list::Iter<'g, T, C> as std::iter::Iterator>::next":["fn next(&mut self) -> Option<Self::Item>{\n        while let Some(c) = unsafe { self.curr.as_ref() } {\n            let succ = c.next.load(Acquire, self.guard);\n\n            if succ.tag() == 1 {\n                // This entry was removed. Try unlinking it from the list.\n                let succ = succ.with_tag(0);\n\n                // The tag should always be zero, because removing a node after a logically deleted\n                // node leaves the list in an invalid state.\n                debug_assert!(self.curr.tag() == 0);\n\n                // Try to unlink `curr` from the list, and get the new value of `self.pred`.\n                let succ = match self\n                    .pred\n                    .compare_and_set(self.curr, succ, Acquire, self.guard)\n                {\n                    Ok(_) => {\n                        // We succeeded in unlinking `curr`, so we have to schedule\n                        // deallocation. Deferred drop is okay, because `list.delete()` can only be\n                        // called if `T: 'static`.\n                        unsafe {\n                            C::finalize(self.curr.deref(), self.guard);\n                        }\n\n                        // `succ` is the new value of `self.pred`.\n                        succ\n                    }\n                    Err(e) => {\n                        // `e.current` is the current value of `self.pred`.\n                        e.current\n                    }\n                };\n\n                // If the predecessor node is already marked as deleted, we need to restart from\n                // `head`.\n                if succ.tag() != 0 {\n                    self.pred = self.head;\n                    self.curr = self.head.load(Acquire, self.guard);\n\n                    return Some(Err(IterError::Stalled));\n                }\n\n                // Move over the removed by only advancing `curr`, not `pred`.\n                self.curr = succ;\n                continue;\n            }\n\n            // Move one step forward.\n            self.pred = &c.next;\n            self.curr = succ;\n\n            return Some(Ok(unsafe { C::element_of(c) }));\n        }\n\n        // We reached the end of the list.\n        None\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"<sync::list::List<T, C> as std::ops::Drop>::drop":["fn drop(&mut self){\n        unsafe {\n            let guard = unprotected();\n            let mut curr = self.head.load(Relaxed, guard);\n            while let Some(c) = curr.as_ref() {\n                let succ = c.next.load(Relaxed, guard);\n                // Verify that all elements have been removed from the list.\n                assert_eq!(succ.tag(), 1);\n\n                C::finalize(curr.deref(), guard);\n                curr = succ;\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"<sync::queue::Queue<T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        unsafe {\n            let guard = unprotected();\n\n            while self.try_pop(guard).is_some() {}\n\n            // Destroy the remaining sentinel node.\n            let sentinel = self.head.load(Relaxed, guard);\n            drop(sentinel.into_owned());\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"atomic::Array":["/// Array with size.\n///\n/// # Memory layout\n///\n/// An array consisting of size and elements:\n///\n/// ```ignore\n///          elements\n///          |\n///          |\n/// ------------------------------------\n/// | size | 0 | 1 | 2 | 3 | 4 | 5 | 6 |\n/// ------------------------------------\n/// ```\n///\n/// Its memory layout is different from that of `Box<[T]>` in that size is in the allocation (not\n/// along with pointer as in `Box<[T]>`).\n///\n/// Elements are not present in the type, but they will be in the allocation.\n/// ```\n///\n#[repr(C)]\nstruct Array<T> {\n    size: usize,\n    elements: [MaybeUninit<T>; 0],\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic":["/// An atomic pointer that can be safely shared between threads.\n///\n/// The pointer must be properly aligned. Since it is aligned, a tag can be stored into the unused\n/// least significant bits of the address. For example, the tag for a pointer to a sized type `T`\n/// should be less than `(1 << mem::align_of::<T>().trailing_zeros())`.\n///\n/// Any method that loads the pointer must be passed a reference to a [`Guard`].\n///\n/// Crossbeam supports dynamically sized types.  See [`Pointable`] for details.\n///\n/// [`Guard`]: struct.Guard.html\n/// [`Pointable`]: trait.Pointable.html\npub struct Atomic<T: ?Sized + Pointable> {\n    data: AtomicUsize,\n    _marker: PhantomData<*mut T>,\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::compare_and_set":["/// Stores the pointer `new` (either `Shared` or `Owned`) into the atomic pointer if the current\n/// value is the same as `current`. The tag is also taken into account, so two pointers to the\n/// same object, but with different tags, will not be considered equal.\n///\n/// The return value is a result indicating whether the new pointer was written. On success the\n/// pointer that was written is returned. On failure the actual current value and `new` are\n/// returned.\n///\n/// This method takes a [`CompareAndSetOrdering`] argument which describes the memory\n/// ordering of this operation.\n///\n/// [`CompareAndSetOrdering`]: trait.CompareAndSetOrdering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n///\n/// let guard = &epoch::pin();\n/// let curr = a.load(SeqCst, guard);\n/// let res1 = a.compare_and_set(curr, Shared::null(), SeqCst, guard);\n/// let res2 = a.compare_and_set(curr, Owned::new(5678), SeqCst, guard);\n/// ```\npub fn compare_and_set<'g, O, P>(\n        &self,\n        current: Shared<'_, T>,\n        new: P,\n        ord: O,\n        _: &'g Guard,\n    ) -> Result<Shared<'g, T>, CompareAndSetError<'g, T, P>>\n    where\n        O: CompareAndSetOrdering,\n        P: Pointer<T>,{\n        let new = new.into_usize();\n        self.data\n            .compare_exchange(current.into_usize(), new, ord.success(), ord.failure())\n            .map(|_| unsafe { Shared::from_usize(new) })\n            .map_err(|current| unsafe {\n                CompareAndSetError {\n                    current: Shared::from_usize(current),\n                    new: P::from_usize(new),\n                }\n            })\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::compare_and_set_weak":["/// Stores the pointer `new` (either `Shared` or `Owned`) into the atomic pointer if the current\n/// value is the same as `current`. The tag is also taken into account, so two pointers to the\n/// same object, but with different tags, will not be considered equal.\n///\n/// Unlike [`compare_and_set`], this method is allowed to spuriously fail even when comparison\n/// succeeds, which can result in more efficient code on some platforms.  The return value is a\n/// result indicating whether the new pointer was written. On success the pointer that was\n/// written is returned. On failure the actual current value and `new` are returned.\n///\n/// This method takes a [`CompareAndSetOrdering`] argument which describes the memory\n/// ordering of this operation.\n///\n/// [`compare_and_set`]: struct.Atomic.html#method.compare_and_set\n/// [`CompareAndSetOrdering`]: trait.CompareAndSetOrdering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// let guard = &epoch::pin();\n///\n/// let mut new = Owned::new(5678);\n/// let mut ptr = a.load(SeqCst, guard);\n/// loop {\n///     match a.compare_and_set_weak(ptr, new, SeqCst, guard) {\n///         Ok(p) => {\n///             ptr = p;\n///             break;\n///         }\n///         Err(err) => {\n///             ptr = err.current;\n///             new = err.new;\n///         }\n///     }\n/// }\n///\n/// let mut curr = a.load(SeqCst, guard);\n/// loop {\n///     match a.compare_and_set_weak(curr, Shared::null(), SeqCst, guard) {\n///         Ok(_) => break,\n///         Err(err) => curr = err.current,\n///     }\n/// }\n/// ```\npub fn compare_and_set_weak<'g, O, P>(\n        &self,\n        current: Shared<'_, T>,\n        new: P,\n        ord: O,\n        _: &'g Guard,\n    ) -> Result<Shared<'g, T>, CompareAndSetError<'g, T, P>>\n    where\n        O: CompareAndSetOrdering,\n        P: Pointer<T>,{\n        let new = new.into_usize();\n        self.data\n            .compare_exchange_weak(current.into_usize(), new, ord.success(), ord.failure())\n            .map(|_| unsafe { Shared::from_usize(new) })\n            .map_err(|current| unsafe {\n                CompareAndSetError {\n                    current: Shared::from_usize(current),\n                    new: P::from_usize(new),\n                }\n            })\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::fetch_and":["/// Bitwise \"and\" with the current tag.\n///\n/// Performs a bitwise \"and\" operation on the current tag and the argument `val`, and sets the\n/// new tag to the result. Returns the previous pointer.\n///\n/// This method takes an [`Ordering`] argument which describes the memory ordering of this\n/// operation.\n///\n/// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::<i32>::from(Shared::null().with_tag(3));\n/// let guard = &epoch::pin();\n/// assert_eq!(a.fetch_and(2, SeqCst, guard).tag(), 3);\n/// assert_eq!(a.load(SeqCst, guard).tag(), 2);\n/// ```\npub fn fetch_and<'g>(&self, val: usize, ord: Ordering, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.data.fetch_and(val | !low_bits::<T>(), ord)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::fetch_or":["/// Bitwise \"or\" with the current tag.\n///\n/// Performs a bitwise \"or\" operation on the current tag and the argument `val`, and sets the\n/// new tag to the result. Returns the previous pointer.\n///\n/// This method takes an [`Ordering`] argument which describes the memory ordering of this\n/// operation.\n///\n/// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::<i32>::from(Shared::null().with_tag(1));\n/// let guard = &epoch::pin();\n/// assert_eq!(a.fetch_or(2, SeqCst, guard).tag(), 1);\n/// assert_eq!(a.load(SeqCst, guard).tag(), 3);\n/// ```\npub fn fetch_or<'g>(&self, val: usize, ord: Ordering, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.data.fetch_or(val & low_bits::<T>(), ord)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::fetch_xor":["/// Bitwise \"xor\" with the current tag.\n///\n/// Performs a bitwise \"xor\" operation on the current tag and the argument `val`, and sets the\n/// new tag to the result. Returns the previous pointer.\n///\n/// This method takes an [`Ordering`] argument which describes the memory ordering of this\n/// operation.\n///\n/// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::<i32>::from(Shared::null().with_tag(1));\n/// let guard = &epoch::pin();\n/// assert_eq!(a.fetch_xor(3, SeqCst, guard).tag(), 1);\n/// assert_eq!(a.load(SeqCst, guard).tag(), 2);\n/// ```\npub fn fetch_xor<'g>(&self, val: usize, ord: Ordering, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.data.fetch_xor(val & low_bits::<T>(), ord)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::from_usize":["/// Returns a new atomic pointer pointing to the tagged pointer `data`.\nfn from_usize(data: usize) -> Self{\n        Self {\n            data: AtomicUsize::new(data),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::init":["/// Allocates `value` on the heap and returns a new atomic pointer pointing to it.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Atomic;\n///\n/// let a = Atomic::<i32>::init(1234);\n/// ```\npub fn init(init: T::Init) -> Atomic<T>{\n        Self::from(Owned::init(init))\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::into_owned":["/// Takes ownership of the pointee.\n///\n/// This consumes the atomic and converts it into [`Owned`]. As [`Atomic`] doesn't have a\n/// destructor and doesn't drop the pointee while [`Owned`] does, this is suitable for\n/// destructors of data structures.\n///\n/// # Panics\n///\n/// Panics if this pointer is null, but only in debug mode.\n///\n/// # Safety\n///\n/// This method may be called only if the pointer is valid and nobody else is holding a\n/// reference to the same object.\n///\n/// # Examples\n///\n/// ```rust\n/// # use std::mem;\n/// # use crossbeam_epoch::Atomic;\n/// struct DataStructure {\n///     ptr: Atomic<usize>,\n/// }\n///\n/// impl Drop for DataStructure {\n///     fn drop(&mut self) {\n///         // By now the DataStructure lives only in our thread and we are sure we don't hold\n///         // any Shared or & to it ourselves.\n///         unsafe {\n///             drop(mem::replace(&mut self.ptr, Atomic::null()).into_owned());\n///         }\n///     }\n/// }\n/// ```\npub unsafe fn into_owned(self) -> Owned<T>{\n        Owned::from_usize(self.data.into_inner())\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::load":["/// Loads a `Shared` from the atomic pointer.\n///\n/// This method takes an [`Ordering`] argument which describes the memory ordering of this\n/// operation.\n///\n/// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// let guard = &epoch::pin();\n/// let p = a.load(SeqCst, guard);\n/// ```\npub fn load<'g>(&self, ord: Ordering, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.data.load(ord)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::load_consume":["/// Loads a `Shared` from the atomic pointer using a \"consume\" memory ordering.\n///\n/// This is similar to the \"acquire\" ordering, except that an ordering is\n/// only guaranteed with operations that \"depend on\" the result of the load.\n/// However consume loads are usually much faster than acquire loads on\n/// architectures with a weak memory model since they don't require memory\n/// fence instructions.\n///\n/// The exact definition of \"depend on\" is a bit vague, but it works as you\n/// would expect in practice since a lot of software, especially the Linux\n/// kernel, rely on this behavior.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n///\n/// let a = Atomic::new(1234);\n/// let guard = &epoch::pin();\n/// let p = a.load_consume(guard);\n/// ```\npub fn load_consume<'g>(&self, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.data.load_consume()) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::new":["/// Allocates `value` on the heap and returns a new atomic pointer pointing to it.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Atomic;\n///\n/// let a = Atomic::new(1234);\n/// ```\npub fn new(init: T) -> Atomic<T>{\n        Self::init(init)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::null":["/// Returns a new null atomic pointer.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Atomic;\n///\n/// let a = Atomic::<i32>::null();\n/// ```\n#[cfg(not(feature = \"nightly\"))]\npub fn null() -> Atomic<T>{\n        Self {\n            data: AtomicUsize::new(0),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::store":["/// Stores a `Shared` or `Owned` pointer into the atomic pointer.\n///\n/// This method takes an [`Ordering`] argument which describes the memory ordering of this\n/// operation.\n///\n/// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{Atomic, Owned, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// a.store(Shared::null(), SeqCst);\n/// a.store(Owned::new(1234), SeqCst);\n/// ```\npub fn store<P: Pointer<T>>(&self, new: P, ord: Ordering){\n        self.data.store(new.into_usize(), ord);\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Atomic::<T>::swap":["/// Stores a `Shared` or `Owned` pointer into the atomic pointer, returning the previous\n/// `Shared`.\n///\n/// This method takes an [`Ordering`] argument which describes the memory ordering of this\n/// operation.\n///\n/// [`Ordering`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Shared};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// let guard = &epoch::pin();\n/// let p = a.swap(Shared::null(), SeqCst, guard);\n/// ```\npub fn swap<'g, P: Pointer<T>>(&self, new: P, ord: Ordering, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.data.swap(new.into_usize(), ord)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::CompareAndSetError":["/// The error returned on failed compare-and-set operation.\npub struct CompareAndSetError<'g, T: ?Sized + Pointable, P: Pointer<T>> {\n    /// The value in the atomic pointer at the time of the failed operation.\n    pub current: Shared<'g, T>,\n\n    /// The new value, which the operation failed to store.\n    pub new: P,\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::CompareAndSetOrdering":["/// Memory orderings for compare-and-set operations.\n///\n/// A compare-and-set operation can have different memory orderings depending on whether it\n/// succeeds or fails. This trait generalizes different ways of specifying memory orderings.\n///\n/// The two ways of specifying orderings for compare-and-set are:\n///\n/// 1. Just one `Ordering` for the success case. In case of failure, the strongest appropriate\n///    ordering is chosen.\n/// 2. A pair of `Ordering`s. The first one is for the success case, while the second one is\n///    for the failure case.\npub trait CompareAndSetOrdering {\n    /// The ordering of the operation when it succeeds.\n    fn success(&self) -> Ordering;\n\n    /// The ordering of the operation when it fails.\n    ///\n    /// The failure ordering can't be `Release` or `AcqRel` and must be equivalent or weaker than\n    /// the success ordering.\n    fn failure(&self) -> Ordering;\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned":["/// An owned heap-allocated object.\n///\n/// This type is very similar to `Box<T>`.\n///\n/// The pointer must be properly aligned. Since it is aligned, a tag can be stored into the unused\n/// least significant bits of the address.\npub struct Owned<T: ?Sized + Pointable> {\n    data: usize,\n    _marker: PhantomData<Box<T>>,\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::from_raw":["/// Returns a new owned pointer pointing to `raw`.\n///\n/// This function is unsafe because improper use may lead to memory problems. Argument `raw`\n/// must be a valid pointer. Also, a double-free may occur if the function is called twice on\n/// the same raw pointer.\n///\n/// # Panics\n///\n/// Panics if `raw` is not properly aligned.\n///\n/// # Safety\n///\n/// The given `raw` should have been derived from `Owned`, and one `raw` should not be converted\n/// back by `Owned::from_raw()` mutliple times.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// let o = unsafe { Owned::from_raw(Box::into_raw(Box::new(1234))) };\n/// ```\npub unsafe fn from_raw(raw: *mut T) -> Owned<T>{\n        let raw = raw as usize;\n        ensure_aligned::<T>(raw);\n        Self::from_usize(raw)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::init":["/// Allocates `value` on the heap and returns a new owned pointer pointing to it.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// let o = Owned::<i32>::init(1234);\n/// ```\npub fn init(init: T::Init) -> Owned<T>{\n        unsafe { Self::from_usize(T::init(init)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::into_box":["/// Converts the owned pointer into a `Box`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// let o = Owned::new(1234);\n/// let b: Box<i32> = o.into_box();\n/// assert_eq!(*b, 1234);\n/// ```\npub fn into_box(self) -> Box<T>{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        mem::forget(self);\n        unsafe { Box::from_raw(raw as *mut _) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::into_shared":["/// Converts the owned pointer into a [`Shared`].\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Owned};\n///\n/// let o = Owned::new(1234);\n/// let guard = &epoch::pin();\n/// let p = o.into_shared(guard);\n/// ```\n///\n/// [`Shared`]: struct.Shared.html\n#[allow(clippy::needless_lifetimes)]\npub fn into_shared<'g>(self, _: &'g Guard) -> Shared<'g, T>{\n        unsafe { Shared::from_usize(self.into_usize()) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::new":["/// Allocates `value` on the heap and returns a new owned pointer pointing to it.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// let o = Owned::new(1234);\n/// ```\npub fn new(init: T) -> Owned<T>{\n        Self::init(init)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::tag":["/// Returns the tag stored within the pointer.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// assert_eq!(Owned::new(1234).tag(), 0);\n/// ```\npub fn tag(&self) -> usize{\n        let (_, tag) = decompose_tag::<T>(self.data);\n        tag\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Owned::<T>::with_tag":["/// Returns the same pointer, but tagged with `tag`. `tag` is truncated to be fit into the\n/// unused bits of the pointer to `T`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Owned;\n///\n/// let o = Owned::new(0u64);\n/// assert_eq!(o.tag(), 0);\n/// let o = o.with_tag(2);\n/// assert_eq!(o.tag(), 2);\n/// ```\npub fn with_tag(self, tag: usize) -> Owned<T>{\n        let data = self.into_usize();\n        unsafe { Self::from_usize(compose_tag::<T>(data, tag)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Pointable":["/// Types that are pointed to by a single word.\n///\n/// In concurrent programming, it is necessary to represent an object within a word because atomic\n/// operations (e.g., reads, writes, read-modify-writes) support only single words.  This trait\n/// qualifies such types that are pointed to by a single word.\n///\n/// The trait generalizes `Box<T>` for a sized type `T`.  In a box, an object of type `T` is\n/// allocated in heap and it is owned by a single-word pointer.  This trait is also implemented for\n/// `[MaybeUninit<T>]` by storing its size along with its elements and pointing to the pair of array\n/// size and elements.\n///\n/// Pointers to `Pointable` types can be stored in [`Atomic`], [`Owned`], and [`Shared`].  In\n/// particular, Crossbeam supports dynamically sized slices as follows.\n///\n/// ```\n/// use std::mem::MaybeUninit;\n/// use crossbeam_epoch::Owned;\n///\n/// let o = Owned::<[MaybeUninit<i32>]>::init(10); // allocating [i32; 10]\n/// ```\n///\n/// [`Atomic`]: struct.Atomic.html\n/// [`Owned`]: struct.Owned.html\n/// [`Shared`]: struct.Shared.html\npub trait Pointable {\n    /// The alignment of pointer.\n    const ALIGN: usize;\n\n    /// The type for initializers.\n    type Init;\n\n    /// Initializes a with the given initializer.\n    ///\n    /// # Safety\n    ///\n    /// The result should be a multiple of `ALIGN`.\n    unsafe fn init(init: Self::Init) -> usize;\n\n    /// Dereferences the given pointer.\n    ///\n    /// # Safety\n    ///\n    /// - The given `ptr` should have been initialized with [`Pointable::init`].\n    /// - `ptr` should not have yet been dropped by [`Pointable::drop`].\n    /// - `ptr` should not be mutably dereferenced by [`Pointable::deref_mut`] concurrently.\n    ///\n    /// [`Pointable::init`]: trait.Pointable.html#method.init\n    /// [`Pointable::drop`]: trait.Pointable.html#method.drop\n    /// [`Pointable::deref`]: trait.Pointable.html#method.deref\n    unsafe fn deref<'a>(ptr: usize) -> &'a Self;\n\n    /// Mutably dereferences the given pointer.\n    ///\n    /// # Safety\n    ///\n    /// - The given `ptr` should have been initialized with [`Pointable::init`].\n    /// - `ptr` should not have yet been dropped by [`Pointable::drop`].\n    /// - `ptr` should not be dereferenced by [`Pointable::deref`] or [`Pointable::deref_mut`]\n    ///   concurrently.\n    ///\n    /// [`Pointable::init`]: trait.Pointable.html#method.init\n    /// [`Pointable::drop`]: trait.Pointable.html#method.drop\n    /// [`Pointable::deref`]: trait.Pointable.html#method.deref\n    /// [`Pointable::deref_mut`]: trait.Pointable.html#method.deref_mut\n    unsafe fn deref_mut<'a>(ptr: usize) -> &'a mut Self;\n\n    /// Drops the object pointed to by the given pointer.\n    ///\n    /// # Safety\n    ///\n    /// - The given `ptr` should have been initialized with [`Pointable::init`].\n    /// - `ptr` should not have yet been dropped by [`Pointable::drop`].\n    /// - `ptr` should not be dereferenced by [`Pointable::deref`] or [`Pointable::deref_mut`]\n    ///   concurrently.\n    ///\n    /// [`Pointable::init`]: trait.Pointable.html#method.init\n    /// [`Pointable::drop`]: trait.Pointable.html#method.drop\n    /// [`Pointable::deref`]: trait.Pointable.html#method.deref\n    /// [`Pointable::deref_mut`]: trait.Pointable.html#method.deref_mut\n    unsafe fn drop(ptr: usize);\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Pointer":["/// A trait for either `Owned` or `Shared` pointers.\npub trait Pointer<T: ?Sized + Pointable> {\n    /// Returns the machine representation of the pointer.\n    fn into_usize(self) -> usize;\n\n    /// Returns a new pointer pointing to the tagged pointer `data`.\n    ///\n    /// # Safety\n    ///\n    /// The given `data` should have been created by `Pointer::into_usize()`, and one `data` should\n    /// not be converted back by `Pointer::from_usize()` mutliple times.\n    unsafe fn from_usize(data: usize) -> Self;\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared":["/// A pointer to an object protected by the epoch GC.\n///\n/// The pointer is valid for use only during the lifetime `'g`.\n///\n/// The pointer must be properly aligned. Since it is aligned, a tag can be stored into the unused\n/// least significant bits of the address.\npub struct Shared<'g, T: 'g + ?Sized + Pointable> {\n    data: usize,\n    _marker: PhantomData<(&'g (), *const T)>,\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::as_raw":["/// Converts the pointer to a raw pointer (without the tag).\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let o = Owned::new(1234);\n/// let raw = &*o as *const _;\n/// let a = Atomic::from(o);\n///\n/// let guard = &epoch::pin();\n/// let p = a.load(SeqCst, guard);\n/// assert_eq!(p.as_raw(), raw);\n/// ```\n#[allow(clippy::trivially_copy_pass_by_ref)]\npub fn as_raw(&self) -> *const T{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        raw as *const _\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::as_ref":["/// Converts the pointer to a reference.\n///\n/// Returns `None` if the pointer is null, or else a reference to the object wrapped in `Some`.\n///\n/// # Safety\n///\n/// Dereferencing a pointer is unsafe because it could be pointing to invalid memory.\n///\n/// Another concern is the possiblity of data races due to lack of proper synchronization.\n/// For example, consider the following scenario:\n///\n/// 1. A thread creates a new object: `a.store(Owned::new(10), Relaxed)`\n/// 2. Another thread reads it: `*a.load(Relaxed, guard).as_ref().unwrap()`\n///\n/// The problem is that relaxed orderings don't synchronize initialization of the object with\n/// the read from the second thread. This is a data race. A possible solution would be to use\n/// `Release` and `Acquire` orderings.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// let guard = &epoch::pin();\n/// let p = a.load(SeqCst, guard);\n/// unsafe {\n///     assert_eq!(p.as_ref(), Some(&1234));\n/// }\n/// ```\n#[allow(clippy::trivially_copy_pass_by_ref)]\npub unsafe fn as_ref(&self) -> Option<&'g T>{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        if raw == 0 {\n            None\n        } else {\n            Some(T::deref(raw))\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::deref":["/// Dereferences the pointer.\n///\n/// Returns a reference to the pointee that is valid during the lifetime `'g`.\n///\n/// # Safety\n///\n/// Dereferencing a pointer is unsafe because it could be pointing to invalid memory.\n///\n/// Another concern is the possiblity of data races due to lack of proper synchronization.\n/// For example, consider the following scenario:\n///\n/// 1. A thread creates a new object: `a.store(Owned::new(10), Relaxed)`\n/// 2. Another thread reads it: `*a.load(Relaxed, guard).as_ref().unwrap()`\n///\n/// The problem is that relaxed orderings don't synchronize initialization of the object with\n/// the read from the second thread. This is a data race. A possible solution would be to use\n/// `Release` and `Acquire` orderings.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// let guard = &epoch::pin();\n/// let p = a.load(SeqCst, guard);\n/// unsafe {\n///     assert_eq!(p.deref(), &1234);\n/// }\n/// ```\n#[allow(clippy::trivially_copy_pass_by_ref)]\n#[allow(clippy::should_implement_trait)]\npub unsafe fn deref(&self) -> &'g T{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        T::deref(raw)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::deref_mut":["/// Dereferences the pointer.\n///\n/// Returns a mutable reference to the pointee that is valid during the lifetime `'g`.\n///\n/// # Safety\n///\n/// * There is no guarantee that there are no more threads attempting to read/write from/to the\n///   actual object at the same time.\n///\n///   The user must know that there are no concurrent accesses towards the object itself.\n///\n/// * Other than the above, all safety concerns of `deref()` applies here.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(vec![1, 2, 3, 4]);\n/// let guard = &epoch::pin();\n///\n/// let mut p = a.load(SeqCst, guard);\n/// unsafe {\n///     assert!(!p.is_null());\n///     let b = p.deref_mut();\n///     assert_eq!(b, &vec![1, 2, 3, 4]);\n///     b.push(5);\n///     assert_eq!(b, &vec![1, 2, 3, 4, 5]);\n/// }\n///\n/// let p = a.load(SeqCst, guard);\n/// unsafe {\n///     assert_eq!(p.deref(), &vec![1, 2, 3, 4, 5]);\n/// }\n/// ```\n#[allow(clippy::should_implement_trait)]\npub unsafe fn deref_mut(&mut self) -> &'g mut T{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        T::deref_mut(raw)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::into_owned":["/// Takes ownership of the pointee.\n///\n/// # Panics\n///\n/// Panics if this pointer is null, but only in debug mode.\n///\n/// # Safety\n///\n/// This method may be called only if the pointer is valid and nobody else is holding a\n/// reference to the same object.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(1234);\n/// unsafe {\n///     let guard = &epoch::unprotected();\n///     let p = a.load(SeqCst, guard);\n///     drop(p.into_owned());\n/// }\n/// ```\npub unsafe fn into_owned(self) -> Owned<T>{\n        debug_assert!(!self.is_null(), \"converting a null `Shared` into `Owned`\");\n        Owned::from_usize(self.data)\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::is_null":["/// Returns `true` if the pointer is null.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::null();\n/// let guard = &epoch::pin();\n/// assert!(a.load(SeqCst, guard).is_null());\n/// a.store(Owned::new(1234), SeqCst);\n/// assert!(!a.load(SeqCst, guard).is_null());\n/// ```\n#[allow(clippy::trivially_copy_pass_by_ref)]\npub fn is_null(&self) -> bool{\n        let (raw, _) = decompose_tag::<T>(self.data);\n        raw == 0\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::null":["/// Returns a new null pointer.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::Shared;\n///\n/// let p = Shared::<i32>::null();\n/// assert!(p.is_null());\n/// ```\npub fn null() -> Shared<'g, T>{\n        Shared {\n            data: 0,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::tag":["/// Returns the tag stored within the pointer.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::<u64>::from(Owned::new(0u64).with_tag(2));\n/// let guard = &epoch::pin();\n/// let p = a.load(SeqCst, guard);\n/// assert_eq!(p.tag(), 2);\n/// ```\n#[allow(clippy::trivially_copy_pass_by_ref)]\npub fn tag(&self) -> usize{\n        let (_, tag) = decompose_tag::<T>(self.data);\n        tag\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::Shared::<'g, T>::with_tag":["/// Returns the same pointer, but tagged with `tag`. `tag` is truncated to be fit into the\n/// unused bits of the pointer to `T`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(0u64);\n/// let guard = &epoch::pin();\n/// let p1 = a.load(SeqCst, guard);\n/// let p2 = p1.with_tag(2);\n///\n/// assert_eq!(p1.tag(), 0);\n/// assert_eq!(p2.tag(), 2);\n/// assert_eq!(p1.as_raw(), p2.as_raw());\n/// ```\n#[allow(clippy::trivially_copy_pass_by_ref)]\npub fn with_tag(&self, tag: usize) -> Shared<'g, T>{\n        unsafe { Self::from_usize(compose_tag::<T>(self.data, tag)) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::compose_tag":["/// Given a tagged pointer `data`, returns the same pointer, but tagged with `tag`.\n///\n/// `tag` is truncated to fit into the unused bits of the pointer to `T`.\n#[inline]\nfn compose_tag<T: ?Sized + Pointable>(data: usize, tag: usize) -> usize{\n    (data & !low_bits::<T>()) | (tag & low_bits::<T>())\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::decompose_tag":["/// Decomposes a tagged pointer `data` into the pointer and the tag.\n#[inline]\nfn decompose_tag<T: ?Sized + Pointable>(data: usize) -> (usize, usize){\n    (data & !low_bits::<T>(), data & low_bits::<T>())\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::ensure_aligned":["/// Panics if the pointer is not properly unaligned.\n#[inline]\nfn ensure_aligned<T: ?Sized + Pointable>(raw: usize){\n    assert_eq!(raw & low_bits::<T>(), 0, \"unaligned pointer\");\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::low_bits":["/// Returns a bitmask containing the unused least significant bits of an aligned pointer to `T`.\n#[inline]\nfn low_bits<T: ?Sized + Pointable>() -> usize{\n    (1 << T::ALIGN.trailing_zeros()) - 1\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"atomic::strongest_failure_ordering":["/// Given ordering for the success case in a compare-exchange operation, returns the strongest\n/// appropriate ordering for the failure case.\n#[inline]\nfn strongest_failure_ordering(ord: Ordering) -> Ordering{\n    use self::Ordering::*;\n    match ord {\n        Relaxed | Release => Relaxed,\n        Acquire | AcqRel => Acquire,\n        _ => SeqCst,\n    }\n}","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))"],"collector::Collector":["/// An epoch-based garbage collector.\npub struct Collector {\n    pub(crate) global: Arc<Global>,\n}","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"collector::Collector::new":["/// Creates a new collector.\npub fn new() -> Self{\n        Self::default()\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"collector::Collector::register":["/// Registers a new handle for the collector.\npub fn register(&self) -> LocalHandle{\n        Local::register(self)\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"collector::LocalHandle":["/// A handle to a garbage collector.\npub struct LocalHandle {\n    pub(crate) local: *const Local,\n}","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"collector::LocalHandle::collector":["/// Returns the `Collector` associated with this handle.\n#[inline]\npub fn collector(&self) -> &Collector{\n        unsafe { (*self.local).collector() }\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"collector::LocalHandle::is_pinned":["/// Returns `true` if the handle is pinned.\n#[inline]\npub fn is_pinned(&self) -> bool{\n        unsafe { (*self.local).is_pinned() }\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"collector::LocalHandle::pin":["/// Pins the handle.\n#[inline]\npub fn pin(&self) -> Guard{\n        unsafe { (*self.local).pin() }\n    }","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))"],"default::COLLECTOR":["#[allow(missing_copy_implementations)]\n#[allow(non_camel_case_types)]\n#[allow(dead_code)]\n#[$attr]\nstruct $N {__private_field: ()}","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"default::HANDLE::__getit":["inline\nunsafe fn __getit(\n                init: $crate::option::Option<&mut $crate::option::Option<$t>>,\n            ) -> $crate::option::Option<&'static $t>{\n                #[cfg(all(target_family = \"wasm\", not(target_feature = \"atomics\")))]\n                static __KEY: $crate::thread::__StaticLocalKeyInner<$t> =\n                    $crate::thread::__StaticLocalKeyInner::new();\n\n                #[thread_local]\n                #[cfg(all(\n                    target_thread_local,\n                    not(all(target_family = \"wasm\", not(target_feature = \"atomics\"))),\n                ))]\n                static __KEY: $crate::thread::__FastLocalKeyInner<$t> =\n                    $crate::thread::__FastLocalKeyInner::new();\n\n                #[cfg(all(\n                    not(target_thread_local),\n                    not(all(target_family = \"wasm\", not(target_feature = \"atomics\"))),\n                ))]\n                static __KEY: $crate::thread::__OsLocalKeyInner<$t> =\n                    $crate::thread::__OsLocalKeyInner::new();\n\n                // FIXME: remove the #[allow(...)] marker when macros don't\n                // raise warning for missing/extraneous unsafe blocks anymore.\n                // See https://github.com/rust-lang/rust/issues/74838.\n                #[allow(unused_unsafe)]\n                unsafe {\n                    __KEY.get(move || {\n                        if let $crate::option::Option::Some(init) = init {\n                            if let $crate::option::Option::Some(value) = init.take() {\n                                return value;\n                            } else if $crate::cfg!(debug_assertions) {\n                                $crate::unreachable!(\"missing default value\");\n                            }\n                        }\n                        __init()\n                    })\n                }\n            }","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })"],"default::HANDLE::__init":["#[inline]\nfn __init() -> $t{ $init }","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })"],"default::default_collector":["/// Returns the default global collector.\npub fn default_collector() -> &'static Collector{\n    &COLLECTOR\n}","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))"],"default::is_pinned":["/// Returns `true` if the current thread is pinned.\n#[inline]\npub fn is_pinned() -> bool{\n    with_handle(|handle| handle.is_pinned())\n}","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))"],"default::pin":["/// Pins the current thread.\n#[inline]\npub fn pin() -> Guard{\n    with_handle(|handle| handle.pin())\n}","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))"],"default::with_handle":["#[inline]\nfn with_handle<F, R>(mut f: F) -> R\nwhere\n    F: FnMut(&LocalHandle) -> R,{\n    HANDLE\n        .try_with(|h| f(h))\n        .unwrap_or_else(|_| f(&COLLECTOR.register()))\n}","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))"],"deferred::Deferred":["/// A `FnOnce()` that is stored inline if small, or otherwise boxed on the heap.\n///\n/// This is a handy way of keeping an unsized `FnOnce()` within a sized structure.\npub struct Deferred {\n    call: unsafe fn(*mut u8),\n    data: Data,\n    _marker: PhantomData<*mut ()>, // !Send + !Sync\n}","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))"],"deferred::Deferred::call":["/// Calls the function.\n#[inline]\npub fn call(mut self){\n        let call = self.call;\n        unsafe { call(&mut self.data as *mut Data as *mut u8) };\n    }","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))"],"deferred::Deferred::new":["/// Constructs a new `Deferred` from a `FnOnce()`.\npub fn new<F: FnOnce()>(f: F) -> Self{\n        let size = mem::size_of::<F>();\n        let align = mem::align_of::<F>();\n\n        unsafe {\n            if size <= mem::size_of::<Data>() && align <= mem::align_of::<Data>() {\n                let mut data = MaybeUninit::<Data>::uninit();\n                ptr::write(data.as_mut_ptr() as *mut F, f);\n\n                unsafe fn call<F: FnOnce()>(raw: *mut u8) {\n                    let f: F = ptr::read(raw as *mut F);\n                    f();\n                }\n\n                Deferred {\n                    call: call::<F>,\n                    data: data.assume_init(),\n                    _marker: PhantomData,\n                }\n            } else {\n                let b: Box<F> = Box::new(f);\n                let mut data = MaybeUninit::<Data>::uninit();\n                ptr::write(data.as_mut_ptr() as *mut Box<F>, b);\n\n                unsafe fn call<F: FnOnce()>(raw: *mut u8) {\n                    // It's safe to cast `raw` from `*mut u8` to `*mut Box<F>`, because `raw` is\n                    // originally derived from `*mut Box<F>`.\n                    #[allow(clippy::cast_ptr_alignment)]\n                    let b: Box<F> = ptr::read(raw as *mut Box<F>);\n                    (*b)();\n                }\n\n                Deferred {\n                    call: call::<F>,\n                    data: data.assume_init(),\n                    _marker: PhantomData,\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))"],"deferred::Deferred::new::call":["unsafe fn call<F: FnOnce()>(raw: *mut u8){\n                    // It's safe to cast `raw` from `*mut u8` to `*mut Box<F>`, because `raw` is\n                    // originally derived from `*mut Box<F>`.\n                    #[allow(clippy::cast_ptr_alignment)]\n                    let b: Box<F> = ptr::read(raw as *mut Box<F>);\n                    (*b)();\n                }","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))"],"epoch::AtomicEpoch":["/// An atomic value that holds an `Epoch`.\npub struct AtomicEpoch {\n    /// Since `Epoch` is just a wrapper around `usize`, an `AtomicEpoch` is similarly represented\n    /// using an `AtomicUsize`.\n    data: AtomicUsize,\n}","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::AtomicEpoch::compare_and_swap":["/// Stores a value into the atomic epoch if the current value is the same as `current`.\n///\n/// The return value is always the previous value. If it is equal to `current`, then the value\n/// is updated.\n///\n/// The `Ordering` argument describes the memory ordering of this operation.\n#[inline]\npub fn compare_and_swap(&self, current: Epoch, new: Epoch, ord: Ordering) -> Epoch{\n        let data = self.data.compare_and_swap(current.data, new.data, ord);\n        Epoch { data }\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::AtomicEpoch::load":["/// Loads a value from the atomic epoch.\n#[inline]\npub fn load(&self, ord: Ordering) -> Epoch{\n        Epoch {\n            data: self.data.load(ord),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::AtomicEpoch::new":["/// Creates a new atomic epoch.\n#[inline]\npub fn new(epoch: Epoch) -> Self{\n        let data = AtomicUsize::new(epoch.data);\n        AtomicEpoch { data }\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::AtomicEpoch::store":["/// Stores a value into the atomic epoch.\n#[inline]\npub fn store(&self, epoch: Epoch, ord: Ordering){\n        self.data.store(epoch.data, ord);\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch":["/// An epoch that can be marked as pinned or unpinned.\n///\n/// Internally, the epoch is represented as an integer that wraps around at some unspecified point\n/// and a flag that represents whether it is pinned or unpinned.\npub struct Epoch {\n    /// The least significant bit is set if pinned. The rest of the bits hold the epoch.\n    data: usize,\n}","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch::is_pinned":["/// Returns `true` if the epoch is marked as pinned.\n#[inline]\npub fn is_pinned(self) -> bool{\n        (self.data & 1) == 1\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch::pinned":["/// Returns the same epoch, but marked as pinned.\n#[inline]\npub fn pinned(self) -> Epoch{\n        Epoch {\n            data: self.data | 1,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch::starting":["/// Returns the starting epoch in unpinned state.\n#[inline]\npub fn starting() -> Self{\n        Self::default()\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch::successor":["/// Returns the successor epoch.\n///\n/// The returned epoch will be marked as pinned only if the previous one was as well.\n#[inline]\npub fn successor(self) -> Epoch{\n        Epoch {\n            data: self.data.wrapping_add(2),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch::unpinned":["/// Returns the same epoch, but marked as unpinned.\n#[inline]\npub fn unpinned(self) -> Epoch{\n        Epoch {\n            data: self.data & !1,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"epoch::Epoch::wrapping_sub":["/// Returns the number of epochs `self` is ahead of `rhs`.\n///\n/// Internally, epochs are represented as numbers in the range `(isize::MIN / 2) .. (isize::MAX\n/// / 2)`, so the returned distance will be in the same interval.\npub fn wrapping_sub(self, rhs: Self) -> isize{\n        // The result is the same with `(self.data & !1).wrapping_sub(rhs.data & !1) as isize >> 1`,\n        // because the possible difference of LSB in `(self.data & !1).wrapping_sub(rhs.data & !1)`\n        // will be ignored in the shift operation.\n        self.data.wrapping_sub(rhs.data & !1) as isize >> 1\n    }","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))"],"guard::Guard":["/// A guard that keeps the current thread pinned.\n///\n/// # Pinning\n///\n/// The current thread is pinned by calling [`pin`], which returns a new guard:\n///\n/// ```\n/// use crossbeam_epoch as epoch;\n///\n/// // It is often convenient to prefix a call to `pin` with a `&` in order to create a reference.\n/// // This is not really necessary, but makes passing references to the guard a bit easier.\n/// let guard = &epoch::pin();\n/// ```\n///\n/// When a guard gets dropped, the current thread is automatically unpinned.\n///\n/// # Pointers on the stack\n///\n/// Having a guard allows us to create pointers on the stack to heap-allocated objects.\n/// For example:\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// // Create a heap-allocated number.\n/// let a = Atomic::new(777);\n///\n/// // Pin the current thread.\n/// let guard = &epoch::pin();\n///\n/// // Load the heap-allocated object and create pointer `p` on the stack.\n/// let p = a.load(SeqCst, guard);\n///\n/// // Dereference the pointer and print the value:\n/// if let Some(num) = unsafe { p.as_ref() } {\n///     println!(\"The number is {}.\", num);\n/// }\n/// ```\n///\n/// # Multiple guards\n///\n/// Pinning is reentrant and it is perfectly legal to create multiple guards. In that case, the\n/// thread will actually be pinned only when the first guard is created and unpinned when the last\n/// one is dropped:\n///\n/// ```\n/// use crossbeam_epoch as epoch;\n///\n/// let guard1 = epoch::pin();\n/// let guard2 = epoch::pin();\n/// assert!(epoch::is_pinned());\n/// drop(guard1);\n/// assert!(epoch::is_pinned());\n/// drop(guard2);\n/// assert!(!epoch::is_pinned());\n/// ```\n///\n/// [`pin`]: fn.pin.html\npub struct Guard {\n    pub(crate) local: *const Local,\n}","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::collector":["/// Returns the `Collector` associated with this guard.\n///\n/// This method is useful when you need to ensure that all guards used with\n/// a data structure come from the same collector.\n///\n/// If this method is called from an [`unprotected`] guard, then `None` is returned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch as epoch;\n///\n/// let guard1 = epoch::pin();\n/// let guard2 = epoch::pin();\n/// assert!(guard1.collector() == guard2.collector());\n/// ```\n///\n/// [`unprotected`]: fn.unprotected.html\npub fn collector(&self) -> Option<&Collector>{\n        unsafe { self.local.as_ref().map(|local| local.collector()) }\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::defer":["/// Stores a function so that it can be executed at some point after all currently pinned\n/// threads get unpinned.\n///\n/// This method first stores `f` into the thread-local (or handle-local) cache. If this cache\n/// becomes full, some functions are moved into the global cache. At the same time, some\n/// functions from both local and global caches may get executed in order to incrementally\n/// clean up the caches as they fill up.\n///\n/// There is no guarantee when exactly `f` will be executed. The only guarantee is that it\n/// won't be executed until all currently pinned threads get unpinned. In theory, `f` might\n/// never run, but the epoch-based garbage collection will make an effort to execute it\n/// reasonably soon.\n///\n/// If this method is called from an [`unprotected`] guard, the function will simply be\n/// executed immediately.\n///\n/// [`unprotected`]: fn.unprotected.html\npub fn defer<F, R>(&self, f: F)\n    where\n        F: FnOnce() -> R,\n        F: Send + 'static,{\n        unsafe {\n            self.defer_unchecked(f);\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::defer_destroy":["/// Stores a destructor for an object so that it can be deallocated and dropped at some point\n/// after all currently pinned threads get unpinned.\n///\n/// This method first stores the destructor into the thread-local (or handle-local) cache. If\n/// this cache becomes full, some destructors are moved into the global cache. At the same\n/// time, some destructors from both local and global caches may get executed in order to\n/// incrementally clean up the caches as they fill up.\n///\n/// There is no guarantee when exactly the destructor will be executed. The only guarantee is\n/// that it won't be executed until all currently pinned threads get unpinned. In theory, the\n/// destructor might never run, but the epoch-based garbage collection will make an effort to\n/// execute it reasonably soon.\n///\n/// If this method is called from an [`unprotected`] guard, the destructor will simply be\n/// executed immediately.\n///\n/// # Safety\n///\n/// The object must not be reachable by other threads anymore, otherwise it might be still in\n/// use when the destructor runs.\n///\n/// Apart from that, keep in mind that another thread may execute the destructor, so the object\n/// must be sendable to other threads.\n///\n/// We intentionally didn't require `T: Send`, because Rust's type systems usually cannot prove\n/// `T: Send` for typical use cases. For example, consider the following code snippet, which\n/// exemplifies the typical use case of deferring the deallocation of a shared reference:\n///\n/// ```ignore\n/// let shared = Owned::new(7i32).into_shared(guard);\n/// guard.defer_destroy(shared); // `Shared` is not `Send`!\n/// ```\n///\n/// While `Shared` is not `Send`, it's safe for another thread to call the destructor, because\n/// it's called only after the grace period and `shared` is no longer shared with other\n/// threads. But we don't expect type systems to prove this.\n///\n/// # Examples\n///\n/// When a heap-allocated object in a data structure becomes unreachable, it has to be\n/// deallocated. However, the current thread and other threads may be still holding references\n/// on the stack to that same object. Therefore it cannot be deallocated before those references\n/// get dropped. This method can defer deallocation until all those threads get unpinned and\n/// consequently drop all their references on the stack.\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(\"foo\");\n///\n/// // Now suppose that `a` is shared among multiple threads and concurrently\n/// // accessed and modified...\n///\n/// // Pin the current thread.\n/// let guard = &epoch::pin();\n///\n/// // Steal the object currently stored in `a` and swap it with another one.\n/// let p = a.swap(Owned::new(\"bar\").into_shared(guard), SeqCst, guard);\n///\n/// if !p.is_null() {\n///     // The object `p` is pointing to is now unreachable.\n///     // Defer its deallocation until all currently pinned threads get unpinned.\n///     unsafe {\n///         guard.defer_destroy(p);\n///     }\n/// }\n/// ```\n///\n/// [`unprotected`]: fn.unprotected.html\npub unsafe fn defer_destroy<T>(&self, ptr: Shared<'_, T>){\n        self.defer_unchecked(move || ptr.into_owned());\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::defer_unchecked":["/// Stores a function so that it can be executed at some point after all currently pinned\n/// threads get unpinned.\n///\n/// This method first stores `f` into the thread-local (or handle-local) cache. If this cache\n/// becomes full, some functions are moved into the global cache. At the same time, some\n/// functions from both local and global caches may get executed in order to incrementally\n/// clean up the caches as they fill up.\n///\n/// There is no guarantee when exactly `f` will be executed. The only guarantee is that it\n/// won't be executed until all currently pinned threads get unpinned. In theory, `f` might\n/// never run, but the epoch-based garbage collection will make an effort to execute it\n/// reasonably soon.\n///\n/// If this method is called from an [`unprotected`] guard, the function will simply be\n/// executed immediately.\n///\n/// # Safety\n///\n/// The given function must not hold reference onto the stack. It is highly recommended that\n/// the passed function is **always** marked with `move` in order to prevent accidental\n/// borrows.\n///\n/// ```\n/// use crossbeam_epoch as epoch;\n///\n/// let guard = &epoch::pin();\n/// let message = \"Hello!\";\n/// unsafe {\n///     // ALWAYS use `move` when sending a closure into `defer_unchecked`.\n///     guard.defer_unchecked(move || {\n///         println!(\"{}\", message);\n///     });\n/// }\n/// ```\n///\n/// Apart from that, keep in mind that another thread may execute `f`, so anything accessed by\n/// the closure must be `Send`.\n///\n/// We intentionally didn't require `F: Send`, because Rust's type systems usually cannot prove\n/// `F: Send` for typical use cases. For example, consider the following code snippet, which\n/// exemplifies the typical use case of deferring the deallocation of a shared reference:\n///\n/// ```ignore\n/// let shared = Owned::new(7i32).into_shared(guard);\n/// guard.defer_unchecked(move || shared.into_owned()); // `Shared` is not `Send`!\n/// ```\n///\n/// While `Shared` is not `Send`, it's safe for another thread to call the deferred function,\n/// because it's called only after the grace period and `shared` is no longer shared with other\n/// threads. But we don't expect type systems to prove this.\n///\n/// # Examples\n///\n/// When a heap-allocated object in a data structure becomes unreachable, it has to be\n/// deallocated. However, the current thread and other threads may be still holding references\n/// on the stack to that same object. Therefore it cannot be deallocated before those references\n/// get dropped. This method can defer deallocation until all those threads get unpinned and\n/// consequently drop all their references on the stack.\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic, Owned};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(\"foo\");\n///\n/// // Now suppose that `a` is shared among multiple threads and concurrently\n/// // accessed and modified...\n///\n/// // Pin the current thread.\n/// let guard = &epoch::pin();\n///\n/// // Steal the object currently stored in `a` and swap it with another one.\n/// let p = a.swap(Owned::new(\"bar\").into_shared(guard), SeqCst, guard);\n///\n/// if !p.is_null() {\n///     // The object `p` is pointing to is now unreachable.\n///     // Defer its deallocation until all currently pinned threads get unpinned.\n///     unsafe {\n///         // ALWAYS use `move` when sending a closure into `defer_unchecked`.\n///         guard.defer_unchecked(move || {\n///             println!(\"{} is now being deallocated.\", p.deref());\n///             // Now we have unique access to the object pointed to by `p` and can turn it\n///             // into an `Owned`. Dropping the `Owned` will deallocate the object.\n///             drop(p.into_owned());\n///         });\n///     }\n/// }\n/// ```\n///\n/// [`unprotected`]: fn.unprotected.html\npub unsafe fn defer_unchecked<F, R>(&self, f: F)\n    where\n        F: FnOnce() -> R,{\n        if let Some(local) = self.local.as_ref() {\n            local.defer(Deferred::new(move || drop(f())), self);\n        } else {\n            drop(f());\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::flush":["/// Clears up the thread-local cache of deferred functions by executing them or moving into the\n/// global cache.\n///\n/// Call this method after deferring execution of a function if you want to get it executed as\n/// soon as possible. Flushing will make sure it is residing in in the global cache, so that\n/// any thread has a chance of taking the function and executing it.\n///\n/// If this method is called from an [`unprotected`] guard, it is a no-op (nothing happens).\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch as epoch;\n///\n/// let guard = &epoch::pin();\n/// guard.defer(move || {\n///     println!(\"This better be printed as soon as possible!\");\n/// });\n/// guard.flush();\n/// ```\n///\n/// [`unprotected`]: fn.unprotected.html\npub fn flush(&self){\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.flush(self);\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::repin":["/// Unpins and then immediately re-pins the thread.\n///\n/// This method is useful when you don't want delay the advancement of the global epoch by\n/// holding an old epoch. For safety, you should not maintain any guard-based reference across\n/// the call (the latter is enforced by `&mut self`). The thread will only be repinned if this\n/// is the only active guard for the current thread.\n///\n/// If this method is called from an [`unprotected`] guard, then the call will be just no-op.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// let a = Atomic::new(777);\n/// let mut guard = epoch::pin();\n/// {\n///     let p = a.load(SeqCst, &guard);\n///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n/// }\n/// guard.repin();\n/// {\n///     let p = a.load(SeqCst, &guard);\n///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n/// }\n/// ```\n///\n/// [`unprotected`]: fn.unprotected.html\npub fn repin(&mut self){\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            local.repin();\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::Guard::repin_after":["/// Temporarily unpins the thread, executes the given function and then re-pins the thread.\n///\n/// This method is useful when you need to perform a long-running operation (e.g. sleeping)\n/// and don't need to maintain any guard-based reference across the call (the latter is enforced\n/// by `&mut self`). The thread will only be unpinned if this is the only active guard for the\n/// current thread.\n///\n/// If this method is called from an [`unprotected`] guard, then the passed function is called\n/// directly without unpinning the thread.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::SeqCst;\n/// use std::thread;\n/// use std::time::Duration;\n///\n/// let a = Atomic::new(777);\n/// let mut guard = epoch::pin();\n/// {\n///     let p = a.load(SeqCst, &guard);\n///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n/// }\n/// guard.repin_after(|| thread::sleep(Duration::from_millis(50)));\n/// {\n///     let p = a.load(SeqCst, &guard);\n///     assert_eq!(unsafe { p.as_ref() }, Some(&777));\n/// }\n/// ```\n///\n/// [`unprotected`]: fn.unprotected.html\npub fn repin_after<F, R>(&mut self, f: F) -> R\n    where\n        F: FnOnce() -> R,{\n        if let Some(local) = unsafe { self.local.as_ref() } {\n            // We need to acquire a handle here to ensure the Local doesn't\n            // disappear from under us.\n            local.acquire_handle();\n            local.unpin();\n        }\n\n        // Ensure the Guard is re-pinned even if the function panics\n        defer! {\n            if let Some(local) = unsafe { self.local.as_ref() } {\n                mem::forget(local.pin());\n                local.release_handle();\n            }\n        }\n\n        f()\n    }","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"guard::unprotected":["/// Returns a reference to a dummy guard that allows unprotected access to [`Atomic`]s.\n///\n/// This guard should be used in special occasions only. Note that it doesn't actually keep any\n/// thread pinned - it's just a fake guard that allows loading from [`Atomic`]s unsafely.\n///\n/// Note that calling [`defer`] with a dummy guard will not defer the function - it will just\n/// execute the function immediately.\n///\n/// If necessary, it's possible to create more dummy guards by cloning: `unprotected().clone()`.\n///\n/// # Safety\n///\n/// Loading and dereferencing data from an [`Atomic`] using this guard is safe only if the\n/// [`Atomic`] is not being concurrently modified by other threads.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::sync::atomic::Ordering::Relaxed;\n///\n/// let a = Atomic::new(7);\n///\n/// unsafe {\n///     // Load `a` without pinning the current thread.\n///     a.load(Relaxed, epoch::unprotected());\n///\n///     // It's possible to create more dummy guards by calling `clone()`.\n///     let dummy = &epoch::unprotected().clone();\n///\n///     dummy.defer(move || {\n///         println!(\"This gets executed immediately.\");\n///     });\n///\n///     // Dropping `dummy` doesn't affect the current thread - it's just a noop.\n/// }\n/// ```\n///\n/// The most common use of this function is when constructing or destructing a data structure.\n///\n/// For example, we can use a dummy guard in the destructor of a Treiber stack because at that\n/// point no other thread could concurrently modify the [`Atomic`]s we are accessing.\n///\n/// If we were to actually pin the current thread during destruction, that would just unnecessarily\n/// delay garbage collection and incur some performance cost, so in cases like these `unprotected`\n/// is very helpful.\n///\n/// ```\n/// use crossbeam_epoch::{self as epoch, Atomic};\n/// use std::mem::ManuallyDrop;\n/// use std::sync::atomic::Ordering::Relaxed;\n///\n/// struct Stack<T> {\n///     head: Atomic<Node<T>>,\n/// }\n///\n/// struct Node<T> {\n///     data: ManuallyDrop<T>,\n///     next: Atomic<Node<T>>,\n/// }\n///\n/// impl<T> Drop for Stack<T> {\n///     fn drop(&mut self) {\n///         unsafe {\n///             // Unprotected load.\n///             let mut node = self.head.load(Relaxed, epoch::unprotected());\n///\n///             while let Some(n) = node.as_ref() {\n///                 // Unprotected load.\n///                 let next = n.next.load(Relaxed, epoch::unprotected());\n///\n///                 // Take ownership of the node, then drop its data and deallocate it.\n///                 let mut o = node.into_owned();\n///                 ManuallyDrop::drop(&mut o.data);\n///                 drop(o);\n///\n///                 node = next;\n///             }\n///         }\n///     }\n/// }\n/// ```\n///\n/// [`Atomic`]: struct.Atomic.html\n/// [`defer`]: struct.Guard.html#method.defer\n#[inline]\npub unsafe fn unprotected() -> &'static Guard{\n    // HACK(stjepang): An unprotected guard is just a `Guard` with its field `local` set to null.\n    // Since this function returns a `'static` reference to a `Guard`, we must return a reference\n    // to a global guard. However, it's not possible to create a `static` `Guard` because it does\n    // not implement `Sync`. To get around the problem, we create a static `usize` initialized to\n    // zero and then transmute it into a `Guard`. This is safe because `usize` and `Guard`\n    // (consisting of a single pointer) have the same representation in memory.\n    static UNPROTECTED: usize = 0;\n    &*(&UNPROTECTED as *const _ as *const Guard)\n}","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))"],"internal::Bag":["/// A bag of deferred functions.\npub struct Bag {\n    /// Stashed objects.\n    deferreds: [Deferred; MAX_OBJECTS],\n    len: usize,\n}","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Bag::is_empty":["/// Returns `true` if the bag is empty.\npub fn is_empty(&self) -> bool{\n        self.len == 0\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Bag::new":["/// Returns a new, empty bag.\npub fn new() -> Self{\n        Self::default()\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Bag::seal":["/// Seals the bag with the given epoch.\nfn seal(self, epoch: Epoch) -> SealedBag{\n        SealedBag { epoch, bag: self }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Bag::try_push":["/// Attempts to insert a deferred function into the bag.\n///\n/// Returns `Ok(())` if successful, and `Err(deferred)` for the given `deferred` if the bag is\n/// full.\n///\n/// # Safety\n///\n/// It should be safe for another thread to execute the given function.\npub unsafe fn try_push(&mut self, deferred: Deferred) -> Result<(), Deferred>{\n        if self.len < MAX_OBJECTS {\n            self.deferreds[self.len] = deferred;\n            self.len += 1;\n            Ok(())\n        } else {\n            Err(deferred)\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Global":["/// The global data for a garbage collector.\npub struct Global {\n    /// The intrusive linked list of `Local`s.\n    locals: List<Local>,\n\n    /// The global queue of bags of deferred functions.\n    queue: Queue<SealedBag>,\n\n    /// The global epoch.\n    pub(crate) epoch: CachePadded<AtomicEpoch>,\n}","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Global::collect":["/// Collects several bags from the global queue and executes deferred functions in them.\n///\n/// Note: This may itself produce garbage and in turn allocate new bags.\n///\n/// `pin()` rarely calls `collect()`, so we want the compiler to place that call on a cold\n/// path. In other words, we want the compiler to optimize branching for the case when\n/// `collect()` is not called.\n#[cold]\npub fn collect(&self, guard: &Guard){\n        let global_epoch = self.try_advance(guard);\n\n        let steps = if cfg!(feature = \"sanitize\") {\n            usize::max_value()\n        } else {\n            Self::COLLECT_STEPS\n        };\n\n        for _ in 0..steps {\n            match self.queue.try_pop_if(\n                &|sealed_bag: &SealedBag| sealed_bag.is_expired(global_epoch),\n                guard,\n            ) {\n                None => break,\n                Some(sealed_bag) => drop(sealed_bag),\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Global::new":["/// Creates a new global data for garbage collection.\n#[inline]\npub fn new() -> Self{\n        Self {\n            locals: List::new(),\n            queue: Queue::new(),\n            epoch: CachePadded::new(AtomicEpoch::new(Epoch::starting())),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Global::push_bag":["/// Pushes the bag into the global queue and replaces the bag with a new empty bag.\npub fn push_bag(&self, bag: &mut Bag, guard: &Guard){\n        let bag = mem::replace(bag, Bag::new());\n\n        atomic::fence(Ordering::SeqCst);\n\n        let epoch = self.epoch.load(Ordering::Relaxed);\n        self.queue.push(bag.seal(epoch), guard);\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Global::try_advance":["/// Attempts to advance the global epoch.\n///\n/// The global epoch can advance only if all currently pinned participants have been pinned in\n/// the current epoch.\n///\n/// Returns the current global epoch.\n///\n/// `try_advance()` is annotated `#[cold]` because it is rarely called.\n#[cold]\npub fn try_advance(&self, guard: &Guard) -> Epoch{\n        let global_epoch = self.epoch.load(Ordering::Relaxed);\n        atomic::fence(Ordering::SeqCst);\n\n        // TODO(stjepang): `Local`s are stored in a linked list because linked lists are fairly\n        // easy to implement in a lock-free manner. However, traversal can be slow due to cache\n        // misses and data dependencies. We should experiment with other data structures as well.\n        for local in self.locals.iter(&guard) {\n            match local {\n                Err(IterError::Stalled) => {\n                    // A concurrent thread stalled this iteration. That thread might also try to\n                    // advance the epoch, in which case we leave the job to it. Otherwise, the\n                    // epoch will not be advanced.\n                    return global_epoch;\n                }\n                Ok(local) => {\n                    let local_epoch = local.epoch.load(Ordering::Relaxed);\n\n                    // If the participant was pinned in a different epoch, we cannot advance the\n                    // global epoch just yet.\n                    if local_epoch.is_pinned() && local_epoch.unpinned() != global_epoch {\n                        return global_epoch;\n                    }\n                }\n            }\n        }\n        atomic::fence(Ordering::Acquire);\n\n        // All pinned participants were pinned in the current global epoch.\n        // Now let's advance the global epoch...\n        //\n        // Note that if another thread already advanced it before us, this store will simply\n        // overwrite the global epoch with the same value. This is true because `try_advance` was\n        // called from a thread that was pinned in `global_epoch`, and the global epoch cannot be\n        // advanced two steps ahead of it.\n        let new_epoch = global_epoch.successor();\n        self.epoch.store(new_epoch, Ordering::Release);\n        new_epoch\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local":["/// Participant for garbage collection.\npub struct Local {\n    /// A node in the intrusive linked list of `Local`s.\n    entry: Entry,\n\n    /// The local epoch.\n    epoch: AtomicEpoch,\n\n    /// A reference to the global data.\n    ///\n    /// When all guards and handles get dropped, this reference is destroyed.\n    collector: UnsafeCell<ManuallyDrop<Collector>>,\n\n    /// The local bag of deferred functions.\n    pub(crate) bag: UnsafeCell<Bag>,\n\n    /// The number of guards keeping this participant pinned.\n    guard_count: Cell<usize>,\n\n    /// The number of active handles.\n    handle_count: Cell<usize>,\n\n    /// Total number of pinnings performed.\n    ///\n    /// This is just an auxilliary counter that sometimes kicks off collection.\n    pin_count: Cell<Wrapping<usize>>,\n}","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::acquire_handle":["/// Increments the handle count.\n#[inline]\npub fn acquire_handle(&self){\n        let handle_count = self.handle_count.get();\n        debug_assert!(handle_count >= 1);\n        self.handle_count.set(handle_count + 1);\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::collector":["/// Returns a reference to the `Collector` in which this `Local` resides.\n#[inline]\npub fn collector(&self) -> &Collector{\n        unsafe { &**self.collector.get() }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::defer":["/// Adds `deferred` to the thread-local bag.\n///\n/// # Safety\n///\n/// It should be safe for another thread to execute the given function.\npub unsafe fn defer(&self, mut deferred: Deferred, guard: &Guard){\n        let bag = &mut *self.bag.get();\n\n        while let Err(d) = bag.try_push(deferred) {\n            self.global().push_bag(bag, guard);\n            deferred = d;\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::finalize":["/// Removes the `Local` from the global linked list.\n#[cold]\nfn finalize(&self){\n        debug_assert_eq!(self.guard_count.get(), 0);\n        debug_assert_eq!(self.handle_count.get(), 0);\n\n        // Temporarily increment handle count. This is required so that the following call to `pin`\n        // doesn't call `finalize` again.\n        self.handle_count.set(1);\n        unsafe {\n            // Pin and move the local bag into the global queue. It's important that `push_bag`\n            // doesn't defer destruction on any new garbage.\n            let guard = &self.pin();\n            self.global().push_bag(&mut *self.bag.get(), guard);\n        }\n        // Revert the handle count back to zero.\n        self.handle_count.set(0);\n\n        unsafe {\n            // Take the reference to the `Global` out of this `Local`. Since we're not protected\n            // by a guard at this time, it's crucial that the reference is read before marking the\n            // `Local` as deleted.\n            let collector: Collector = ptr::read(&*(*self.collector.get()));\n\n            // Mark this node in the linked list as deleted.\n            self.entry.delete(unprotected());\n\n            // Finally, drop the reference to the global. Note that this might be the last reference\n            // to the `Global`. If so, the global data will be destroyed and all deferred functions\n            // in its queue will be executed.\n            drop(collector);\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::flush":["pub fn flush(&self, guard: &Guard){\n        let bag = unsafe { &mut *self.bag.get() };\n\n        if !bag.is_empty() {\n            self.global().push_bag(bag, guard);\n        }\n\n        self.global().collect(guard);\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::global":["/// Returns a reference to the `Global` in which this `Local` resides.\n#[inline]\npub fn global(&self) -> &Global{\n        &self.collector().global\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::is_pinned":["/// Returns `true` if the current participant is pinned.\n#[inline]\npub fn is_pinned(&self) -> bool{\n        self.guard_count.get() > 0\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::pin":["/// Pins the `Local`.\n#[inline]\npub fn pin(&self) -> Guard{\n        let guard = Guard { local: self };\n\n        let guard_count = self.guard_count.get();\n        self.guard_count.set(guard_count.checked_add(1).unwrap());\n\n        if guard_count == 0 {\n            let global_epoch = self.global().epoch.load(Ordering::Relaxed);\n            let new_epoch = global_epoch.pinned();\n\n            // Now we must store `new_epoch` into `self.epoch` and execute a `SeqCst` fence.\n            // The fence makes sure that any future loads from `Atomic`s will not happen before\n            // this store.\n            if cfg!(any(target_arch = \"x86\", target_arch = \"x86_64\")) {\n                // HACK(stjepang): On x86 architectures there are two different ways of executing\n                // a `SeqCst` fence.\n                //\n                // 1. `atomic::fence(SeqCst)`, which compiles into a `mfence` instruction.\n                // 2. `_.compare_and_swap(_, _, SeqCst)`, which compiles into a `lock cmpxchg`\n                //    instruction.\n                //\n                // Both instructions have the effect of a full barrier, but benchmarks have shown\n                // that the second one makes pinning faster in this particular case.  It is not\n                // clear that this is permitted by the C++ memory model (SC fences work very\n                // differently from SC accesses), but experimental evidence suggests that this\n                // works fine.  Using inline assembly would be a viable (and correct) alternative,\n                // but alas, that is not possible on stable Rust.\n                let current = Epoch::starting();\n                let previous = self\n                    .epoch\n                    .compare_and_swap(current, new_epoch, Ordering::SeqCst);\n                debug_assert_eq!(current, previous, \"participant was expected to be unpinned\");\n                // We add a compiler fence to make it less likely for LLVM to do something wrong\n                // here.  Formally, this is not enough to get rid of data races; practically,\n                // it should go a long way.\n                atomic::compiler_fence(Ordering::SeqCst);\n            } else {\n                self.epoch.store(new_epoch, Ordering::Relaxed);\n                atomic::fence(Ordering::SeqCst);\n            }\n\n            // Increment the pin counter.\n            let count = self.pin_count.get();\n            self.pin_count.set(count + Wrapping(1));\n\n            // After every `PINNINGS_BETWEEN_COLLECT` try advancing the epoch and collecting\n            // some garbage.\n            if count.0 % Self::PINNINGS_BETWEEN_COLLECT == 0 {\n                self.global().collect(&guard);\n            }\n        }\n\n        guard\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::register":["/// Registers a new `Local` in the provided `Global`.\npub fn register(collector: &Collector) -> LocalHandle{\n        unsafe {\n            // Since we dereference no pointers in this block, it is safe to use `unprotected`.\n\n            let local = Owned::new(Local {\n                entry: Entry::default(),\n                epoch: AtomicEpoch::new(Epoch::starting()),\n                collector: UnsafeCell::new(ManuallyDrop::new(collector.clone())),\n                bag: UnsafeCell::new(Bag::new()),\n                guard_count: Cell::new(0),\n                handle_count: Cell::new(1),\n                pin_count: Cell::new(Wrapping(0)),\n            })\n            .into_shared(unprotected());\n            collector.global.locals.insert(local, unprotected());\n            LocalHandle {\n                local: local.as_raw(),\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::release_handle":["/// Decrements the handle count.\n#[inline]\npub fn release_handle(&self){\n        let guard_count = self.guard_count.get();\n        let handle_count = self.handle_count.get();\n        debug_assert!(handle_count >= 1);\n        self.handle_count.set(handle_count - 1);\n\n        if guard_count == 0 && handle_count == 1 {\n            self.finalize();\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::repin":["/// Unpins and then pins the `Local`.\n#[inline]\npub fn repin(&self){\n        let guard_count = self.guard_count.get();\n\n        // Update the local epoch only if there's only one guard.\n        if guard_count == 1 {\n            let epoch = self.epoch.load(Ordering::Relaxed);\n            let global_epoch = self.global().epoch.load(Ordering::Relaxed).pinned();\n\n            // Update the local epoch only if the global epoch is greater than the local epoch.\n            if epoch != global_epoch {\n                // We store the new epoch with `Release` because we need to ensure any memory\n                // accesses from the previous epoch do not leak into the new one.\n                self.epoch.store(global_epoch, Ordering::Release);\n\n                // However, we don't need a following `SeqCst` fence, because it is safe for memory\n                // accesses from the new epoch to be executed before updating the local epoch. At\n                // worse, other threads will see the new epoch late and delay GC slightly.\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::Local::unpin":["/// Unpins the `Local`.\n#[inline]\npub fn unpin(&self){\n        let guard_count = self.guard_count.get();\n        self.guard_count.set(guard_count - 1);\n\n        if guard_count == 1 {\n            self.epoch.store(Epoch::starting(), Ordering::Release);\n\n            if self.handle_count.get() == 0 {\n                self.finalize();\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::SealedBag":["/// A pair of an epoch and a bag.\nstruct SealedBag {\n    epoch: Epoch,\n    bag: Bag,\n}","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::SealedBag::is_expired":["/// Checks if it is safe to drop the bag w.r.t. the given global epoch.\nfn is_expired(&self, global_epoch: Epoch) -> bool{\n        // A pinned participant can witness at most one epoch advancement. Therefore, any bag that\n        // is within one epoch of the current one cannot be destroyed yet.\n        global_epoch.wrapping_sub(self.epoch) >= 2\n    }","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"internal::no_op_func":["fn no_op_func(){}","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))"],"sync::list::Entry":["/// An entry in a linked list.\n///\n/// An Entry is accessed from multiple threads, so it would be beneficial to put it in a different\n/// cache-line than thread-local data in terms of performance.\npub struct Entry {\n    /// The next entry in the linked list.\n    /// If the tag is 1, this entry is marked as deleted.\n    next: Atomic<Entry>,\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::Entry::delete":["/// Marks this entry as deleted, deferring the actual deallocation to a later iteration.\n///\n/// # Safety\n///\n/// The entry should be a member of a linked list, and it should not have been deleted.\n/// It should be safe to call `C::finalize` on the entry after the `guard` is dropped, where `C`\n/// is the associated helper for the linked list.\npub unsafe fn delete(&self, guard: &Guard){\n        self.next.fetch_or(1, Release, guard);\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::IsElement":["/// Implementing this trait asserts that the type `T` can be used as an element in the intrusive\n/// linked list defined in this module. `T` has to contain (or otherwise be linked to) an instance\n/// of `Entry`.\n///\n/// # Example\n///\n/// ```ignore\n/// struct A {\n///     entry: Entry,\n///     data: usize,\n/// }\n///\n/// impl IsElement<A> for A {\n///     fn entry_of(a: &A) -> &Entry {\n///         let entry_ptr = ((a as usize) + offset_of!(A, entry)) as *const Entry;\n///         unsafe { &*entry_ptr }\n///     }\n///\n///     unsafe fn element_of(entry: &Entry) -> &T {\n///         let elem_ptr = ((entry as usize) - offset_of!(A, entry)) as *const T;\n///         &*elem_ptr\n///     }\n///\n///     unsafe fn finalize(entry: &Entry, guard: &Guard) {\n///         guard.defer_destroy(Shared::from(Self::element_of(entry) as *const _));\n///     }\n/// }\n/// ```\n///\n/// This trait is implemented on a type separate from `T` (although it can be just `T`), because\n/// one type might be placeable into multiple lists, in which case it would require multiple\n/// implementations of `IsElement`. In such cases, each struct implementing `IsElement<T>`\n/// represents a distinct `Entry` in `T`.\n///\n/// For example, we can insert the following struct into two lists using `entry1` for one\n/// and `entry2` for the other:\n///\n/// ```ignore\n/// struct B {\n///     entry1: Entry,\n///     entry2: Entry,\n///     data: usize,\n/// }\n/// ```\n///\npub trait IsElement<T> {\n    /// Returns a reference to this element's `Entry`.\n    fn entry_of(_: &T) -> &Entry;\n\n    /// Given a reference to an element's entry, returns that element.\n    ///\n    /// ```ignore\n    /// let elem = ListElement::new();\n    /// assert_eq!(elem.entry_of(),\n    ///            unsafe { ListElement::element_of(elem.entry_of()) } );\n    /// ```\n    ///\n    /// # Safety\n    ///\n    /// The caller has to guarantee that the `Entry` is called with was retrieved from an instance\n    /// of the element type (`T`).\n    unsafe fn element_of(_: &Entry) -> &T;\n\n    /// The function that is called when an entry is unlinked from list.\n    ///\n    /// # Safety\n    ///\n    /// The caller has to guarantee that the `Entry` is called with was retrieved from an instance\n    /// of the element type (`T`).\n    unsafe fn finalize(_: &Entry, _: &Guard);\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::Iter":["/// An iterator used for retrieving values from the list.\npub struct Iter<'g, T, C: IsElement<T>> {\n    /// The guard that protects the iteration.\n    guard: &'g Guard,\n\n    /// Pointer from the predecessor to the current entry.\n    pred: &'g Atomic<Entry>,\n\n    /// The current entry.\n    curr: Shared<'g, Entry>,\n\n    /// The list head, needed for restarting iteration.\n    head: &'g Atomic<Entry>,\n\n    /// Logically, we store a borrow of an instance of `T` and\n    /// use the type information from `C`.\n    _marker: PhantomData<(&'g T, C)>,\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::IterError":["/// An error that occurs during iteration over the list.\npub enum IterError {\n    /// A concurrent thread modified the state of the list at the same place that this iterator\n    /// was inspecting. Subsequent iteration will restart from the beginning of the list.\n    Stalled,\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::List":["/// A lock-free, intrusive linked list of type `T`.\npub struct List<T, C: IsElement<T> = T> {\n    /// The head of the linked list.\n    head: Atomic<Entry>,\n\n    /// The phantom data for using `T` and `C`.\n    _marker: PhantomData<(T, C)>,\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::List::<T, C>::insert":["/// Inserts `entry` into the head of the list.\n///\n/// # Safety\n///\n/// You should guarantee that:\n///\n/// - `container` is not null\n/// - `container` is immovable, e.g. inside an `Owned`\n/// - the same `Entry` is not inserted more than once\n/// - the inserted object will be removed before the list is dropped\npub unsafe fn insert<'g>(&'g self, container: Shared<'g, T>, guard: &'g Guard){\n        // Insert right after head, i.e. at the beginning of the list.\n        let to = &self.head;\n        // Get the intrusively stored Entry of the new element to insert.\n        let entry: &Entry = C::entry_of(container.deref());\n        // Make a Shared ptr to that Entry.\n        let entry_ptr = Shared::from(entry as *const _);\n        // Read the current successor of where we want to insert.\n        let mut next = to.load(Relaxed, guard);\n\n        loop {\n            // Set the Entry of the to-be-inserted element to point to the previous successor of\n            // `to`.\n            entry.next.store(next, Relaxed);\n            match to.compare_and_set_weak(next, entry_ptr, Release, guard) {\n                Ok(_) => break,\n                // We lost the race or weak CAS failed spuriously. Update the successor and try\n                // again.\n                Err(err) => next = err.current,\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::List::<T, C>::iter":["/// Returns an iterator over all objects.\n///\n/// # Caveat\n///\n/// Every object that is inserted at the moment this function is called and persists at least\n/// until the end of iteration will be returned. Since this iterator traverses a lock-free\n/// linked list that may be concurrently modified, some additional caveats apply:\n///\n/// 1. If a new object is inserted during iteration, it may or may not be returned.\n/// 2. If an object is deleted during iteration, it may or may not be returned.\n/// 3. The iteration may be aborted when it lost in a race condition. In this case, the winning\n///    thread will continue to iterate over the same list.\npub fn iter<'g>(&'g self, guard: &'g Guard) -> Iter<'g, T, C>{\n        Iter {\n            guard,\n            pred: &self.head,\n            curr: self.head.load(Acquire, guard),\n            head: &self.head,\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::list::List::<T, C>::new":["/// Returns a new, empty linked list.\npub fn new() -> Self{\n        Self {\n            head: Atomic::null(),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))"],"sync::queue::Node":["struct Node<T> {\n    /// The slot in which a value of type `T` can be stored.\n    ///\n    /// The type of `data` is `MaybeUninit<T>` because a `Node<T>` doesn't always contain a `T`.\n    /// For example, the sentinel node in a queue never contains a value: its slot is always empty.\n    /// Other nodes start their life with a push operation and contain a value until it gets popped\n    /// out. After that such empty nodes get added to the collector for destruction.\n    data: MaybeUninit<T>,\n\n    next: Atomic<Node<T>>,\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue":["pub struct Queue<T> {\n    head: CachePadded<Atomic<Node<T>>>,\n    tail: CachePadded<Atomic<Node<T>>>,\n}","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::new":["/// Create a new, empty queue.\npub fn new() -> Queue<T>{\n        let q = Queue {\n            head: CachePadded::new(Atomic::null()),\n            tail: CachePadded::new(Atomic::null()),\n        };\n        let sentinel = Owned::new(Node {\n            data: MaybeUninit::uninit(),\n            next: Atomic::null(),\n        });\n        unsafe {\n            let guard = unprotected();\n            let sentinel = sentinel.into_shared(guard);\n            q.head.store(sentinel, Relaxed);\n            q.tail.store(sentinel, Relaxed);\n            q\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::pop_if_internal":["/// Attempts to pop a data node, if the data satisfies the given condition. `Ok(None)` if queue\n/// is empty or the data does not satisfy the condition; `Err(())` if lost race to pop.\n#[inline(always)]\nfn pop_if_internal<F>(&self, condition: F, guard: &Guard) -> Result<Option<T>, ()>\n    where\n        T: Sync,\n        F: Fn(&T) -> bool,{\n        let head = self.head.load(Acquire, guard);\n        let h = unsafe { head.deref() };\n        let next = h.next.load(Acquire, guard);\n        match unsafe { next.as_ref() } {\n            Some(n) if condition(unsafe { &*n.data.as_ptr() }) => unsafe {\n                self.head\n                    .compare_and_set(head, next, Release, guard)\n                    .map(|_| {\n                        let tail = self.tail.load(Relaxed, guard);\n                        // Advance the tail so that we don't retire a pointer to a reachable node.\n                        if head == tail {\n                            let _ = self.tail.compare_and_set(tail, next, Release, guard);\n                        }\n                        guard.defer_destroy(head);\n                        Some(n.data.as_ptr().read())\n                    })\n                    .map_err(|_| ())\n            },\n            None | Some(_) => Ok(None),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::pop_internal":["/// Attempts to pop a data node. `Ok(None)` if queue is empty; `Err(())` if lost race to pop.\n#[inline(always)]\nfn pop_internal(&self, guard: &Guard) -> Result<Option<T>, ()>{\n        let head = self.head.load(Acquire, guard);\n        let h = unsafe { head.deref() };\n        let next = h.next.load(Acquire, guard);\n        match unsafe { next.as_ref() } {\n            Some(n) => unsafe {\n                self.head\n                    .compare_and_set(head, next, Release, guard)\n                    .map(|_| {\n                        let tail = self.tail.load(Relaxed, guard);\n                        // Advance the tail so that we don't retire a pointer to a reachable node.\n                        if head == tail {\n                            let _ = self.tail.compare_and_set(tail, next, Release, guard);\n                        }\n                        guard.defer_destroy(head);\n                        // TODO: Replace with MaybeUninit::read when api is stable\n                        Some(n.data.as_ptr().read())\n                    })\n                    .map_err(|_| ())\n            },\n            None => Ok(None),\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::push":["/// Adds `t` to the back of the queue, possibly waking up threads blocked on `pop`.\npub fn push(&self, t: T, guard: &Guard){\n        let new = Owned::new(Node {\n            data: MaybeUninit::new(t),\n            next: Atomic::null(),\n        });\n        let new = Owned::into_shared(new, guard);\n\n        loop {\n            // We push onto the tail, so we'll start optimistically by looking there first.\n            let tail = self.tail.load(Acquire, guard);\n\n            // Attempt to push onto the `tail` snapshot; fails if `tail.next` has changed.\n            if self.push_internal(tail, new, guard) {\n                break;\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::push_internal":["/// Attempts to atomically place `n` into the `next` pointer of `onto`, and returns `true` on\n/// success. The queue's `tail` pointer may be updated.\n#[inline(always)]\nfn push_internal(\n        &self,\n        onto: Shared<'_, Node<T>>,\n        new: Shared<'_, Node<T>>,\n        guard: &Guard,\n    ) -> bool{\n        // is `onto` the actual tail?\n        let o = unsafe { onto.deref() };\n        let next = o.next.load(Acquire, guard);\n        if unsafe { next.as_ref().is_some() } {\n            // if not, try to \"help\" by moving the tail pointer forward\n            let _ = self.tail.compare_and_set(onto, next, Release, guard);\n            false\n        } else {\n            // looks like the actual tail; attempt to link in `n`\n            let result = o\n                .next\n                .compare_and_set(Shared::null(), new, Release, guard)\n                .is_ok();\n            if result {\n                // try to move the tail pointer forward\n                let _ = self.tail.compare_and_set(onto, new, Release, guard);\n            }\n            result\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::try_pop":["/// Attempts to dequeue from the front.\n///\n/// Returns `None` if the queue is observed to be empty.\npub fn try_pop(&self, guard: &Guard) -> Option<T>{\n        loop {\n            if let Ok(head) = self.pop_internal(guard) {\n                return head;\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"],"sync::queue::Queue::<T>::try_pop_if":["/// Attempts to dequeue from the front, if the item satisfies the given condition.\n///\n/// Returns `None` if the queue is observed to be empty, or the head does not satisfy the given\n/// condition.\npub fn try_pop_if<F>(&self, condition: F, guard: &Guard) -> Option<T>\n    where\n        T: Sync,\n        F: Fn(&T) -> bool,{\n        loop {\n            if let Ok(head) = self.pop_if_internal(&condition, guard) {\n                return head;\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))"]},"struct_constructor":{"&'a [std::mem::MaybeUninit<T>]":["deref"],"&'a mut [std::mem::MaybeUninit<T>]":["deref_mut"],"&'static collector::Collector":["__stability","default_collector"],"&'static collector::LocalHandle":["__getit"],"&'static guard::Guard":["unprotected"],"&collector::Collector":["collector","deref"],"&internal::Global":["global"],"&internal::Local":["element_of"],"&sync::list::Entry":["entry_of"],"(usize, usize)":["decompose_tag"],"<sync::list::Iter<'g, T, C> as std::iter::Iterator>::Item":["next"],"atomic::Atomic":["clone","default","from","from_usize","init","new","null"],"atomic::Owned":["clone","from","from_raw","from_usize","init","into_owned","new"],"atomic::Shared":["clone","default","fetch_and","fetch_or","fetch_xor","from","from_usize","into_shared","load","load_consume","null","swap","with_tag"],"bool":["eq","is_empty","is_expired","is_null","is_pinned","push_internal"],"collector::Collector":["__static_ref_initialize","clone","default","new"],"collector::LocalHandle":["__init","register"],"deferred::Deferred":["new"],"epoch::AtomicEpoch":["default","new"],"epoch::Epoch":["clone","default","load","starting","try_advance"],"guard::Guard":["pin"],"internal::Bag":["default","new"],"internal::Global":["new"],"internal::SealedBag":["default","seal"],"isize":["wrapping_sub"],"std::boxed::Box":["into_box"],"std::cmp::Ordering":["cmp","partial_cmp"],"std::sync::atomic::Ordering":["failure","success"],"sync::list::Entry":["default"],"sync::list::Iter":["iter"],"sync::list::List":["new"],"sync::queue::Queue":["new"],"usize":["init","into_usize","low_bits","tag"]},"struct_to_trait":{"<T as atomic::Pointable>::T":["atomic::Pointable"],"atomic::Atomic":["std::clone::Clone","std::convert::From","std::default::Default","std::fmt::Debug","std::fmt::Pointer","std::marker::Send","std::marker::Sync"],"atomic::CompareAndSetError":["std::fmt::Debug"],"atomic::Owned":["atomic::Pointer","lazy_static::__Deref","std::borrow::Borrow","std::borrow::BorrowMut","std::clone::Clone","std::convert::AsMut","std::convert::AsRef","std::convert::From","std::fmt::Debug","std::ops::DerefMut","std::ops::Drop"],"atomic::Shared":["atomic::Pointer","std::clone::Clone","std::cmp::Eq","std::cmp::Ord","std::cmp::PartialEq","std::cmp::PartialOrd","std::convert::From","std::default::Default","std::fmt::Debug","std::fmt::Pointer","std::marker::Copy"],"collector::Collector":["std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::default::Default","std::fmt::Debug","std::marker::Send","std::marker::Sync"],"collector::LocalHandle":["std::fmt::Debug","std::ops::Drop"],"default::COLLECTOR":["lazy_static::LazyStatic","lazy_static::__Deref"],"deferred::Deferred":["std::fmt::Debug"],"epoch::AtomicEpoch":["std::default::Default","std::fmt::Debug"],"epoch::Epoch":["std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::default::Default","std::fmt::Debug","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq"],"guard::Guard":["std::fmt::Debug","std::ops::Drop"],"internal::Bag":["std::default::Default","std::fmt::Debug","std::marker::Send","std::ops::Drop"],"internal::Local":["sync::list::IsElement"],"internal::SealedBag":["std::default::Default","std::fmt::Debug","std::marker::Sync"],"std::sync::atomic::Ordering":["atomic::CompareAndSetOrdering"],"sync::list::Entry":["std::default::Default","std::fmt::Debug"],"sync::list::Iter":["std::iter::Iterator"],"sync::list::IterError":["std::cmp::PartialEq","std::fmt::Debug","std::marker::StructuralPartialEq"],"sync::list::List":["std::fmt::Debug","std::ops::Drop"],"sync::queue::Queue":["std::fmt::Debug","std::marker::Send","std::marker::Sync","std::ops::Drop"]},"targets":{"<(std::sync::atomic::Ordering, std::sync::atomic::Ordering) as atomic::CompareAndSetOrdering>::failure":["failure","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::CompareAndSetOrdering"],"<(std::sync::atomic::Ordering, std::sync::atomic::Ordering) as atomic::CompareAndSetOrdering>::success":["success","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::CompareAndSetOrdering"],"<T as atomic::Pointable>::deref":["deref","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<T as atomic::Pointable>::deref_mut":["deref_mut","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<T as atomic::Pointable>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<T as atomic::Pointable>::init":["init","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::deref":["deref","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::deref_mut":["deref_mut","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<[std::mem::MaybeUninit<T>] as atomic::Pointable>::init":["init","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointable"],"<atomic::Atomic<T> as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::clone::Clone"],"<atomic::Atomic<T> as std::convert::From<*const T>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Atomic<T> as std::convert::From<T>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Atomic<T> as std::convert::From<atomic::Owned<T>>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Atomic<T> as std::convert::From<atomic::Shared<'g, T>>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Atomic<T> as std::convert::From<std::boxed::Box<T>>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Atomic<T> as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::default::Default"],"<atomic::Atomic<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::fmt::Debug"],"<atomic::Atomic<T> as std::fmt::Pointer>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::fmt::Pointer"],"<atomic::CompareAndSetError<'g, T, P> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::fmt::Debug"],"<atomic::Owned<T> as atomic::Pointer<T>>::from_usize":["from_usize","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointer"],"<atomic::Owned<T> as atomic::Pointer<T>>::into_usize":["into_usize","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointer"],"<atomic::Owned<T> as lazy_static::__Deref>::deref":["deref","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","lazy_static::__Deref"],"<atomic::Owned<T> as std::borrow::Borrow<T>>::borrow":["borrow","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::borrow::Borrow"],"<atomic::Owned<T> as std::borrow::BorrowMut<T>>::borrow_mut":["borrow_mut","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::borrow::BorrowMut"],"<atomic::Owned<T> as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::clone::Clone"],"<atomic::Owned<T> as std::convert::AsMut<T>>::as_mut":["as_mut","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::AsMut"],"<atomic::Owned<T> as std::convert::AsRef<T>>::as_ref":["as_ref","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::AsRef"],"<atomic::Owned<T> as std::convert::From<T>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Owned<T> as std::convert::From<std::boxed::Box<T>>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Owned<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::fmt::Debug"],"<atomic::Owned<T> as std::ops::DerefMut>::deref_mut":["deref_mut","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::ops::DerefMut"],"<atomic::Owned<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::ops::Drop"],"<atomic::Shared<'_, T> as atomic::Pointer<T>>::from_usize":["from_usize","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointer"],"<atomic::Shared<'_, T> as atomic::Pointer<T>>::into_usize":["into_usize","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::Pointer"],"<atomic::Shared<'_, T> as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::clone::Clone"],"<atomic::Shared<'_, T> as std::cmp::Ord>::cmp":["cmp","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::cmp::Ord"],"<atomic::Shared<'_, T> as std::convert::From<*const T>>::from":["from","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::convert::From"],"<atomic::Shared<'_, T> as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::default::Default"],"<atomic::Shared<'_, T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::fmt::Debug"],"<atomic::Shared<'_, T> as std::fmt::Pointer>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::fmt::Pointer"],"<atomic::Shared<'g, T> as std::cmp::PartialEq>::eq":["eq","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::cmp::PartialEq"],"<atomic::Shared<'g, T> as std::cmp::PartialOrd>::partial_cmp":["partial_cmp","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","std::cmp::PartialOrd"],"<collector::Collector as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))","std::clone::Clone"],"<collector::Collector as std::cmp::PartialEq>::eq":["eq","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))","std::cmp::PartialEq"],"<collector::Collector as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))","std::default::Default"],"<collector::Collector as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))","std::fmt::Debug"],"<collector::LocalHandle as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))","std::fmt::Debug"],"<collector::LocalHandle as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))","std::ops::Drop"],"<default::COLLECTOR as lazy_static::LazyStatic>::initialize":["initialize","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))","lazy_static::LazyStatic"],"<default::COLLECTOR as lazy_static::__Deref>::deref":["deref","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))","lazy_static::__Deref"],"<default::COLLECTOR as lazy_static::__Deref>::deref::__stability":["__stability","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))",""],"<default::COLLECTOR as lazy_static::__Deref>::deref::__static_ref_initialize":["__static_ref_initialize","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))",""],"<deferred::Deferred as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))","std::fmt::Debug"],"<guard::Guard as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))","std::fmt::Debug"],"<guard::Guard as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))","std::ops::Drop"],"<internal::Bag as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))","std::default::Default"],"<internal::Bag as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))","std::fmt::Debug"],"<internal::Bag as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))","std::ops::Drop"],"<internal::Local as sync::list::IsElement<internal::Local>>::element_of":["element_of","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))","sync::list::IsElement"],"<internal::Local as sync::list::IsElement<internal::Local>>::entry_of":["entry_of","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))","sync::list::IsElement"],"<internal::Local as sync::list::IsElement<internal::Local>>::finalize":["finalize","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))","sync::list::IsElement"],"<std::sync::atomic::Ordering as atomic::CompareAndSetOrdering>::failure":["failure","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::CompareAndSetOrdering"],"<std::sync::atomic::Ordering as atomic::CompareAndSetOrdering>::success":["success","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))","atomic::CompareAndSetOrdering"],"<sync::list::Entry as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))","std::default::Default"],"<sync::list::Iter<'g, T, C> as std::iter::Iterator>::next":["next","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))","std::iter::Iterator"],"<sync::list::List<T, C> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))","std::ops::Drop"],"<sync::queue::Queue<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))","std::ops::Drop"],"atomic::Atomic::<T>::compare_and_set":["compare_and_set","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::compare_and_set_weak":["compare_and_set_weak","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::from_usize":["from_usize","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::init":["init","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::into_owned":["into_owned","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::load":["load","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::load_consume":["load_consume","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::new":["new","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::null":["null","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::store":["store","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Atomic::<T>::swap":["swap","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::from_raw":["from_raw","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::init":["init","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::into_box":["into_box","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::into_shared":["into_shared","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::new":["new","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::tag":["tag","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Owned::<T>::with_tag":["with_tag","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::as_raw":["as_raw","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::as_ref":["as_ref","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::deref":["deref","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::deref_mut":["deref_mut","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::into_owned":["into_owned","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::is_null":["is_null","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::null":["null","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::tag":["tag","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::Shared::<'g, T>::with_tag":["with_tag","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::compose_tag":["compose_tag","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::decompose_tag":["decompose_tag","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::ensure_aligned":["ensure_aligned","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::low_bits":["low_bits","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"atomic::strongest_failure_ordering":["strongest_failure_ordering","Real(LocalPath(\"crossbeam-epoch/src/atomic.rs\"))",""],"collector::Collector::new":["new","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))",""],"collector::Collector::register":["register","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))",""],"collector::LocalHandle::collector":["collector","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))",""],"collector::LocalHandle::is_pinned":["is_pinned","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))",""],"collector::LocalHandle::pin":["pin","Real(LocalPath(\"crossbeam-epoch/src/collector.rs\"))",""],"default::HANDLE::__getit":["__getit","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })",""],"default::HANDLE::__init":["__init","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })",""],"default::default_collector":["default_collector","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))",""],"default::is_pinned":["is_pinned","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))",""],"default::pin":["pin","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))",""],"default::with_handle":["with_handle","Real(LocalPath(\"crossbeam-epoch/src/default.rs\"))",""],"deferred::Deferred::call":["call","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))",""],"deferred::Deferred::new":["new","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))",""],"deferred::Deferred::new::call":["call","Real(LocalPath(\"crossbeam-epoch/src/deferred.rs\"))",""],"epoch::AtomicEpoch::compare_and_swap":["compare_and_swap","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::AtomicEpoch::load":["load","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::AtomicEpoch::new":["new","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::AtomicEpoch::store":["store","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::Epoch::is_pinned":["is_pinned","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::Epoch::pinned":["pinned","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::Epoch::starting":["starting","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::Epoch::successor":["successor","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::Epoch::unpinned":["unpinned","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"epoch::Epoch::wrapping_sub":["wrapping_sub","Real(LocalPath(\"crossbeam-epoch/src/epoch.rs\"))",""],"guard::Guard::collector":["collector","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::Guard::defer":["defer","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::Guard::defer_destroy":["defer_destroy","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::Guard::defer_unchecked":["defer_unchecked","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::Guard::flush":["flush","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::Guard::repin":["repin","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::Guard::repin_after":["repin_after","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"guard::unprotected":["unprotected","Real(LocalPath(\"crossbeam-epoch/src/guard.rs\"))",""],"internal::Bag::is_empty":["is_empty","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Bag::new":["new","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Bag::seal":["seal","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Bag::try_push":["try_push","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Global::collect":["collect","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Global::new":["new","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Global::push_bag":["push_bag","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Global::try_advance":["try_advance","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::acquire_handle":["acquire_handle","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::collector":["collector","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::defer":["defer","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::finalize":["finalize","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::flush":["flush","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::global":["global","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::is_pinned":["is_pinned","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::pin":["pin","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::register":["register","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::release_handle":["release_handle","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::repin":["repin","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::Local::unpin":["unpin","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::SealedBag::is_expired":["is_expired","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"internal::no_op_func":["no_op_func","Real(LocalPath(\"crossbeam-epoch/src/internal.rs\"))",""],"sync::list::Entry::delete":["delete","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))",""],"sync::list::List::<T, C>::insert":["insert","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))",""],"sync::list::List::<T, C>::iter":["iter","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))",""],"sync::list::List::<T, C>::new":["new","Real(LocalPath(\"crossbeam-epoch/src/sync/list.rs\"))",""],"sync::queue::Queue::<T>::new":["new","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""],"sync::queue::Queue::<T>::pop_if_internal":["pop_if_internal","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""],"sync::queue::Queue::<T>::pop_internal":["pop_internal","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""],"sync::queue::Queue::<T>::push":["push","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""],"sync::queue::Queue::<T>::push_internal":["push_internal","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""],"sync::queue::Queue::<T>::try_pop":["try_pop","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""],"sync::queue::Queue::<T>::try_pop_if":["try_pop_if","Real(LocalPath(\"crossbeam-epoch/src/sync/queue.rs\"))",""]},"trait_to_struct":{"atomic::CompareAndSetOrdering":["std::sync::atomic::Ordering"],"atomic::Pointable":["<T as atomic::Pointable>::T"],"atomic::Pointer":["atomic::Owned","atomic::Shared"],"lazy_static::LazyStatic":["default::COLLECTOR"],"lazy_static::__Deref":["atomic::Owned","default::COLLECTOR"],"std::borrow::Borrow":["atomic::Owned"],"std::borrow::BorrowMut":["atomic::Owned"],"std::clone::Clone":["atomic::Atomic","atomic::Owned","atomic::Shared","collector::Collector","epoch::Epoch"],"std::cmp::Eq":["atomic::Shared","collector::Collector","epoch::Epoch"],"std::cmp::Ord":["atomic::Shared"],"std::cmp::PartialEq":["atomic::Shared","collector::Collector","epoch::Epoch","sync::list::IterError"],"std::cmp::PartialOrd":["atomic::Shared"],"std::convert::AsMut":["atomic::Owned"],"std::convert::AsRef":["atomic::Owned"],"std::convert::From":["atomic::Atomic","atomic::Owned","atomic::Shared"],"std::default::Default":["atomic::Atomic","atomic::Shared","collector::Collector","epoch::AtomicEpoch","epoch::Epoch","internal::Bag","internal::SealedBag","sync::list::Entry"],"std::fmt::Debug":["atomic::Atomic","atomic::CompareAndSetError","atomic::Owned","atomic::Shared","collector::Collector","collector::LocalHandle","deferred::Deferred","epoch::AtomicEpoch","epoch::Epoch","guard::Guard","internal::Bag","internal::SealedBag","sync::list::Entry","sync::list::IterError","sync::list::List","sync::queue::Queue"],"std::fmt::Pointer":["atomic::Atomic","atomic::Shared"],"std::iter::Iterator":["sync::list::Iter"],"std::marker::Copy":["atomic::Shared","epoch::Epoch"],"std::marker::Send":["atomic::Atomic","collector::Collector","internal::Bag","sync::queue::Queue"],"std::marker::StructuralEq":["epoch::Epoch"],"std::marker::StructuralPartialEq":["epoch::Epoch","sync::list::IterError"],"std::marker::Sync":["atomic::Atomic","collector::Collector","internal::SealedBag","sync::queue::Queue"],"std::ops::DerefMut":["atomic::Owned"],"std::ops::Drop":["atomic::Owned","collector::LocalHandle","guard::Guard","internal::Bag","sync::list::List","sync::queue::Queue"],"sync::list::IsElement":["internal::Local"]},"type_to_def_path":{"atomic::Array<T>":"atomic::Array","atomic::Atomic<T>":"atomic::Atomic","atomic::CompareAndSetError<'g, T, P>":"atomic::CompareAndSetError","atomic::Owned<T>":"atomic::Owned","atomic::Shared<'g, T>":"atomic::Shared","collector::Collector":"collector::Collector","collector::LocalHandle":"collector::LocalHandle","default::COLLECTOR":"default::COLLECTOR","deferred::Deferred":"deferred::Deferred","epoch::AtomicEpoch":"epoch::AtomicEpoch","epoch::Epoch":"epoch::Epoch","guard::Guard":"guard::Guard","internal::Bag":"internal::Bag","internal::Global":"internal::Global","internal::Local":"internal::Local","internal::SealedBag":"internal::SealedBag","sync::list::Entry":"sync::list::Entry","sync::list::Iter<'g, T, C>":"sync::list::Iter","sync::list::IterError":"sync::list::IterError","sync::list::List<T, C>":"sync::list::List","sync::queue::Node<T>":"sync::queue::Node","sync::queue::Queue<T>":"sync::queue::Queue"}}