{"dependencies":{"<array_queue::ArrayQueue<T> as std::fmt::Debug>::fmt":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize"],"<array_queue::ArrayQueue<T> as std::ops::Drop>::drop":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"<err::PopError as std::clone::Clone>::clone":["err::PopError"],"<err::PopError as std::cmp::Eq>::assert_receiver_is_total_eq":["err::PopError"],"<err::PopError as std::cmp::PartialEq>::eq":["err::PopError"],"<err::PopError as std::fmt::Debug>::fmt":["err::PopError","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<err::PopError as std::fmt::Display>::fmt":["err::PopError","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<err::PushError<T> as std::clone::Clone>::clone":["err::PushError","std::marker::Sized"],"<err::PushError<T> as std::cmp::Eq>::assert_receiver_is_total_eq":["err::PushError","std::marker::Sized"],"<err::PushError<T> as std::cmp::PartialEq>::eq":["err::PushError","std::marker::Sized"],"<err::PushError<T> as std::fmt::Debug>::fmt":["err::PushError","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<err::PushError<T> as std::fmt::Display>::fmt":["err::PushError","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<seg_queue::SegQueue<T> as std::default::Default>::default":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"<seg_queue::SegQueue<T> as std::fmt::Debug>::fmt":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<seg_queue::SegQueue<T> as std::ops::Drop>::drop":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"array_queue::ArrayQueue":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::capacity":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::is_empty":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::is_full":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::len":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::new":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::pop":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize"],"array_queue::ArrayQueue::<T>::push":["array_queue::ArrayQueue","array_queue::Slot","crossbeam_utils::CachePadded","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::atomic::AtomicUsize"],"array_queue::Slot":["array_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicUsize"],"err::PopError":["err::PopError"],"err::PushError":["err::PushError","std::marker::Sized"],"seg_queue::Block":["seg_queue::Block","seg_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"seg_queue::Block::<T>::destroy":["seg_queue::Block","seg_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"seg_queue::Block::<T>::new":["seg_queue::Block","seg_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"seg_queue::Block::<T>::wait_next":["seg_queue::Block","seg_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"seg_queue::Position":["seg_queue::Position","std::marker::Sized","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"seg_queue::SegQueue":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"seg_queue::SegQueue::<T>::is_empty":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"seg_queue::SegQueue::<T>::len":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"seg_queue::SegQueue::<T>::new":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"seg_queue::SegQueue::<T>::pop":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"seg_queue::SegQueue::<T>::push":["crossbeam_utils::CachePadded","seg_queue::SegQueue","std::marker::PhantomData","std::marker::Sized"],"seg_queue::Slot":["seg_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicUsize"],"seg_queue::Slot::<T>::wait_write":["seg_queue::Slot","std::cell::UnsafeCell","std::marker::Sized","std::sync::atomic::AtomicUsize"]},"glob_path_import":{},"self_to_fn":{"array_queue::ArrayQueue":["impl<T> ArrayQueue<T> {\n    /// Creates a new bounded queue with the given capacity.\n    ///\n    /// # Panics\n    ///\n    /// Panics if the capacity is zero.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::ArrayQueue;\n    ///\n    /// let q = ArrayQueue::<i32>::new(100);\n    /// ```\n    pub fn new(cap: usize) -> ArrayQueue<T> {\n        assert!(cap > 0, \"capacity must be non-zero\");\n\n        // Head is initialized to `{ lap: 0, index: 0 }`.\n        // Tail is initialized to `{ lap: 0, index: 0 }`.\n        let head = 0;\n        let tail = 0;\n\n        // Allocate a buffer of `cap` slots initialized\n        // with stamps.\n        let buffer = {\n            let mut v: Vec<Slot<T>> = (0..cap)\n                .map(|i| {\n                    // Set the stamp to `{ lap: 0, index: i }`.\n                    Slot {\n                        stamp: AtomicUsize::new(i),\n                        value: UnsafeCell::new(MaybeUninit::uninit()),\n                    }\n                })\n                .collect();\n            let ptr = v.as_mut_ptr();\n            mem::forget(v);\n            ptr\n        };\n\n        // One lap is the smallest power of two greater than `cap`.\n        let one_lap = (cap + 1).next_power_of_two();\n\n        ArrayQueue {\n            buffer,\n            cap,\n            one_lap,\n            head: CachePadded::new(AtomicUsize::new(head)),\n            tail: CachePadded::new(AtomicUsize::new(tail)),\n            _marker: PhantomData,\n        }\n    }\n\n    /// Attempts to push an element into the queue.\n    ///\n    /// If the queue is full, the element is returned back as an error.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::{ArrayQueue, PushError};\n    ///\n    /// let q = ArrayQueue::new(1);\n    ///\n    /// assert_eq!(q.push(10), Ok(()));\n    /// assert_eq!(q.push(20), Err(PushError(20)));\n    /// ```\n    pub fn push(&self, value: T) -> Result<(), PushError<T>> {\n        let backoff = Backoff::new();\n        let mut tail = self.tail.load(Ordering::Relaxed);\n\n        loop {\n            // Deconstruct the tail.\n            let index = tail & (self.one_lap - 1);\n            let lap = tail & !(self.one_lap - 1);\n\n            // Inspect the corresponding slot.\n            let slot = unsafe { &*self.buffer.add(index) };\n            let stamp = slot.stamp.load(Ordering::Acquire);\n\n            // If the tail and the stamp match, we may attempt to push.\n            if tail == stamp {\n                let new_tail = if index + 1 < self.cap {\n                    // Same lap, incremented index.\n                    // Set to `{ lap: lap, index: index + 1 }`.\n                    tail + 1\n                } else {\n                    // One lap forward, index wraps around to zero.\n                    // Set to `{ lap: lap.wrapping_add(1), index: 0 }`.\n                    lap.wrapping_add(self.one_lap)\n                };\n\n                // Try moving the tail.\n                match self.tail.compare_exchange_weak(\n                    tail,\n                    new_tail,\n                    Ordering::SeqCst,\n                    Ordering::Relaxed,\n                ) {\n                    Ok(_) => {\n                        // Write the value into the slot and update the stamp.\n                        unsafe {\n                            slot.value.get().write(MaybeUninit::new(value));\n                        }\n                        slot.stamp.store(tail + 1, Ordering::Release);\n                        return Ok(());\n                    }\n                    Err(t) => {\n                        tail = t;\n                        backoff.spin();\n                    }\n                }\n            } else if stamp.wrapping_add(self.one_lap) == tail + 1 {\n                atomic::fence(Ordering::SeqCst);\n                let head = self.head.load(Ordering::Relaxed);\n\n                // If the head lags one lap behind the tail as well...\n                if head.wrapping_add(self.one_lap) == tail {\n                    // ...then the queue is full.\n                    return Err(PushError(value));\n                }\n\n                backoff.spin();\n                tail = self.tail.load(Ordering::Relaxed);\n            } else {\n                // Snooze because we need to wait for the stamp to get updated.\n                backoff.snooze();\n                tail = self.tail.load(Ordering::Relaxed);\n            }\n        }\n    }\n\n    /// Attempts to pop an element from the queue.\n    ///\n    /// If the queue is empty, an error is returned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::{ArrayQueue, PopError};\n    ///\n    /// let q = ArrayQueue::new(1);\n    /// assert_eq!(q.push(10), Ok(()));\n    ///\n    /// assert_eq!(q.pop(), Ok(10));\n    /// assert_eq!(q.pop(), Err(PopError));\n    /// ```\n    pub fn pop(&self) -> Result<T, PopError> {\n        let backoff = Backoff::new();\n        let mut head = self.head.load(Ordering::Relaxed);\n\n        loop {\n            // Deconstruct the head.\n            let index = head & (self.one_lap - 1);\n            let lap = head & !(self.one_lap - 1);\n\n            // Inspect the corresponding slot.\n            let slot = unsafe { &*self.buffer.add(index) };\n            let stamp = slot.stamp.load(Ordering::Acquire);\n\n            // If the the stamp is ahead of the head by 1, we may attempt to pop.\n            if head + 1 == stamp {\n                let new = if index + 1 < self.cap {\n                    // Same lap, incremented index.\n                    // Set to `{ lap: lap, index: index + 1 }`.\n                    head + 1\n                } else {\n                    // One lap forward, index wraps around to zero.\n                    // Set to `{ lap: lap.wrapping_add(1), index: 0 }`.\n                    lap.wrapping_add(self.one_lap)\n                };\n\n                // Try moving the head.\n                match self.head.compare_exchange_weak(\n                    head,\n                    new,\n                    Ordering::SeqCst,\n                    Ordering::Relaxed,\n                ) {\n                    Ok(_) => {\n                        // Read the value from the slot and update the stamp.\n                        let msg = unsafe { slot.value.get().read().assume_init() };\n                        slot.stamp\n                            .store(head.wrapping_add(self.one_lap), Ordering::Release);\n                        return Ok(msg);\n                    }\n                    Err(h) => {\n                        head = h;\n                        backoff.spin();\n                    }\n                }\n            } else if stamp == head {\n                atomic::fence(Ordering::SeqCst);\n                let tail = self.tail.load(Ordering::Relaxed);\n\n                // If the tail equals the head, that means the channel is empty.\n                if tail == head {\n                    return Err(PopError);\n                }\n\n                backoff.spin();\n                head = self.head.load(Ordering::Relaxed);\n            } else {\n                // Snooze because we need to wait for the stamp to get updated.\n                backoff.snooze();\n                head = self.head.load(Ordering::Relaxed);\n            }\n        }\n    }\n\n    /// Returns the capacity of the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::ArrayQueue;\n    ///\n    /// let q = ArrayQueue::<i32>::new(100);\n    ///\n    /// assert_eq!(q.capacity(), 100);\n    /// ```\n    pub fn capacity(&self) -> usize {\n        self.cap\n    }\n\n    /// Returns `true` if the queue is empty.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::ArrayQueue;\n    ///\n    /// let q = ArrayQueue::new(100);\n    ///\n    /// assert!(q.is_empty());\n    /// q.push(1).unwrap();\n    /// assert!(!q.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        let head = self.head.load(Ordering::SeqCst);\n        let tail = self.tail.load(Ordering::SeqCst);\n\n        // Is the tail lagging one lap behind head?\n        // Is the tail equal to the head?\n        //\n        // Note: If the head changes just before we load the tail, that means there was a moment\n        // when the channel was not empty, so it is safe to just return `false`.\n        tail == head\n    }\n\n    /// Returns `true` if the queue is full.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::ArrayQueue;\n    ///\n    /// let q = ArrayQueue::new(1);\n    ///\n    /// assert!(!q.is_full());\n    /// q.push(1).unwrap();\n    /// assert!(q.is_full());\n    /// ```\n    pub fn is_full(&self) -> bool {\n        let tail = self.tail.load(Ordering::SeqCst);\n        let head = self.head.load(Ordering::SeqCst);\n\n        // Is the head lagging one lap behind tail?\n        //\n        // Note: If the tail changes just before we load the head, that means there was a moment\n        // when the queue was not full, so it is safe to just return `false`.\n        head.wrapping_add(self.one_lap) == tail\n    }\n\n    /// Returns the number of elements in the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::ArrayQueue;\n    ///\n    /// let q = ArrayQueue::new(100);\n    /// assert_eq!(q.len(), 0);\n    ///\n    /// q.push(10).unwrap();\n    /// assert_eq!(q.len(), 1);\n    ///\n    /// q.push(20).unwrap();\n    /// assert_eq!(q.len(), 2);\n    /// ```\n    pub fn len(&self) -> usize {\n        loop {\n            // Load the tail, then load the head.\n            let tail = self.tail.load(Ordering::SeqCst);\n            let head = self.head.load(Ordering::SeqCst);\n\n            // If the tail didn't change, we've got consistent values to work with.\n            if self.tail.load(Ordering::SeqCst) == tail {\n                let hix = head & (self.one_lap - 1);\n                let tix = tail & (self.one_lap - 1);\n\n                return if hix < tix {\n                    tix - hix\n                } else if hix > tix {\n                    self.cap - hix + tix\n                } else if tail == head {\n                    0\n                } else {\n                    self.cap\n                };\n            }\n        }\n    }\n}","impl<T> Drop for ArrayQueue<T> {\n    fn drop(&mut self) {\n        // Get the index of the head.\n        let hix = self.head.load(Ordering::Relaxed) & (self.one_lap - 1);\n\n        // Loop over all slots that hold a message and drop them.\n        for i in 0..self.len() {\n            // Compute the index of the next slot holding a message.\n            let index = if hix + i < self.cap {\n                hix + i\n            } else {\n                hix + i - self.cap\n            };\n\n            unsafe {\n                let p = {\n                    let slot = &mut *self.buffer.add(index);\n                    let value = &mut *slot.value.get();\n                    value.as_mut_ptr()\n                };\n                p.drop_in_place();\n            }\n        }\n\n        // Finally, deallocate the buffer, but don't run any destructors.\n        unsafe {\n            Vec::from_raw_parts(self.buffer, 0, self.cap);\n        }\n    }\n}","impl<T> fmt::Debug for ArrayQueue<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"ArrayQueue { .. }\")\n    }\n}","unsafe impl<T: Send> Send for ArrayQueue<T> {}","unsafe impl<T: Send> Sync for ArrayQueue<T> {}"],"err::PopError":["Clone","Copy","Eq","PartialEq","impl ::std::error::Error for PopError {}","impl fmt::Debug for PopError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        \"PopError\".fmt(f)\n    }\n}","impl fmt::Display for PopError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        \"popping from an empty queue\".fmt(f)\n    }\n}"],"err::PushError":["Clone","Copy","Eq","PartialEq","impl<T: Send> ::std::error::Error for PushError<T> {}","impl<T> fmt::Debug for PushError<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        \"PushError(..)\".fmt(f)\n    }\n}","impl<T> fmt::Display for PushError<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        \"pushing into a full queue\".fmt(f)\n    }\n}"],"seg_queue::Block":["impl<T> Block<T> {\n    /// Creates an empty block that starts at `start_index`.\n    fn new() -> Block<T> {\n        // SAFETY: This is safe because:\n        //  [1] `Block::next` (AtomicPtr) may be safely zero initialized.\n        //  [2] `Block::slots` (Array) may be safely zero initialized because of [3, 4].\n        //  [3] `Slot::value` (UnsafeCell) may be safely zero initialized because it\n        //       holds a MaybeUninit.\n        //  [4] `Slot::state` (AtomicUsize) may be safely zero initialized.\n        unsafe { MaybeUninit::zeroed().assume_init() }\n    }\n\n    /// Waits until the next pointer is set.\n    fn wait_next(&self) -> *mut Block<T> {\n        let backoff = Backoff::new();\n        loop {\n            let next = self.next.load(Ordering::Acquire);\n            if !next.is_null() {\n                return next;\n            }\n            backoff.snooze();\n        }\n    }\n\n    /// Sets the `DESTROY` bit in slots starting from `start` and destroys the block.\n    unsafe fn destroy(this: *mut Block<T>, start: usize) {\n        // It is not necessary to set the `DESTROY` bit in the last slot because that slot has\n        // begun destruction of the block.\n        for i in start..BLOCK_CAP - 1 {\n            let slot = (*this).slots.get_unchecked(i);\n\n            // Mark the `DESTROY` bit if a thread is still using the slot.\n            if slot.state.load(Ordering::Acquire) & READ == 0\n                && slot.state.fetch_or(DESTROY, Ordering::AcqRel) & READ == 0\n            {\n                // If a thread is still using the slot, it will continue destruction of the block.\n                return;\n            }\n        }\n\n        // No thread is using the block, now it is safe to destroy it.\n        drop(Box::from_raw(this));\n    }\n}"],"seg_queue::SegQueue":["impl<T> Default for SegQueue<T> {\n    fn default() -> SegQueue<T> {\n        SegQueue::new()\n    }\n}","impl<T> Drop for SegQueue<T> {\n    fn drop(&mut self) {\n        let mut head = self.head.index.load(Ordering::Relaxed);\n        let mut tail = self.tail.index.load(Ordering::Relaxed);\n        let mut block = self.head.block.load(Ordering::Relaxed);\n\n        // Erase the lower bits.\n        head &= !((1 << SHIFT) - 1);\n        tail &= !((1 << SHIFT) - 1);\n\n        unsafe {\n            // Drop all values between `head` and `tail` and deallocate the heap-allocated blocks.\n            while head != tail {\n                let offset = (head >> SHIFT) % LAP;\n\n                if offset < BLOCK_CAP {\n                    // Drop the value in the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    let p = &mut *slot.value.get();\n                    p.as_mut_ptr().drop_in_place();\n                } else {\n                    // Deallocate the block and move to the next one.\n                    let next = (*block).next.load(Ordering::Relaxed);\n                    drop(Box::from_raw(block));\n                    block = next;\n                }\n\n                head = head.wrapping_add(1 << SHIFT);\n            }\n\n            // Deallocate the last remaining block.\n            if !block.is_null() {\n                drop(Box::from_raw(block));\n            }\n        }\n    }\n}","impl<T> SegQueue<T> {\n    /// Creates a new unbounded queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::SegQueue;\n    ///\n    /// let q = SegQueue::<i32>::new();\n    /// ```\n    pub fn new() -> SegQueue<T> {\n        SegQueue {\n            head: CachePadded::new(Position {\n                block: AtomicPtr::new(ptr::null_mut()),\n                index: AtomicUsize::new(0),\n            }),\n            tail: CachePadded::new(Position {\n                block: AtomicPtr::new(ptr::null_mut()),\n                index: AtomicUsize::new(0),\n            }),\n            _marker: PhantomData,\n        }\n    }\n\n    /// Pushes an element into the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::SegQueue;\n    ///\n    /// let q = SegQueue::new();\n    ///\n    /// q.push(10);\n    /// q.push(20);\n    /// ```\n    pub fn push(&self, value: T) {\n        let backoff = Backoff::new();\n        let mut tail = self.tail.index.load(Ordering::Acquire);\n        let mut block = self.tail.block.load(Ordering::Acquire);\n        let mut next_block = None;\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (tail >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n                tail = self.tail.index.load(Ordering::Acquire);\n                block = self.tail.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // If we're going to have to install the next block, allocate it in advance in order to\n            // make the wait for other threads as short as possible.\n            if offset + 1 == BLOCK_CAP && next_block.is_none() {\n                next_block = Some(Box::new(Block::<T>::new()));\n            }\n\n            // If this is the first push operation, we need to allocate the first block.\n            if block.is_null() {\n                let new = Box::into_raw(Box::new(Block::<T>::new()));\n\n                if self\n                    .tail\n                    .block\n                    .compare_and_swap(block, new, Ordering::Release)\n                    == block\n                {\n                    self.head.block.store(new, Ordering::Release);\n                    block = new;\n                } else {\n                    next_block = unsafe { Some(Box::from_raw(new)) };\n                    tail = self.tail.index.load(Ordering::Acquire);\n                    block = self.tail.block.load(Ordering::Acquire);\n                    continue;\n                }\n            }\n\n            let new_tail = tail + (1 << SHIFT);\n\n            // Try advancing the tail forward.\n            match self.tail.index.compare_exchange_weak(\n                tail,\n                new_tail,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, install the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next_block = Box::into_raw(next_block.unwrap());\n                        let next_index = new_tail.wrapping_add(1 << SHIFT);\n\n                        self.tail.block.store(next_block, Ordering::Release);\n                        self.tail.index.store(next_index, Ordering::Release);\n                        (*block).next.store(next_block, Ordering::Release);\n                    }\n\n                    // Write the value into the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.value.get().write(MaybeUninit::new(value));\n                    slot.state.fetch_or(WRITE, Ordering::Release);\n\n                    return;\n                },\n                Err(t) => {\n                    tail = t;\n                    block = self.tail.block.load(Ordering::Acquire);\n                    backoff.spin();\n                }\n            }\n        }\n    }\n\n    /// Pops an element from the queue.\n    ///\n    /// If the queue is empty, an error is returned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::{PopError, SegQueue};\n    ///\n    /// let q = SegQueue::new();\n    ///\n    /// q.push(10);\n    /// assert_eq!(q.pop(), Ok(10));\n    /// assert_eq!(q.pop(), Err(PopError));\n    /// ```\n    pub fn pop(&self) -> Result<T, PopError> {\n        let backoff = Backoff::new();\n        let mut head = self.head.index.load(Ordering::Acquire);\n        let mut block = self.head.block.load(Ordering::Acquire);\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n                head = self.head.index.load(Ordering::Acquire);\n                block = self.head.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            let mut new_head = head + (1 << SHIFT);\n\n            if new_head & HAS_NEXT == 0 {\n                atomic::fence(Ordering::SeqCst);\n                let tail = self.tail.index.load(Ordering::Relaxed);\n\n                // If the tail equals the head, that means the queue is empty.\n                if head >> SHIFT == tail >> SHIFT {\n                    return Err(PopError);\n                }\n\n                // If head and tail are not in the same block, set `HAS_NEXT` in head.\n                if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                    new_head |= HAS_NEXT;\n                }\n            }\n\n            // The block can be null here only if the first push operation is in progress. In that\n            // case, just wait until it gets initialized.\n            if block.is_null() {\n                backoff.snooze();\n                head = self.head.index.load(Ordering::Acquire);\n                block = self.head.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // Try moving the head index forward.\n            match self.head.index.compare_exchange_weak(\n                head,\n                new_head,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, move to the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next = (*block).wait_next();\n                        let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                        if !(*next).next.load(Ordering::Relaxed).is_null() {\n                            next_index |= HAS_NEXT;\n                        }\n\n                        self.head.block.store(next, Ordering::Release);\n                        self.head.index.store(next_index, Ordering::Release);\n                    }\n\n                    // Read the value.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.wait_write();\n                    let value = slot.value.get().read().assume_init();\n\n                    // Destroy the block if we've reached the end, or if another thread wanted to\n                    // destroy but couldn't because we were busy reading from the slot.\n                    if offset + 1 == BLOCK_CAP {\n                        Block::destroy(block, 0);\n                    } else if slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0 {\n                        Block::destroy(block, offset + 1);\n                    }\n\n                    return Ok(value);\n                },\n                Err(h) => {\n                    head = h;\n                    block = self.head.block.load(Ordering::Acquire);\n                    backoff.spin();\n                }\n            }\n        }\n    }\n\n    /// Returns `true` if the queue is empty.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::SegQueue;\n    ///\n    /// let q = SegQueue::new();\n    ///\n    /// assert!(q.is_empty());\n    /// q.push(1);\n    /// assert!(!q.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        let head = self.head.index.load(Ordering::SeqCst);\n        let tail = self.tail.index.load(Ordering::SeqCst);\n        head >> SHIFT == tail >> SHIFT\n    }\n\n    /// Returns the number of elements in the queue.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_queue::SegQueue;\n    ///\n    /// let q = SegQueue::new();\n    /// assert_eq!(q.len(), 0);\n    ///\n    /// q.push(10);\n    /// assert_eq!(q.len(), 1);\n    ///\n    /// q.push(20);\n    /// assert_eq!(q.len(), 2);\n    /// ```\n    pub fn len(&self) -> usize {\n        loop {\n            // Load the tail index, then load the head index.\n            let mut tail = self.tail.index.load(Ordering::SeqCst);\n            let mut head = self.head.index.load(Ordering::SeqCst);\n\n            // If the tail index didn't change, we've got consistent indices to work with.\n            if self.tail.index.load(Ordering::SeqCst) == tail {\n                // Erase the lower bits.\n                tail &= !((1 << SHIFT) - 1);\n                head &= !((1 << SHIFT) - 1);\n\n                // Fix up indices if they fall onto block ends.\n                if (tail >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    tail = tail.wrapping_add(1 << SHIFT);\n                }\n                if (head >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    head = head.wrapping_add(1 << SHIFT);\n                }\n\n                // Rotate indices so that head falls into the first block.\n                let lap = (head >> SHIFT) / LAP;\n                tail = tail.wrapping_sub((lap * LAP) << SHIFT);\n                head = head.wrapping_sub((lap * LAP) << SHIFT);\n\n                // Remove the lower bits.\n                tail >>= SHIFT;\n                head >>= SHIFT;\n\n                // Return the difference minus the number of blocks between tail and head.\n                return tail - head - tail / LAP;\n            }\n        }\n    }\n}","impl<T> fmt::Debug for SegQueue<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"SegQueue { .. }\")\n    }\n}","unsafe impl<T: Send> Send for SegQueue<T> {}","unsafe impl<T: Send> Sync for SegQueue<T> {}"],"seg_queue::Slot":["impl<T> Slot<T> {\n    /// Waits until a value is written into the slot.\n    fn wait_write(&self) {\n        let backoff = Backoff::new();\n        while self.state.load(Ordering::Acquire) & WRITE == 0 {\n            backoff.snooze();\n        }\n    }\n}"]},"single_path_import":{"array_queue::ArrayQueue":"ArrayQueue","err::PopError":"PopError","err::PushError":"PushError","seg_queue::SegQueue":"SegQueue"},"srcs":{"<array_queue::ArrayQueue<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"ArrayQueue { .. }\")\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"<array_queue::ArrayQueue<T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        // Get the index of the head.\n        let hix = self.head.load(Ordering::Relaxed) & (self.one_lap - 1);\n\n        // Loop over all slots that hold a message and drop them.\n        for i in 0..self.len() {\n            // Compute the index of the next slot holding a message.\n            let index = if hix + i < self.cap {\n                hix + i\n            } else {\n                hix + i - self.cap\n            };\n\n            unsafe {\n                let p = {\n                    let slot = &mut *self.buffer.add(index);\n                    let value = &mut *slot.value.get();\n                    value.as_mut_ptr()\n                };\n                p.drop_in_place();\n            }\n        }\n\n        // Finally, deallocate the buffer, but don't run any destructors.\n        unsafe {\n            Vec::from_raw_parts(self.buffer, 0, self.cap);\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"<err::PopError as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        \"PopError\".fmt(f)\n    }","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))"],"<err::PopError as std::fmt::Display>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        \"popping from an empty queue\".fmt(f)\n    }","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))"],"<err::PushError<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        \"PushError(..)\".fmt(f)\n    }","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))"],"<err::PushError<T> as std::fmt::Display>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        \"pushing into a full queue\".fmt(f)\n    }","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))"],"<seg_queue::SegQueue<T> as std::default::Default>::default":["fn default() -> SegQueue<T>{\n        SegQueue::new()\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"<seg_queue::SegQueue<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"SegQueue { .. }\")\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"<seg_queue::SegQueue<T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        let mut head = self.head.index.load(Ordering::Relaxed);\n        let mut tail = self.tail.index.load(Ordering::Relaxed);\n        let mut block = self.head.block.load(Ordering::Relaxed);\n\n        // Erase the lower bits.\n        head &= !((1 << SHIFT) - 1);\n        tail &= !((1 << SHIFT) - 1);\n\n        unsafe {\n            // Drop all values between `head` and `tail` and deallocate the heap-allocated blocks.\n            while head != tail {\n                let offset = (head >> SHIFT) % LAP;\n\n                if offset < BLOCK_CAP {\n                    // Drop the value in the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    let p = &mut *slot.value.get();\n                    p.as_mut_ptr().drop_in_place();\n                } else {\n                    // Deallocate the block and move to the next one.\n                    let next = (*block).next.load(Ordering::Relaxed);\n                    drop(Box::from_raw(block));\n                    block = next;\n                }\n\n                head = head.wrapping_add(1 << SHIFT);\n            }\n\n            // Deallocate the last remaining block.\n            if !block.is_null() {\n                drop(Box::from_raw(block));\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"array_queue::ArrayQueue":["/// A bounded multi-producer multi-consumer queue.\n///\n/// This queue allocates a fixed-capacity buffer on construction, which is used to store pushed\n/// elements. The queue cannot hold more elements than the buffer allows. Attempting to push an\n/// element into a full queue will fail. Having a buffer allocated upfront makes this queue a bit\n/// faster than [`SegQueue`].\n///\n/// [`SegQueue`]: struct.SegQueue.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::{ArrayQueue, PushError};\n///\n/// let q = ArrayQueue::new(2);\n///\n/// assert_eq!(q.push('a'), Ok(()));\n/// assert_eq!(q.push('b'), Ok(()));\n/// assert_eq!(q.push('c'), Err(PushError('c')));\n/// assert_eq!(q.pop(), Ok('a'));\n/// ```\npub struct ArrayQueue<T> {\n    /// The head of the queue.\n    ///\n    /// This value is a \"stamp\" consisting of an index into the buffer and a lap, but packed into a\n    /// single `usize`. The lower bits represent the index, while the upper bits represent the lap.\n    ///\n    /// Elements are popped from the head of the queue.\n    head: CachePadded<AtomicUsize>,\n\n    /// The tail of the queue.\n    ///\n    /// This value is a \"stamp\" consisting of an index into the buffer and a lap, but packed into a\n    /// single `usize`. The lower bits represent the index, while the upper bits represent the lap.\n    ///\n    /// Elements are pushed into the tail of the queue.\n    tail: CachePadded<AtomicUsize>,\n\n    /// The buffer holding slots.\n    buffer: *mut Slot<T>,\n\n    /// The queue capacity.\n    cap: usize,\n\n    /// A stamp with the value of `{ lap: 1, index: 0 }`.\n    one_lap: usize,\n\n    /// Indicates that dropping an `ArrayQueue<T>` may drop elements of type `T`.\n    _marker: PhantomData<T>,\n}","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::capacity":["/// Returns the capacity of the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::ArrayQueue;\n///\n/// let q = ArrayQueue::<i32>::new(100);\n///\n/// assert_eq!(q.capacity(), 100);\n/// ```\npub fn capacity(&self) -> usize{\n        self.cap\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::is_empty":["/// Returns `true` if the queue is empty.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::ArrayQueue;\n///\n/// let q = ArrayQueue::new(100);\n///\n/// assert!(q.is_empty());\n/// q.push(1).unwrap();\n/// assert!(!q.is_empty());\n/// ```\npub fn is_empty(&self) -> bool{\n        let head = self.head.load(Ordering::SeqCst);\n        let tail = self.tail.load(Ordering::SeqCst);\n\n        // Is the tail lagging one lap behind head?\n        // Is the tail equal to the head?\n        //\n        // Note: If the head changes just before we load the tail, that means there was a moment\n        // when the channel was not empty, so it is safe to just return `false`.\n        tail == head\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::is_full":["/// Returns `true` if the queue is full.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::ArrayQueue;\n///\n/// let q = ArrayQueue::new(1);\n///\n/// assert!(!q.is_full());\n/// q.push(1).unwrap();\n/// assert!(q.is_full());\n/// ```\npub fn is_full(&self) -> bool{\n        let tail = self.tail.load(Ordering::SeqCst);\n        let head = self.head.load(Ordering::SeqCst);\n\n        // Is the head lagging one lap behind tail?\n        //\n        // Note: If the tail changes just before we load the head, that means there was a moment\n        // when the queue was not full, so it is safe to just return `false`.\n        head.wrapping_add(self.one_lap) == tail\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::len":["/// Returns the number of elements in the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::ArrayQueue;\n///\n/// let q = ArrayQueue::new(100);\n/// assert_eq!(q.len(), 0);\n///\n/// q.push(10).unwrap();\n/// assert_eq!(q.len(), 1);\n///\n/// q.push(20).unwrap();\n/// assert_eq!(q.len(), 2);\n/// ```\npub fn len(&self) -> usize{\n        loop {\n            // Load the tail, then load the head.\n            let tail = self.tail.load(Ordering::SeqCst);\n            let head = self.head.load(Ordering::SeqCst);\n\n            // If the tail didn't change, we've got consistent values to work with.\n            if self.tail.load(Ordering::SeqCst) == tail {\n                let hix = head & (self.one_lap - 1);\n                let tix = tail & (self.one_lap - 1);\n\n                return if hix < tix {\n                    tix - hix\n                } else if hix > tix {\n                    self.cap - hix + tix\n                } else if tail == head {\n                    0\n                } else {\n                    self.cap\n                };\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::new":["/// Creates a new bounded queue with the given capacity.\n///\n/// # Panics\n///\n/// Panics if the capacity is zero.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::ArrayQueue;\n///\n/// let q = ArrayQueue::<i32>::new(100);\n/// ```\npub fn new(cap: usize) -> ArrayQueue<T>{\n        assert!(cap > 0, \"capacity must be non-zero\");\n\n        // Head is initialized to `{ lap: 0, index: 0 }`.\n        // Tail is initialized to `{ lap: 0, index: 0 }`.\n        let head = 0;\n        let tail = 0;\n\n        // Allocate a buffer of `cap` slots initialized\n        // with stamps.\n        let buffer = {\n            let mut v: Vec<Slot<T>> = (0..cap)\n                .map(|i| {\n                    // Set the stamp to `{ lap: 0, index: i }`.\n                    Slot {\n                        stamp: AtomicUsize::new(i),\n                        value: UnsafeCell::new(MaybeUninit::uninit()),\n                    }\n                })\n                .collect();\n            let ptr = v.as_mut_ptr();\n            mem::forget(v);\n            ptr\n        };\n\n        // One lap is the smallest power of two greater than `cap`.\n        let one_lap = (cap + 1).next_power_of_two();\n\n        ArrayQueue {\n            buffer,\n            cap,\n            one_lap,\n            head: CachePadded::new(AtomicUsize::new(head)),\n            tail: CachePadded::new(AtomicUsize::new(tail)),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::pop":["/// Attempts to pop an element from the queue.\n///\n/// If the queue is empty, an error is returned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::{ArrayQueue, PopError};\n///\n/// let q = ArrayQueue::new(1);\n/// assert_eq!(q.push(10), Ok(()));\n///\n/// assert_eq!(q.pop(), Ok(10));\n/// assert_eq!(q.pop(), Err(PopError));\n/// ```\npub fn pop(&self) -> Result<T, PopError>{\n        let backoff = Backoff::new();\n        let mut head = self.head.load(Ordering::Relaxed);\n\n        loop {\n            // Deconstruct the head.\n            let index = head & (self.one_lap - 1);\n            let lap = head & !(self.one_lap - 1);\n\n            // Inspect the corresponding slot.\n            let slot = unsafe { &*self.buffer.add(index) };\n            let stamp = slot.stamp.load(Ordering::Acquire);\n\n            // If the the stamp is ahead of the head by 1, we may attempt to pop.\n            if head + 1 == stamp {\n                let new = if index + 1 < self.cap {\n                    // Same lap, incremented index.\n                    // Set to `{ lap: lap, index: index + 1 }`.\n                    head + 1\n                } else {\n                    // One lap forward, index wraps around to zero.\n                    // Set to `{ lap: lap.wrapping_add(1), index: 0 }`.\n                    lap.wrapping_add(self.one_lap)\n                };\n\n                // Try moving the head.\n                match self.head.compare_exchange_weak(\n                    head,\n                    new,\n                    Ordering::SeqCst,\n                    Ordering::Relaxed,\n                ) {\n                    Ok(_) => {\n                        // Read the value from the slot and update the stamp.\n                        let msg = unsafe { slot.value.get().read().assume_init() };\n                        slot.stamp\n                            .store(head.wrapping_add(self.one_lap), Ordering::Release);\n                        return Ok(msg);\n                    }\n                    Err(h) => {\n                        head = h;\n                        backoff.spin();\n                    }\n                }\n            } else if stamp == head {\n                atomic::fence(Ordering::SeqCst);\n                let tail = self.tail.load(Ordering::Relaxed);\n\n                // If the tail equals the head, that means the channel is empty.\n                if tail == head {\n                    return Err(PopError);\n                }\n\n                backoff.spin();\n                head = self.head.load(Ordering::Relaxed);\n            } else {\n                // Snooze because we need to wait for the stamp to get updated.\n                backoff.snooze();\n                head = self.head.load(Ordering::Relaxed);\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::ArrayQueue::<T>::push":["/// Attempts to push an element into the queue.\n///\n/// If the queue is full, the element is returned back as an error.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::{ArrayQueue, PushError};\n///\n/// let q = ArrayQueue::new(1);\n///\n/// assert_eq!(q.push(10), Ok(()));\n/// assert_eq!(q.push(20), Err(PushError(20)));\n/// ```\npub fn push(&self, value: T) -> Result<(), PushError<T>>{\n        let backoff = Backoff::new();\n        let mut tail = self.tail.load(Ordering::Relaxed);\n\n        loop {\n            // Deconstruct the tail.\n            let index = tail & (self.one_lap - 1);\n            let lap = tail & !(self.one_lap - 1);\n\n            // Inspect the corresponding slot.\n            let slot = unsafe { &*self.buffer.add(index) };\n            let stamp = slot.stamp.load(Ordering::Acquire);\n\n            // If the tail and the stamp match, we may attempt to push.\n            if tail == stamp {\n                let new_tail = if index + 1 < self.cap {\n                    // Same lap, incremented index.\n                    // Set to `{ lap: lap, index: index + 1 }`.\n                    tail + 1\n                } else {\n                    // One lap forward, index wraps around to zero.\n                    // Set to `{ lap: lap.wrapping_add(1), index: 0 }`.\n                    lap.wrapping_add(self.one_lap)\n                };\n\n                // Try moving the tail.\n                match self.tail.compare_exchange_weak(\n                    tail,\n                    new_tail,\n                    Ordering::SeqCst,\n                    Ordering::Relaxed,\n                ) {\n                    Ok(_) => {\n                        // Write the value into the slot and update the stamp.\n                        unsafe {\n                            slot.value.get().write(MaybeUninit::new(value));\n                        }\n                        slot.stamp.store(tail + 1, Ordering::Release);\n                        return Ok(());\n                    }\n                    Err(t) => {\n                        tail = t;\n                        backoff.spin();\n                    }\n                }\n            } else if stamp.wrapping_add(self.one_lap) == tail + 1 {\n                atomic::fence(Ordering::SeqCst);\n                let head = self.head.load(Ordering::Relaxed);\n\n                // If the head lags one lap behind the tail as well...\n                if head.wrapping_add(self.one_lap) == tail {\n                    // ...then the queue is full.\n                    return Err(PushError(value));\n                }\n\n                backoff.spin();\n                tail = self.tail.load(Ordering::Relaxed);\n            } else {\n                // Snooze because we need to wait for the stamp to get updated.\n                backoff.snooze();\n                tail = self.tail.load(Ordering::Relaxed);\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"array_queue::Slot":["/// A slot in a queue.\nstruct Slot<T> {\n    /// The current stamp.\n    ///\n    /// If the stamp equals the tail, this node will be next written to. If it equals head + 1,\n    /// this node will be next read from.\n    stamp: AtomicUsize,\n\n    /// The value in this slot.\n    value: UnsafeCell<MaybeUninit<T>>,\n}","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))"],"err::PopError":["/// Error which occurs when popping from an empty queue.\npub struct PopError;","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))"],"err::PushError":["/// Error which occurs when pushing into a full queue.\npub struct PushError<T>(pub T);","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))"],"seg_queue::Block":["/// A block in a linked list.\n///\n/// Each block in the list can hold up to `BLOCK_CAP` values.\nstruct Block<T> {\n    /// The next block in the linked list.\n    next: AtomicPtr<Block<T>>,\n\n    /// Slots for values.\n    slots: [Slot<T>; BLOCK_CAP],\n}","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::Block::<T>::destroy":["/// Sets the `DESTROY` bit in slots starting from `start` and destroys the block.\nunsafe fn destroy(this: *mut Block<T>, start: usize){\n        // It is not necessary to set the `DESTROY` bit in the last slot because that slot has\n        // begun destruction of the block.\n        for i in start..BLOCK_CAP - 1 {\n            let slot = (*this).slots.get_unchecked(i);\n\n            // Mark the `DESTROY` bit if a thread is still using the slot.\n            if slot.state.load(Ordering::Acquire) & READ == 0\n                && slot.state.fetch_or(DESTROY, Ordering::AcqRel) & READ == 0\n            {\n                // If a thread is still using the slot, it will continue destruction of the block.\n                return;\n            }\n        }\n\n        // No thread is using the block, now it is safe to destroy it.\n        drop(Box::from_raw(this));\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::Block::<T>::new":["/// Creates an empty block that starts at `start_index`.\nfn new() -> Block<T>{\n        // SAFETY: This is safe because:\n        //  [1] `Block::next` (AtomicPtr) may be safely zero initialized.\n        //  [2] `Block::slots` (Array) may be safely zero initialized because of [3, 4].\n        //  [3] `Slot::value` (UnsafeCell) may be safely zero initialized because it\n        //       holds a MaybeUninit.\n        //  [4] `Slot::state` (AtomicUsize) may be safely zero initialized.\n        unsafe { MaybeUninit::zeroed().assume_init() }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::Block::<T>::wait_next":["/// Waits until the next pointer is set.\nfn wait_next(&self) -> *mut Block<T>{\n        let backoff = Backoff::new();\n        loop {\n            let next = self.next.load(Ordering::Acquire);\n            if !next.is_null() {\n                return next;\n            }\n            backoff.snooze();\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::Position":["/// A position in a queue.\nstruct Position<T> {\n    /// The index in the queue.\n    index: AtomicUsize,\n\n    /// The block in the linked list.\n    block: AtomicPtr<Block<T>>,\n}","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::SegQueue":["/// An unbounded multi-producer multi-consumer queue.\n///\n/// This queue is implemented as a linked list of segments, where each segment is a small buffer\n/// that can hold a handful of elements. There is no limit to how many elements can be in the queue\n/// at a time. However, since segments need to be dynamically allocated as elements get pushed,\n/// this queue is somewhat slower than [`ArrayQueue`].\n///\n/// [`ArrayQueue`]: struct.ArrayQueue.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::{PopError, SegQueue};\n///\n/// let q = SegQueue::new();\n///\n/// q.push('a');\n/// q.push('b');\n///\n/// assert_eq!(q.pop(), Ok('a'));\n/// assert_eq!(q.pop(), Ok('b'));\n/// assert_eq!(q.pop(), Err(PopError));\n/// ```\npub struct SegQueue<T> {\n    /// The head of the queue.\n    head: CachePadded<Position<T>>,\n\n    /// The tail of the queue.\n    tail: CachePadded<Position<T>>,\n\n    /// Indicates that dropping a `SegQueue<T>` may drop values of type `T`.\n    _marker: PhantomData<T>,\n}","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::SegQueue::<T>::is_empty":["/// Returns `true` if the queue is empty.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::SegQueue;\n///\n/// let q = SegQueue::new();\n///\n/// assert!(q.is_empty());\n/// q.push(1);\n/// assert!(!q.is_empty());\n/// ```\npub fn is_empty(&self) -> bool{\n        let head = self.head.index.load(Ordering::SeqCst);\n        let tail = self.tail.index.load(Ordering::SeqCst);\n        head >> SHIFT == tail >> SHIFT\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::SegQueue::<T>::len":["/// Returns the number of elements in the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::SegQueue;\n///\n/// let q = SegQueue::new();\n/// assert_eq!(q.len(), 0);\n///\n/// q.push(10);\n/// assert_eq!(q.len(), 1);\n///\n/// q.push(20);\n/// assert_eq!(q.len(), 2);\n/// ```\npub fn len(&self) -> usize{\n        loop {\n            // Load the tail index, then load the head index.\n            let mut tail = self.tail.index.load(Ordering::SeqCst);\n            let mut head = self.head.index.load(Ordering::SeqCst);\n\n            // If the tail index didn't change, we've got consistent indices to work with.\n            if self.tail.index.load(Ordering::SeqCst) == tail {\n                // Erase the lower bits.\n                tail &= !((1 << SHIFT) - 1);\n                head &= !((1 << SHIFT) - 1);\n\n                // Fix up indices if they fall onto block ends.\n                if (tail >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    tail = tail.wrapping_add(1 << SHIFT);\n                }\n                if (head >> SHIFT) & (LAP - 1) == LAP - 1 {\n                    head = head.wrapping_add(1 << SHIFT);\n                }\n\n                // Rotate indices so that head falls into the first block.\n                let lap = (head >> SHIFT) / LAP;\n                tail = tail.wrapping_sub((lap * LAP) << SHIFT);\n                head = head.wrapping_sub((lap * LAP) << SHIFT);\n\n                // Remove the lower bits.\n                tail >>= SHIFT;\n                head >>= SHIFT;\n\n                // Return the difference minus the number of blocks between tail and head.\n                return tail - head - tail / LAP;\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::SegQueue::<T>::new":["/// Creates a new unbounded queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::SegQueue;\n///\n/// let q = SegQueue::<i32>::new();\n/// ```\npub fn new() -> SegQueue<T>{\n        SegQueue {\n            head: CachePadded::new(Position {\n                block: AtomicPtr::new(ptr::null_mut()),\n                index: AtomicUsize::new(0),\n            }),\n            tail: CachePadded::new(Position {\n                block: AtomicPtr::new(ptr::null_mut()),\n                index: AtomicUsize::new(0),\n            }),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::SegQueue::<T>::pop":["/// Pops an element from the queue.\n///\n/// If the queue is empty, an error is returned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::{PopError, SegQueue};\n///\n/// let q = SegQueue::new();\n///\n/// q.push(10);\n/// assert_eq!(q.pop(), Ok(10));\n/// assert_eq!(q.pop(), Err(PopError));\n/// ```\npub fn pop(&self) -> Result<T, PopError>{\n        let backoff = Backoff::new();\n        let mut head = self.head.index.load(Ordering::Acquire);\n        let mut block = self.head.block.load(Ordering::Acquire);\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (head >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n                head = self.head.index.load(Ordering::Acquire);\n                block = self.head.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            let mut new_head = head + (1 << SHIFT);\n\n            if new_head & HAS_NEXT == 0 {\n                atomic::fence(Ordering::SeqCst);\n                let tail = self.tail.index.load(Ordering::Relaxed);\n\n                // If the tail equals the head, that means the queue is empty.\n                if head >> SHIFT == tail >> SHIFT {\n                    return Err(PopError);\n                }\n\n                // If head and tail are not in the same block, set `HAS_NEXT` in head.\n                if (head >> SHIFT) / LAP != (tail >> SHIFT) / LAP {\n                    new_head |= HAS_NEXT;\n                }\n            }\n\n            // The block can be null here only if the first push operation is in progress. In that\n            // case, just wait until it gets initialized.\n            if block.is_null() {\n                backoff.snooze();\n                head = self.head.index.load(Ordering::Acquire);\n                block = self.head.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // Try moving the head index forward.\n            match self.head.index.compare_exchange_weak(\n                head,\n                new_head,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, move to the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next = (*block).wait_next();\n                        let mut next_index = (new_head & !HAS_NEXT).wrapping_add(1 << SHIFT);\n                        if !(*next).next.load(Ordering::Relaxed).is_null() {\n                            next_index |= HAS_NEXT;\n                        }\n\n                        self.head.block.store(next, Ordering::Release);\n                        self.head.index.store(next_index, Ordering::Release);\n                    }\n\n                    // Read the value.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.wait_write();\n                    let value = slot.value.get().read().assume_init();\n\n                    // Destroy the block if we've reached the end, or if another thread wanted to\n                    // destroy but couldn't because we were busy reading from the slot.\n                    if offset + 1 == BLOCK_CAP {\n                        Block::destroy(block, 0);\n                    } else if slot.state.fetch_or(READ, Ordering::AcqRel) & DESTROY != 0 {\n                        Block::destroy(block, offset + 1);\n                    }\n\n                    return Ok(value);\n                },\n                Err(h) => {\n                    head = h;\n                    block = self.head.block.load(Ordering::Acquire);\n                    backoff.spin();\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::SegQueue::<T>::push":["/// Pushes an element into the queue.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_queue::SegQueue;\n///\n/// let q = SegQueue::new();\n///\n/// q.push(10);\n/// q.push(20);\n/// ```\npub fn push(&self, value: T){\n        let backoff = Backoff::new();\n        let mut tail = self.tail.index.load(Ordering::Acquire);\n        let mut block = self.tail.block.load(Ordering::Acquire);\n        let mut next_block = None;\n\n        loop {\n            // Calculate the offset of the index into the block.\n            let offset = (tail >> SHIFT) % LAP;\n\n            // If we reached the end of the block, wait until the next one is installed.\n            if offset == BLOCK_CAP {\n                backoff.snooze();\n                tail = self.tail.index.load(Ordering::Acquire);\n                block = self.tail.block.load(Ordering::Acquire);\n                continue;\n            }\n\n            // If we're going to have to install the next block, allocate it in advance in order to\n            // make the wait for other threads as short as possible.\n            if offset + 1 == BLOCK_CAP && next_block.is_none() {\n                next_block = Some(Box::new(Block::<T>::new()));\n            }\n\n            // If this is the first push operation, we need to allocate the first block.\n            if block.is_null() {\n                let new = Box::into_raw(Box::new(Block::<T>::new()));\n\n                if self\n                    .tail\n                    .block\n                    .compare_and_swap(block, new, Ordering::Release)\n                    == block\n                {\n                    self.head.block.store(new, Ordering::Release);\n                    block = new;\n                } else {\n                    next_block = unsafe { Some(Box::from_raw(new)) };\n                    tail = self.tail.index.load(Ordering::Acquire);\n                    block = self.tail.block.load(Ordering::Acquire);\n                    continue;\n                }\n            }\n\n            let new_tail = tail + (1 << SHIFT);\n\n            // Try advancing the tail forward.\n            match self.tail.index.compare_exchange_weak(\n                tail,\n                new_tail,\n                Ordering::SeqCst,\n                Ordering::Acquire,\n            ) {\n                Ok(_) => unsafe {\n                    // If we've reached the end of the block, install the next one.\n                    if offset + 1 == BLOCK_CAP {\n                        let next_block = Box::into_raw(next_block.unwrap());\n                        let next_index = new_tail.wrapping_add(1 << SHIFT);\n\n                        self.tail.block.store(next_block, Ordering::Release);\n                        self.tail.index.store(next_index, Ordering::Release);\n                        (*block).next.store(next_block, Ordering::Release);\n                    }\n\n                    // Write the value into the slot.\n                    let slot = (*block).slots.get_unchecked(offset);\n                    slot.value.get().write(MaybeUninit::new(value));\n                    slot.state.fetch_or(WRITE, Ordering::Release);\n\n                    return;\n                },\n                Err(t) => {\n                    tail = t;\n                    block = self.tail.block.load(Ordering::Acquire);\n                    backoff.spin();\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::Slot":["/// A slot in a block.\nstruct Slot<T> {\n    /// The value.\n    value: UnsafeCell<MaybeUninit<T>>,\n\n    /// The state of the slot.\n    state: AtomicUsize,\n}","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"],"seg_queue::Slot::<T>::wait_write":["/// Waits until a value is written into the slot.\nfn wait_write(&self){\n        let backoff = Backoff::new();\n        while self.state.load(Ordering::Acquire) & WRITE == 0 {\n            backoff.snooze();\n        }\n    }","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))"]},"struct_constructor":{"array_queue::ArrayQueue":["new"],"bool":["eq","is_empty","is_full"],"err::PopError":["clone"],"err::PushError":["clone"],"seg_queue::Block":["new"],"seg_queue::SegQueue":["default","new"],"usize":["capacity","len"]},"struct_to_trait":{"array_queue::ArrayQueue":["std::fmt::Debug","std::marker::Send","std::marker::Sync","std::ops::Drop"],"err::PopError":["std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::error::Error","std::fmt::Debug","std::fmt::Display","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq"],"err::PushError":["std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::error::Error","std::fmt::Debug","std::fmt::Display","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq"],"seg_queue::SegQueue":["std::default::Default","std::fmt::Debug","std::marker::Send","std::marker::Sync","std::ops::Drop"]},"targets":{"<array_queue::ArrayQueue<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))","std::fmt::Debug"],"<array_queue::ArrayQueue<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))","std::ops::Drop"],"<err::PopError as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))","std::fmt::Debug"],"<err::PopError as std::fmt::Display>::fmt":["fmt","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))","std::fmt::Display"],"<err::PushError<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))","std::fmt::Debug"],"<err::PushError<T> as std::fmt::Display>::fmt":["fmt","Real(LocalPath(\"crossbeam-queue/src/err.rs\"))","std::fmt::Display"],"<seg_queue::SegQueue<T> as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))","std::default::Default"],"<seg_queue::SegQueue<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))","std::fmt::Debug"],"<seg_queue::SegQueue<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))","std::ops::Drop"],"array_queue::ArrayQueue::<T>::capacity":["capacity","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"array_queue::ArrayQueue::<T>::is_empty":["is_empty","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"array_queue::ArrayQueue::<T>::is_full":["is_full","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"array_queue::ArrayQueue::<T>::len":["len","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"array_queue::ArrayQueue::<T>::new":["new","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"array_queue::ArrayQueue::<T>::pop":["pop","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"array_queue::ArrayQueue::<T>::push":["push","Real(LocalPath(\"crossbeam-queue/src/array_queue.rs\"))",""],"seg_queue::Block::<T>::destroy":["destroy","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::Block::<T>::new":["new","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::Block::<T>::wait_next":["wait_next","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::SegQueue::<T>::is_empty":["is_empty","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::SegQueue::<T>::len":["len","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::SegQueue::<T>::new":["new","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::SegQueue::<T>::pop":["pop","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::SegQueue::<T>::push":["push","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""],"seg_queue::Slot::<T>::wait_write":["wait_write","Real(LocalPath(\"crossbeam-queue/src/seg_queue.rs\"))",""]},"trait_to_struct":{"std::clone::Clone":["err::PopError","err::PushError"],"std::cmp::Eq":["err::PopError","err::PushError"],"std::cmp::PartialEq":["err::PopError","err::PushError"],"std::default::Default":["seg_queue::SegQueue"],"std::error::Error":["err::PopError","err::PushError"],"std::fmt::Debug":["array_queue::ArrayQueue","err::PopError","err::PushError","seg_queue::SegQueue"],"std::fmt::Display":["err::PopError","err::PushError"],"std::marker::Copy":["err::PopError","err::PushError"],"std::marker::Send":["array_queue::ArrayQueue","seg_queue::SegQueue"],"std::marker::StructuralEq":["err::PopError","err::PushError"],"std::marker::StructuralPartialEq":["err::PopError","err::PushError"],"std::marker::Sync":["array_queue::ArrayQueue","seg_queue::SegQueue"],"std::ops::Drop":["array_queue::ArrayQueue","seg_queue::SegQueue"]},"type_to_def_path":{"array_queue::ArrayQueue<T>":"array_queue::ArrayQueue","array_queue::Slot<T>":"array_queue::Slot","err::PopError":"err::PopError","err::PushError<T>":"err::PushError","seg_queue::Block<T>":"seg_queue::Block","seg_queue::Position<T>":"seg_queue::Position","seg_queue::SegQueue<T>":"seg_queue::SegQueue","seg_queue::Slot<T>":"seg_queue::Slot"}}