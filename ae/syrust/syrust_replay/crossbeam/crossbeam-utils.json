{"dependencies":{"<<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder as std::fmt::Debug>::fmt":["<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<atomic::atomic_cell::AtomicCell<T> as std::default::Default>::default":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"<atomic::atomic_cell::AtomicCell<T> as std::fmt::Debug>::fmt":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<atomic::seq_lock::SeqLockWriteGuard as std::ops::Drop>::drop":["atomic::seq_lock::SeqLock","atomic::seq_lock::SeqLockWriteGuard","std::sync::atomic::AtomicUsize"],"<backoff::Backoff as std::default::Default>::default":["backoff::Backoff","std::cell::Cell"],"<backoff::Backoff as std::fmt::Debug>::fmt":["backoff::Backoff","std::cell::Cell","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<cache_padded::CachePadded<T> as lazy_static::__Deref>::deref":["cache_padded::CachePadded","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::clone::Clone>::clone":["cache_padded::CachePadded","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::cmp::Eq>::assert_receiver_is_total_eq":["cache_padded::CachePadded","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::cmp::PartialEq>::eq":["cache_padded::CachePadded","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::convert::From<T>>::from":["cache_padded::CachePadded","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::default::Default>::default":["cache_padded::CachePadded","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::fmt::Debug>::fmt":["cache_padded::CachePadded","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<cache_padded::CachePadded<T> as std::hash::Hash>::hash":["cache_padded::CachePadded","std::hash::Hasher","std::marker::Sized"],"<cache_padded::CachePadded<T> as std::ops::DerefMut>::deref_mut":["cache_padded::CachePadded","std::marker::Sized"],"<std::sync::atomic::AtomicBool as atomic::consume::AtomicConsume>::load_consume":["std::sync::atomic::AtomicBool"],"<std::sync::atomic::AtomicIsize as atomic::consume::AtomicConsume>::load_consume":["std::sync::atomic::AtomicIsize"],"<std::sync::atomic::AtomicPtr<T> as atomic::consume::AtomicConsume>::load_consume":["std::marker::Sized","std::sync::atomic::AtomicPtr"],"<std::sync::atomic::AtomicUsize as atomic::consume::AtomicConsume>::load_consume":["std::sync::atomic::AtomicUsize"],"<sync::parker::Parker as std::default::Default>::default":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"<sync::parker::Parker as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"<sync::parker::Unparker as std::clone::Clone>::clone":["std::sync::Arc","sync::parker::Unparker"],"<sync::parker::Unparker as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::Sized","std::result::Result","std::sync::Arc","sync::parker::Unparker"],"<sync::sharded_lock::Registration as std::ops::Drop>::drop":["std::thread::ThreadId","sync::sharded_lock::Registration"],"<sync::sharded_lock::ShardedLock<T> as std::convert::From<T>>::from":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","sync::sharded_lock::ShardedLock"],"<sync::sharded_lock::ShardedLock<T> as std::default::Default>::default":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","sync::sharded_lock::ShardedLock"],"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder":["<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as lazy_static::__Deref>::deref":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::RwLockReadGuard","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as std::fmt::Debug>::fmt":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::RwLockReadGuard","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as std::fmt::Display>::fmt":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::RwLockReadGuard","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as lazy_static::__Deref>::deref":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockWriteGuard"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::fmt::Debug>::fmt":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockWriteGuard"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::fmt::Display>::fmt":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockWriteGuard"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::ops::DerefMut>::deref_mut":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockWriteGuard"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::ops::Drop>::drop":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockWriteGuard"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::LazyStatic>::initialize":["sync::sharded_lock::THREAD_INDICES"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref":["std::sync::Mutex","sync::sharded_lock::THREAD_INDICES"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref::__stability":["std::sync::Mutex"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref::__static_ref_initialize":["std::sync::Mutex"],"<sync::wait_group::WaitGroup as std::clone::Clone>::clone":["std::sync::Arc","sync::wait_group::WaitGroup"],"<sync::wait_group::WaitGroup as std::default::Default>::default":["std::sync::Arc","sync::wait_group::WaitGroup"],"<sync::wait_group::WaitGroup as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::Sized","std::result::Result","std::sync::Arc","sync::wait_group::WaitGroup"],"<sync::wait_group::WaitGroup as std::ops::Drop>::drop":["std::sync::Arc","sync::wait_group::WaitGroup"],"<thread::Scope<'_> as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::Arc","sync::wait_group::WaitGroup","thread::Scope"],"<thread::ScopedJoinHandle<'_, T> as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::Arc","std::thread::Thread","thread::ScopedJoinHandle"],"<thread::ScopedJoinHandle<'_, T> as std::os::unix::thread::JoinHandleExt>::as_pthread_t":["std::marker::PhantomData","std::marker::Sized","std::sync::Arc","std::thread::Thread","thread::ScopedJoinHandle"],"<thread::ScopedJoinHandle<'_, T> as std::os::unix::thread::JoinHandleExt>::into_pthread_t":["std::marker::PhantomData","std::marker::Sized","std::sync::Arc","std::thread::Thread","thread::ScopedJoinHandle"],"<thread::ScopedThreadBuilder<'scope, 'env> as std::fmt::Debug>::fmt":["std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::Arc","std::thread::Builder","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedThreadBuilder"],"atomic::atomic_cell::AtomicCell":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::as_ptr":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::compare_and_swap":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::compare_exchange":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell","std::marker::Sized","std::result::Result"],"atomic::atomic_cell::AtomicCell::<T>::into_inner":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::is_lock_free":[],"atomic::atomic_cell::AtomicCell::<T>::load":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::new":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::store":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::swap":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<T>::take":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<bool>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<bool>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<bool>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_add":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_and":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_or":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_sub":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_xor":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell"],"atomic::atomic_cell::AtomicUnit":["atomic::atomic_cell::AtomicUnit"],"atomic::atomic_cell::AtomicUnit::compare_exchange_weak":["atomic::atomic_cell::AtomicUnit","std::marker::Sized","std::result::Result","std::sync::atomic::Ordering"],"atomic::atomic_cell::AtomicUnit::load":["atomic::atomic_cell::AtomicUnit","std::sync::atomic::Ordering"],"atomic::atomic_cell::AtomicUnit::store":["atomic::atomic_cell::AtomicUnit","std::sync::atomic::Ordering"],"atomic::atomic_cell::AtomicUnit::swap":["atomic::atomic_cell::AtomicUnit","std::sync::atomic::Ordering"],"atomic::atomic_cell::atomic_compare_exchange_weak":["cache_padded::CachePadded","std::cmp::Eq","std::marker::Copy","std::marker::Sized","std::result::Result"],"atomic::atomic_cell::atomic_is_lock_free":["std::marker::Sized"],"atomic::atomic_cell::atomic_load":["cache_padded::CachePadded","std::marker::Copy","std::marker::Sized"],"atomic::atomic_cell::atomic_store":["std::marker::Sized"],"atomic::atomic_cell::atomic_swap":["std::marker::Sized"],"atomic::atomic_cell::can_transmute":["std::marker::Sized"],"atomic::atomic_cell::lock":["atomic::seq_lock::SeqLock","std::sync::atomic::AtomicUsize"],"atomic::consume::AtomicConsume::load_consume":[],"atomic::seq_lock::SeqLock":["atomic::seq_lock::SeqLock","std::sync::atomic::AtomicUsize"],"atomic::seq_lock::SeqLock::new":["atomic::seq_lock::SeqLock","std::sync::atomic::AtomicUsize"],"atomic::seq_lock::SeqLock::optimistic_read":["atomic::seq_lock::SeqLock","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicUsize"],"atomic::seq_lock::SeqLock::validate_read":["atomic::seq_lock::SeqLock","std::sync::atomic::AtomicUsize"],"atomic::seq_lock::SeqLock::write":["atomic::seq_lock::SeqLock","atomic::seq_lock::SeqLockWriteGuard","std::sync::atomic::AtomicUsize"],"atomic::seq_lock::SeqLockWriteGuard":["atomic::seq_lock::SeqLock","atomic::seq_lock::SeqLockWriteGuard","std::sync::atomic::AtomicUsize"],"atomic::seq_lock::SeqLockWriteGuard::abort":["atomic::seq_lock::SeqLock","atomic::seq_lock::SeqLockWriteGuard","std::sync::atomic::AtomicUsize"],"backoff::Backoff":["backoff::Backoff","std::cell::Cell"],"backoff::Backoff::is_completed":["backoff::Backoff","std::cell::Cell"],"backoff::Backoff::new":["backoff::Backoff","std::cell::Cell"],"backoff::Backoff::reset":["backoff::Backoff","std::cell::Cell"],"backoff::Backoff::snooze":["backoff::Backoff","std::cell::Cell"],"backoff::Backoff::spin":["backoff::Backoff","std::cell::Cell"],"cache_padded::CachePadded":["cache_padded::CachePadded","std::marker::Sized"],"cache_padded::CachePadded::<T>::into_inner":["cache_padded::CachePadded","std::marker::Sized"],"cache_padded::CachePadded::<T>::new":["cache_padded::CachePadded","std::marker::Sized"],"sync::parker::Inner":["std::sync::Condvar","std::sync::Mutex","std::sync::atomic::AtomicUsize","sync::parker::Inner"],"sync::parker::Inner::park":["std::marker::Sized","std::option::Option","std::sync::Condvar","std::sync::Mutex","std::sync::atomic::AtomicUsize","sync::parker::Inner"],"sync::parker::Inner::unpark":["std::sync::Condvar","std::sync::Mutex","std::sync::atomic::AtomicUsize","sync::parker::Inner"],"sync::parker::Parker":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Parker::from_raw":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Parker::into_raw":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Parker::new":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Parker::park":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Parker::park_timeout":["std::marker::PhantomData","std::sync::Arc","std::time::Duration","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Parker::unparker":["std::marker::PhantomData","std::sync::Arc","sync::parker::Parker","sync::parker::Unparker"],"sync::parker::Unparker":["std::sync::Arc","sync::parker::Unparker"],"sync::parker::Unparker::from_raw":["std::sync::Arc","sync::parker::Unparker"],"sync::parker::Unparker::into_raw":["std::sync::Arc","sync::parker::Unparker"],"sync::parker::Unparker::unpark":["std::sync::Arc","sync::parker::Unparker"],"sync::sharded_lock::REGISTRATION::__getit":["std::marker::Sized","std::option::Option"],"sync::sharded_lock::REGISTRATION::__init":["std::thread::ThreadId","sync::sharded_lock::Registration"],"sync::sharded_lock::Registration":["std::thread::ThreadId","sync::sharded_lock::Registration"],"sync::sharded_lock::Shard":["std::cell::UnsafeCell","std::sync::RwLock","sync::sharded_lock::Shard"],"sync::sharded_lock::ShardedLock":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::get_mut":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::into_inner":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::is_poisoned":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::new":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::read":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::try_read":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::try_write":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLock::<T>::write":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::Sized","std::result::Result","sync::sharded_lock::ShardedLock"],"sync::sharded_lock::ShardedLockReadGuard":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","std::sync::RwLockReadGuard","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard"],"sync::sharded_lock::ShardedLockWriteGuard":["std::alloc::Allocator","std::boxed::Box","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Sized","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockWriteGuard"],"sync::sharded_lock::THREAD_INDICES":["sync::sharded_lock::THREAD_INDICES"],"sync::sharded_lock::ThreadIndices":["std::alloc::Allocator","std::collections::HashMap","std::marker::Sized","std::vec::Vec","sync::sharded_lock::ThreadIndices"],"sync::sharded_lock::current_index":["std::marker::Sized","std::option::Option"],"sync::wait_group::Inner":["std::sync::Condvar","std::sync::Mutex","sync::wait_group::Inner"],"sync::wait_group::WaitGroup":["std::sync::Arc","sync::wait_group::WaitGroup"],"sync::wait_group::WaitGroup::new":["std::sync::Arc","sync::wait_group::WaitGroup"],"sync::wait_group::WaitGroup::wait":["std::sync::Arc","sync::wait_group::WaitGroup"],"thread::Scope":["std::marker::PhantomData","std::sync::Arc","sync::wait_group::WaitGroup","thread::Scope"],"thread::Scope::<'env>::builder":["std::marker::PhantomData","std::sync::Arc","std::thread::Builder","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedThreadBuilder"],"thread::Scope::<'env>::spawn":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Send","std::marker::Sized","std::ops::FnOnce","std::sync::Arc","std::thread::Thread","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedJoinHandle"],"thread::ScopedJoinHandle":["std::marker::PhantomData","std::marker::Sized","std::sync::Arc","std::thread::Thread","thread::ScopedJoinHandle"],"thread::ScopedJoinHandle::<'_, T>::join":["std::marker::PhantomData","std::marker::Sized","std::result::Result","std::sync::Arc","std::thread::Thread","thread::ScopedJoinHandle"],"thread::ScopedJoinHandle::<'_, T>::thread":["std::marker::PhantomData","std::marker::Sized","std::sync::Arc","std::thread::Thread","thread::ScopedJoinHandle"],"thread::ScopedThreadBuilder":["std::marker::PhantomData","std::sync::Arc","std::thread::Builder","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedThreadBuilder"],"thread::ScopedThreadBuilder::<'scope, 'env>::name":["std::marker::PhantomData","std::string::String","std::sync::Arc","std::thread::Builder","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedThreadBuilder"],"thread::ScopedThreadBuilder::<'scope, 'env>::spawn":["atomic::atomic_cell::AtomicCell","std::cell::UnsafeCell","std::marker::PhantomData","std::marker::Send","std::marker::Sized","std::ops::FnOnce","std::result::Result","std::sync::Arc","std::thread::Builder","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedThreadBuilder"],"thread::ScopedThreadBuilder::<'scope, 'env>::stack_size":["std::marker::PhantomData","std::sync::Arc","std::thread::Builder","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedThreadBuilder"],"thread::scope":["std::marker::Sized","std::ops::FnOnce","std::result::Result"]},"glob_path_import":{},"self_to_fn":{"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder":["impl fmt::Debug for LockedPlaceholder {\n                    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n                        f.write_str(\"<locked>\")\n                    }\n                }"],"atomic::atomic_cell::AtomicCell":["impl AtomicCell<$t> {\n            /// Increments the current value by `val` and returns the previous value.\n            ///\n            /// The addition wraps on overflow.\n            ///\n            /// # Examples\n            ///\n            /// ```\n            /// use crossbeam_utils::atomic::AtomicCell;\n            ///\n            #[doc = $example]\n            ///\n            /// assert_eq!(a.fetch_add(3), 7);\n            /// assert_eq!(a.load(), 10);\n            /// ```\n            #[inline]\n            pub fn fetch_add(&self, val: $t) -> $t {\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }\n\n            /// Decrements the current value by `val` and returns the previous value.\n            ///\n            /// The subtraction wraps on overflow.\n            ///\n            /// # Examples\n            ///\n            /// ```\n            /// use crossbeam_utils::atomic::AtomicCell;\n            ///\n            #[doc = $example]\n            ///\n            /// assert_eq!(a.fetch_sub(3), 7);\n            /// assert_eq!(a.load(), 4);\n            /// ```\n            #[inline]\n            pub fn fetch_sub(&self, val: $t) -> $t {\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }\n\n            /// Applies bitwise \"and\" to the current value and returns the previous value.\n            ///\n            /// # Examples\n            ///\n            /// ```\n            /// use crossbeam_utils::atomic::AtomicCell;\n            ///\n            #[doc = $example]\n            ///\n            /// assert_eq!(a.fetch_and(3), 7);\n            /// assert_eq!(a.load(), 3);\n            /// ```\n            #[inline]\n            pub fn fetch_and(&self, val: $t) -> $t {\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }\n\n            /// Applies bitwise \"or\" to the current value and returns the previous value.\n            ///\n            /// # Examples\n            ///\n            /// ```\n            /// use crossbeam_utils::atomic::AtomicCell;\n            ///\n            #[doc = $example]\n            ///\n            /// assert_eq!(a.fetch_or(16), 7);\n            /// assert_eq!(a.load(), 23);\n            /// ```\n            #[inline]\n            pub fn fetch_or(&self, val: $t) -> $t {\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }\n\n            /// Applies bitwise \"xor\" to the current value and returns the previous value.\n            ///\n            /// # Examples\n            ///\n            /// ```\n            /// use crossbeam_utils::atomic::AtomicCell;\n            ///\n            #[doc = $example]\n            ///\n            /// assert_eq!(a.fetch_xor(2), 7);\n            /// assert_eq!(a.load(), 5);\n            /// ```\n            #[inline]\n            pub fn fetch_xor(&self, val: $t) -> $t {\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }\n        }","impl AtomicCell<bool> {\n    /// Applies logical \"and\" to the current value and returns the previous value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(true);\n    ///\n    /// assert_eq!(a.fetch_and(true), true);\n    /// assert_eq!(a.load(), true);\n    ///\n    /// assert_eq!(a.fetch_and(false), true);\n    /// assert_eq!(a.load(), false);\n    /// ```\n    #[inline]\n    pub fn fetch_and(&self, val: bool) -> bool {\n        let a = unsafe { &*(self.value.get() as *const AtomicBool) };\n        a.fetch_and(val, Ordering::AcqRel)\n    }\n\n    /// Applies logical \"or\" to the current value and returns the previous value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(false);\n    ///\n    /// assert_eq!(a.fetch_or(false), false);\n    /// assert_eq!(a.load(), false);\n    ///\n    /// assert_eq!(a.fetch_or(true), false);\n    /// assert_eq!(a.load(), true);\n    /// ```\n    #[inline]\n    pub fn fetch_or(&self, val: bool) -> bool {\n        let a = unsafe { &*(self.value.get() as *const AtomicBool) };\n        a.fetch_or(val, Ordering::AcqRel)\n    }\n\n    /// Applies logical \"xor\" to the current value and returns the previous value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(true);\n    ///\n    /// assert_eq!(a.fetch_xor(false), true);\n    /// assert_eq!(a.load(), true);\n    ///\n    /// assert_eq!(a.fetch_xor(true), true);\n    /// assert_eq!(a.load(), false);\n    /// ```\n    #[inline]\n    pub fn fetch_xor(&self, val: bool) -> bool {\n        let a = unsafe { &*(self.value.get() as *const AtomicBool) };\n        a.fetch_xor(val, Ordering::AcqRel)\n    }\n}","impl<T: ?Sized> AtomicCell<T> {\n    /// Returns a raw pointer to the underlying data in this atomic cell.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(5);\n    ///\n    /// let ptr = a.as_ptr();\n    /// ```\n    #[inline]\n    pub fn as_ptr(&self) -> *mut T {\n        self.value.get()\n    }\n}","impl<T: Copy + Eq> AtomicCell<T> {\n    /// If the current value equals `current`, stores `new` into the atomic cell.\n    ///\n    /// The return value is always the previous value. If it is equal to `current`, then the value\n    /// was updated.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(1);\n    ///\n    /// assert_eq!(a.compare_and_swap(2, 3), 1);\n    /// assert_eq!(a.load(), 1);\n    ///\n    /// assert_eq!(a.compare_and_swap(1, 2), 1);\n    /// assert_eq!(a.load(), 2);\n    /// ```\n    pub fn compare_and_swap(&self, current: T, new: T) -> T {\n        match self.compare_exchange(current, new) {\n            Ok(v) => v,\n            Err(v) => v,\n        }\n    }\n\n    /// If the current value equals `current`, stores `new` into the atomic cell.\n    ///\n    /// The return value is a result indicating whether the new value was written and containing\n    /// the previous value. On success this value is guaranteed to be equal to `current`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(1);\n    ///\n    /// assert_eq!(a.compare_exchange(2, 3), Err(1));\n    /// assert_eq!(a.load(), 1);\n    ///\n    /// assert_eq!(a.compare_exchange(1, 2), Ok(1));\n    /// assert_eq!(a.load(), 2);\n    /// ```\n    pub fn compare_exchange(&self, current: T, new: T) -> Result<T, T> {\n        unsafe { atomic_compare_exchange_weak(self.value.get(), current, new) }\n    }\n}","impl<T: Copy + fmt::Debug> fmt::Debug for AtomicCell<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"AtomicCell\")\n            .field(\"value\", &self.load())\n            .finish()\n    }\n}","impl<T: Copy> AtomicCell<T> {\n    /// Loads a value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(7);\n    ///\n    /// assert_eq!(a.load(), 7);\n    /// ```\n    pub fn load(&self) -> T {\n        unsafe { atomic_load(self.value.get()) }\n    }\n}","impl<T: Default> AtomicCell<T> {\n    /// Takes the value of the atomic cell, leaving `Default::default()` in its place.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(5);\n    /// let five = a.take();\n    ///\n    /// assert_eq!(five, 5);\n    /// assert_eq!(a.into_inner(), 0);\n    /// ```\n    pub fn take(&self) -> T {\n        self.swap(Default::default())\n    }\n}","impl<T: Default> Default for AtomicCell<T> {\n    fn default() -> AtomicCell<T> {\n        AtomicCell::new(T::default())\n    }\n}","impl<T> AtomicCell<T> {\n    /// Creates a new atomic cell initialized with `val`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(7);\n    /// ```\n    pub const fn new(val: T) -> AtomicCell<T> {\n        AtomicCell {\n            value: UnsafeCell::new(val),\n        }\n    }\n\n    /// Unwraps the atomic cell and returns its inner value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(7);\n    /// let v = a.into_inner();\n    ///\n    /// assert_eq!(v, 7);\n    /// ```\n    pub fn into_inner(self) -> T {\n        self.value.into_inner()\n    }\n\n    /// Returns `true` if operations on values of this type are lock-free.\n    ///\n    /// If the compiler or the platform doesn't support the necessary atomic instructions,\n    /// `AtomicCell<T>` will use global locks for every potentially concurrent atomic operation.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// // This type is internally represented as `AtomicUsize` so we can just use atomic\n    /// // operations provided by it.\n    /// assert_eq!(AtomicCell::<usize>::is_lock_free(), true);\n    ///\n    /// // A wrapper struct around `isize`.\n    /// struct Foo {\n    ///     bar: isize,\n    /// }\n    /// // `AtomicCell<Foo>` will be internally represented as `AtomicIsize`.\n    /// assert_eq!(AtomicCell::<Foo>::is_lock_free(), true);\n    ///\n    /// // Operations on zero-sized types are always lock-free.\n    /// assert_eq!(AtomicCell::<()>::is_lock_free(), true);\n    ///\n    /// // Very large types cannot be represented as any of the standard atomic types, so atomic\n    /// // operations on them will have to use global locks for synchronization.\n    /// assert_eq!(AtomicCell::<[u8; 1000]>::is_lock_free(), false);\n    /// ```\n    pub fn is_lock_free() -> bool {\n        atomic_is_lock_free::<T>()\n    }\n\n    /// Stores `val` into the atomic cell.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(7);\n    ///\n    /// assert_eq!(a.load(), 7);\n    /// a.store(8);\n    /// assert_eq!(a.load(), 8);\n    /// ```\n    pub fn store(&self, val: T) {\n        if mem::needs_drop::<T>() {\n            drop(self.swap(val));\n        } else {\n            unsafe {\n                atomic_store(self.value.get(), val);\n            }\n        }\n    }\n\n    /// Stores `val` into the atomic cell and returns the previous value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::atomic::AtomicCell;\n    ///\n    /// let a = AtomicCell::new(7);\n    ///\n    /// assert_eq!(a.load(), 7);\n    /// assert_eq!(a.swap(8), 7);\n    /// assert_eq!(a.load(), 8);\n    /// ```\n    pub fn swap(&self, val: T) -> T {\n        unsafe { atomic_swap(self.value.get(), val) }\n    }\n}","impl<T> RefUnwindSafe for AtomicCell<T> {}","impl<T> UnwindSafe for AtomicCell<T> {}","unsafe impl<T: Send> Send for AtomicCell<T> {}","unsafe impl<T: Send> Sync for AtomicCell<T> {}"],"atomic::atomic_cell::AtomicUnit":["impl AtomicUnit {\n    #[inline]\n    fn load(&self, _order: Ordering) {}\n\n    #[inline]\n    fn store(&self, _val: (), _order: Ordering) {}\n\n    #[inline]\n    fn swap(&self, _val: (), _order: Ordering) {}\n\n    #[inline]\n    fn compare_exchange_weak(\n        &self,\n        _current: (),\n        _new: (),\n        _success: Ordering,\n        _failure: Ordering,\n    ) -> Result<(), ()> {\n        Ok(())\n    }\n}"],"atomic::seq_lock::SeqLock":["impl SeqLock {\n    pub const fn new() -> Self {\n        Self {\n            state: AtomicUsize::new(0),\n        }\n    }\n\n    /// If not locked, returns the current stamp.\n    ///\n    /// This method should be called before optimistic reads.\n    #[inline]\n    pub fn optimistic_read(&self) -> Option<usize> {\n        let state = self.state.load(Ordering::Acquire);\n        if state == 1 {\n            None\n        } else {\n            Some(state)\n        }\n    }\n\n    /// Returns `true` if the current stamp is equal to `stamp`.\n    ///\n    /// This method should be called after optimistic reads to check whether they are valid. The\n    /// argument `stamp` should correspond to the one returned by method `optimistic_read`.\n    #[inline]\n    pub fn validate_read(&self, stamp: usize) -> bool {\n        atomic::fence(Ordering::Acquire);\n        self.state.load(Ordering::Relaxed) == stamp\n    }\n\n    /// Grabs the lock for writing.\n    #[inline]\n    pub fn write(&'static self) -> SeqLockWriteGuard {\n        let backoff = Backoff::new();\n        loop {\n            let previous = self.state.swap(1, Ordering::Acquire);\n\n            if previous != 1 {\n                atomic::fence(Ordering::Release);\n\n                return SeqLockWriteGuard {\n                    lock: self,\n                    state: previous,\n                };\n            }\n\n            backoff.snooze();\n        }\n    }\n}"],"atomic::seq_lock::SeqLockWriteGuard":["impl Drop for SeqLockWriteGuard {\n    #[inline]\n    fn drop(&mut self) {\n        // Release the lock and increment the stamp.\n        self.lock\n            .state\n            .store(self.state.wrapping_add(2), Ordering::Release);\n    }\n}","impl SeqLockWriteGuard {\n    /// Releases the lock without incrementing the stamp.\n    #[inline]\n    pub fn abort(self) {\n        self.lock.state.store(self.state, Ordering::Release);\n\n        // We specifically don't want to call drop(), since that's\n        // what increments the stamp.\n        mem::forget(self);\n    }\n}"],"backoff::Backoff":["impl Backoff {\n    /// Creates a new `Backoff`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::Backoff;\n    ///\n    /// let backoff = Backoff::new();\n    /// ```\n    #[inline]\n    pub fn new() -> Self {\n        Backoff { step: Cell::new(0) }\n    }\n\n    /// Resets the `Backoff`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::Backoff;\n    ///\n    /// let backoff = Backoff::new();\n    /// backoff.reset();\n    /// ```\n    #[inline]\n    pub fn reset(&self) {\n        self.step.set(0);\n    }\n\n    /// Backs off in a lock-free loop.\n    ///\n    /// This method should be used when we need to retry an operation because another thread made\n    /// progress.\n    ///\n    /// The processor may yield using the *YIELD* or *PAUSE* instruction.\n    ///\n    /// # Examples\n    ///\n    /// Backing off in a lock-free loop:\n    ///\n    /// ```\n    /// use crossbeam_utils::Backoff;\n    /// use std::sync::atomic::AtomicUsize;\n    /// use std::sync::atomic::Ordering::SeqCst;\n    ///\n    /// fn fetch_mul(a: &AtomicUsize, b: usize) -> usize {\n    ///     let backoff = Backoff::new();\n    ///     loop {\n    ///         let val = a.load(SeqCst);\n    ///         if a.compare_and_swap(val, val.wrapping_mul(b), SeqCst) == val {\n    ///             return val;\n    ///         }\n    ///         backoff.spin();\n    ///     }\n    /// }\n    ///\n    /// let a = AtomicUsize::new(7);\n    /// assert_eq!(fetch_mul(&a, 8), 7);\n    /// assert_eq!(a.load(SeqCst), 56);\n    /// ```\n    #[inline]\n    pub fn spin(&self) {\n        for _ in 0..1 << self.step.get().min(SPIN_LIMIT) {\n            atomic::spin_loop_hint();\n        }\n\n        if self.step.get() <= SPIN_LIMIT {\n            self.step.set(self.step.get() + 1);\n        }\n    }\n\n    /// Backs off in a blocking loop.\n    ///\n    /// This method should be used when we need to wait for another thread to make progress.\n    ///\n    /// The processor may yield using the *YIELD* or *PAUSE* instruction and the current thread\n    /// may yield by giving up a timeslice to the OS scheduler.\n    ///\n    /// In `#[no_std]` environments, this method is equivalent to [`spin`].\n    ///\n    /// If possible, use [`is_completed`] to check when it is advised to stop using backoff and\n    /// block the current thread using a different synchronization mechanism instead.\n    ///\n    /// [`spin`]: struct.Backoff.html#method.spin\n    /// [`is_completed`]: struct.Backoff.html#method.is_completed\n    ///\n    /// # Examples\n    ///\n    /// Waiting for an [`AtomicBool`] to become `true`:\n    ///\n    /// ```\n    /// use crossbeam_utils::Backoff;\n    /// use std::sync::Arc;\n    /// use std::sync::atomic::AtomicBool;\n    /// use std::sync::atomic::Ordering::SeqCst;\n    /// use std::thread;\n    /// use std::time::Duration;\n    ///\n    /// fn spin_wait(ready: &AtomicBool) {\n    ///     let backoff = Backoff::new();\n    ///     while !ready.load(SeqCst) {\n    ///         backoff.snooze();\n    ///     }\n    /// }\n    ///\n    /// let ready = Arc::new(AtomicBool::new(false));\n    /// let ready2 = ready.clone();\n    ///\n    /// thread::spawn(move || {\n    ///     thread::sleep(Duration::from_millis(100));\n    ///     ready2.store(true, SeqCst);\n    /// });\n    ///\n    /// assert_eq!(ready.load(SeqCst), false);\n    /// spin_wait(&ready);\n    /// assert_eq!(ready.load(SeqCst), true);\n    /// ```\n    ///\n    /// [`AtomicBool`]: https://doc.rust-lang.org/std/sync/atomic/struct.AtomicBool.html\n    #[inline]\n    pub fn snooze(&self) {\n        if self.step.get() <= SPIN_LIMIT {\n            for _ in 0..1 << self.step.get() {\n                atomic::spin_loop_hint();\n            }\n        } else {\n            #[cfg(not(feature = \"std\"))]\n            for _ in 0..1 << self.step.get() {\n                atomic::spin_loop_hint();\n            }\n\n            #[cfg(feature = \"std\")]\n            ::std::thread::yield_now();\n        }\n\n        if self.step.get() <= YIELD_LIMIT {\n            self.step.set(self.step.get() + 1);\n        }\n    }\n\n    /// Returns `true` if exponential backoff has completed and blocking the thread is advised.\n    ///\n    /// # Examples\n    ///\n    /// Waiting for an [`AtomicBool`] to become `true` and parking the thread after a long wait:\n    ///\n    /// ```\n    /// use crossbeam_utils::Backoff;\n    /// use std::sync::Arc;\n    /// use std::sync::atomic::AtomicBool;\n    /// use std::sync::atomic::Ordering::SeqCst;\n    /// use std::thread;\n    /// use std::time::Duration;\n    ///\n    /// fn blocking_wait(ready: &AtomicBool) {\n    ///     let backoff = Backoff::new();\n    ///     while !ready.load(SeqCst) {\n    ///         if backoff.is_completed() {\n    ///             thread::park();\n    ///         } else {\n    ///             backoff.snooze();\n    ///         }\n    ///     }\n    /// }\n    ///\n    /// let ready = Arc::new(AtomicBool::new(false));\n    /// let ready2 = ready.clone();\n    /// let waiter = thread::current();\n    ///\n    /// thread::spawn(move || {\n    ///     thread::sleep(Duration::from_millis(100));\n    ///     ready2.store(true, SeqCst);\n    ///     waiter.unpark();\n    /// });\n    ///\n    /// assert_eq!(ready.load(SeqCst), false);\n    /// blocking_wait(&ready);\n    /// assert_eq!(ready.load(SeqCst), true);\n    /// ```\n    ///\n    /// [`AtomicBool`]: https://doc.rust-lang.org/std/sync/atomic/struct.AtomicBool.html\n    #[inline]\n    pub fn is_completed(&self) -> bool {\n        self.step.get() > YIELD_LIMIT\n    }\n}","impl Default for Backoff {\n    fn default() -> Backoff {\n        Backoff::new()\n    }\n}","impl fmt::Debug for Backoff {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"Backoff\")\n            .field(\"step\", &self.step)\n            .field(\"is_completed\", &self.is_completed())\n            .finish()\n    }\n}"],"cache_padded::CachePadded":["Clone","Copy","Default","Eq","Hash","PartialEq","impl<T: fmt::Debug> fmt::Debug for CachePadded<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"CachePadded\")\n            .field(\"value\", &self.value)\n            .finish()\n    }\n}","impl<T> CachePadded<T> {\n    /// Pads and aligns a value to the length of a cache line.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::CachePadded;\n    ///\n    /// let padded_value = CachePadded::new(1);\n    /// ```\n    pub const fn new(t: T) -> CachePadded<T> {\n        CachePadded::<T> { value: t }\n    }\n\n    /// Returns the inner value.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::CachePadded;\n    ///\n    /// let padded_value = CachePadded::new(7);\n    /// let value = padded_value.into_inner();\n    /// assert_eq!(value, 7);\n    /// ```\n    pub fn into_inner(self) -> T {\n        self.value\n    }\n}","impl<T> Deref for CachePadded<T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        &self.value\n    }\n}","impl<T> DerefMut for CachePadded<T> {\n    fn deref_mut(&mut self) -> &mut T {\n        &mut self.value\n    }\n}","impl<T> From<T> for CachePadded<T> {\n    fn from(t: T) -> Self {\n        CachePadded::new(t)\n    }\n}","unsafe impl<T: Send> Send for CachePadded<T> {}","unsafe impl<T: Sync> Sync for CachePadded<T> {}"],"std::sync::atomic::AtomicBool":["impl AtomicConsume for ::core::sync::atomic::$atomic {\n            type Val = $val;\n            impl_consume!();\n        }"],"std::sync::atomic::AtomicIsize":["impl AtomicConsume for ::core::sync::atomic::$atomic {\n            type Val = $val;\n            impl_consume!();\n        }"],"std::sync::atomic::AtomicPtr":["impl<T> AtomicConsume for ::core::sync::atomic::AtomicPtr<T> {\n    type Val = *mut T;\n    impl_consume!();\n}"],"std::sync::atomic::AtomicUsize":["impl AtomicConsume for ::core::sync::atomic::$atomic {\n            type Val = $val;\n            impl_consume!();\n        }"],"sync::parker::Inner":["impl Inner {\n    fn park(&self, timeout: Option<Duration>) {\n        // If we were previously notified then we consume this notification and return quickly.\n        if self\n            .state\n            .compare_exchange(NOTIFIED, EMPTY, SeqCst, SeqCst)\n            .is_ok()\n        {\n            return;\n        }\n\n        // If the timeout is zero, then there is no need to actually block.\n        if let Some(ref dur) = timeout {\n            if *dur == Duration::from_millis(0) {\n                return;\n            }\n        }\n\n        // Otherwise we need to coordinate going to sleep.\n        let mut m = self.lock.lock().unwrap();\n\n        match self.state.compare_exchange(EMPTY, PARKED, SeqCst, SeqCst) {\n            Ok(_) => {}\n            // Consume this notification to avoid spurious wakeups in the next park.\n            Err(NOTIFIED) => {\n                // We must read `state` here, even though we know it will be `NOTIFIED`. This is\n                // because `unpark` may have been called again since we read `NOTIFIED` in the\n                // `compare_exchange` above. We must perform an acquire operation that synchronizes\n                // with that `unpark` to observe any writes it made before the call to `unpark`. To\n                // do that we must read from the write it made to `state`.\n                let old = self.state.swap(EMPTY, SeqCst);\n                assert_eq!(old, NOTIFIED, \"park state changed unexpectedly\");\n                return;\n            }\n            Err(n) => panic!(\"inconsistent park_timeout state: {}\", n),\n        }\n\n        match timeout {\n            None => {\n                loop {\n                    // Block the current thread on the conditional variable.\n                    m = self.cvar.wait(m).unwrap();\n\n                    if self\n                        .state\n                        .compare_exchange(NOTIFIED, EMPTY, SeqCst, SeqCst)\n                        .is_ok()\n                    {\n                        // got a notification\n                        return;\n                    }\n\n                    // spurious wakeup, go back to sleep\n                }\n            }\n            Some(timeout) => {\n                // Wait with a timeout, and if we spuriously wake up or otherwise wake up from a\n                // notification we just want to unconditionally set `state` back to `EMPTY`, either\n                // consuming a notification or un-flagging ourselves as parked.\n                let (_m, _result) = self.cvar.wait_timeout(m, timeout).unwrap();\n\n                match self.state.swap(EMPTY, SeqCst) {\n                    NOTIFIED => {} // got a notification\n                    PARKED => {}   // no notification\n                    n => panic!(\"inconsistent park_timeout state: {}\", n),\n                }\n            }\n        }\n    }\n\n    pub fn unpark(&self) {\n        // To ensure the unparked thread will observe any writes we made before this call, we must\n        // perform a release operation that `park` can synchronize with. To do that we must write\n        // `NOTIFIED` even if `state` is already `NOTIFIED`. That is why this must be a swap rather\n        // than a compare-and-swap that returns if it reads `NOTIFIED` on failure.\n        match self.state.swap(NOTIFIED, SeqCst) {\n            EMPTY => return,    // no one was waiting\n            NOTIFIED => return, // already unparked\n            PARKED => {}        // gotta go wake someone up\n            _ => panic!(\"inconsistent state in unpark\"),\n        }\n\n        // There is a period between when the parked thread sets `state` to `PARKED` (or last\n        // checked `state` in the case of a spurious wakeup) and when it actually waits on `cvar`.\n        // If we were to notify during this period it would be ignored and then when the parked\n        // thread went to sleep it would never wake up. Fortunately, it has `lock` locked at this\n        // stage so we can acquire `lock` to wait until it is ready to receive the notification.\n        //\n        // Releasing `lock` before the call to `notify_one` means that when the parked thread wakes\n        // it doesn't get woken only to have to wait for us to release `lock`.\n        drop(self.lock.lock().unwrap());\n        self.cvar.notify_one();\n    }\n}"],"sync::parker::Parker":["impl Default for Parker {\n    fn default() -> Self {\n        Self {\n            unparker: Unparker {\n                inner: Arc::new(Inner {\n                    state: AtomicUsize::new(EMPTY),\n                    lock: Mutex::new(()),\n                    cvar: Condvar::new(),\n                }),\n            },\n            _marker: PhantomData,\n        }\n    }\n}","impl Parker {\n    /// Creates a new `Parker`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    /// ```\n    ///\n    pub fn new() -> Parker {\n        Self::default()\n    }\n\n    /// Blocks the current thread until the token is made available.\n    ///\n    /// A call to `park` may wake up spuriously without consuming the token, and callers should be\n    /// prepared for this possibility.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    /// let u = p.unparker().clone();\n    ///\n    /// // Make the token available.\n    /// u.unpark();\n    ///\n    /// // Wakes up immediately and consumes the token.\n    /// p.park();\n    /// ```\n    pub fn park(&self) {\n        self.unparker.inner.park(None);\n    }\n\n    /// Blocks the current thread until the token is made available, but only for a limited time.\n    ///\n    /// A call to `park_timeout` may wake up spuriously without consuming the token, and callers\n    /// should be prepared for this possibility.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use std::time::Duration;\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    ///\n    /// // Waits for the token to become available, but will not wait longer than 500 ms.\n    /// p.park_timeout(Duration::from_millis(500));\n    /// ```\n    pub fn park_timeout(&self, timeout: Duration) {\n        self.unparker.inner.park(Some(timeout));\n    }\n\n    /// Returns a reference to an associated [`Unparker`].\n    ///\n    /// The returned [`Unparker`] doesn't have to be used by reference - it can also be cloned.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    /// let u = p.unparker().clone();\n    ///\n    /// // Make the token available.\n    /// u.unpark();\n    /// // Wakes up immediately and consumes the token.\n    /// p.park();\n    /// ```\n    ///\n    /// [`park`]: struct.Parker.html#method.park\n    /// [`park_timeout`]: struct.Parker.html#method.park_timeout\n    ///\n    /// [`Unparker`]: struct.Unparker.html\n    pub fn unparker(&self) -> &Unparker {\n        &self.unparker\n    }\n\n    /// Converts a `Parker` into a raw pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    /// let raw = Parker::into_raw(p);\n    /// ```\n    pub fn into_raw(this: Parker) -> *const () {\n        Unparker::into_raw(this.unparker)\n    }\n\n    /// Converts a raw pointer into a `Parker`.\n    ///\n    /// # Safety\n    ///\n    /// This method is safe to use only with pointers returned by [`Parker::into_raw`].\n    ///\n    /// [`Parker::into_raw`]: struct.Parker.html#method.into_raw\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    /// let raw = Parker::into_raw(p);\n    /// let p = unsafe { Parker::from_raw(raw) };\n    /// ```\n    pub unsafe fn from_raw(ptr: *const ()) -> Parker {\n        Parker {\n            unparker: Unparker::from_raw(ptr),\n            _marker: PhantomData,\n        }\n    }\n}","impl fmt::Debug for Parker {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Parker { .. }\")\n    }\n}","unsafe impl Send for Parker {}"],"sync::parker::Unparker":["impl Clone for Unparker {\n    fn clone(&self) -> Unparker {\n        Unparker {\n            inner: self.inner.clone(),\n        }\n    }\n}","impl Unparker {\n    /// Atomically makes the token available if it is not already.\n    ///\n    /// This method will wake up the thread blocked on [`park`] or [`park_timeout`], if there is\n    /// any.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use std::thread;\n    /// use std::time::Duration;\n    /// use crossbeam_utils::sync::Parker;\n    ///\n    /// let p = Parker::new();\n    /// let u = p.unparker().clone();\n    ///\n    /// thread::spawn(move || {\n    ///     thread::sleep(Duration::from_millis(500));\n    ///     u.unpark();\n    /// });\n    ///\n    /// // Wakes up when `u.unpark()` provides the token, but may also wake up\n    /// // spuriously before that without consuming the token.\n    /// p.park();\n    /// ```\n    ///\n    /// [`park`]: struct.Parker.html#method.park\n    /// [`park_timeout`]: struct.Parker.html#method.park_timeout\n    pub fn unpark(&self) {\n        self.inner.unpark()\n    }\n\n    /// Converts an `Unparker` into a raw pointer.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::{Parker, Unparker};\n    ///\n    /// let p = Parker::new();\n    /// let u = p.unparker().clone();\n    /// let raw = Unparker::into_raw(u);\n    /// ```\n    pub fn into_raw(this: Unparker) -> *const () {\n        Arc::into_raw(this.inner) as *const ()\n    }\n\n    /// Converts a raw pointer into an `Unparker`.\n    ///\n    /// # Safety\n    ///\n    /// This method is safe to use only with pointers returned by [`Unparker::into_raw`].\n    ///\n    /// [`Unparker::into_raw`]: struct.Unparker.html#method.into_raw\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::{Parker, Unparker};\n    ///\n    /// let p = Parker::new();\n    /// let u = p.unparker().clone();\n    ///\n    /// let raw = Unparker::into_raw(u);\n    /// let u = unsafe { Unparker::from_raw(raw) };\n    /// ```\n    pub unsafe fn from_raw(ptr: *const ()) -> Unparker {\n        Unparker {\n            inner: Arc::from_raw(ptr as *const Inner),\n        }\n    }\n}","impl fmt::Debug for Unparker {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Unparker { .. }\")\n    }\n}","unsafe impl Send for Unparker {}","unsafe impl Sync for Unparker {}"],"sync::sharded_lock::Registration":["impl Drop for Registration {\n    fn drop(&mut self) {\n        let mut indices = THREAD_INDICES.lock().unwrap();\n        indices.mapping.remove(&self.thread_id);\n        indices.free_list.push(self.index);\n    }\n}"],"sync::sharded_lock::ShardedLock":["impl<T: ?Sized + fmt::Debug> fmt::Debug for ShardedLock<T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self.try_read() {\n            Ok(guard) => f\n                .debug_struct(\"ShardedLock\")\n                .field(\"data\", &&*guard)\n                .finish(),\n            Err(TryLockError::Poisoned(err)) => f\n                .debug_struct(\"ShardedLock\")\n                .field(\"data\", &&**err.get_ref())\n                .finish(),\n            Err(TryLockError::WouldBlock) => {\n                struct LockedPlaceholder;\n                impl fmt::Debug for LockedPlaceholder {\n                    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n                        f.write_str(\"<locked>\")\n                    }\n                }\n                f.debug_struct(\"ShardedLock\")\n                    .field(\"data\", &LockedPlaceholder)\n                    .finish()\n            }\n        }\n    }\n}","impl<T: ?Sized> RefUnwindSafe for ShardedLock<T> {}","impl<T: ?Sized> ShardedLock<T> {\n    /// Returns `true` if the lock is poisoned.\n    ///\n    /// If another thread can still access the lock, it may become poisoned at any time. A `false`\n    /// result should not be trusted without additional synchronization.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    /// use std::sync::Arc;\n    /// use std::thread;\n    ///\n    /// let lock = Arc::new(ShardedLock::new(0));\n    /// let c_lock = lock.clone();\n    ///\n    /// let _ = thread::spawn(move || {\n    ///     let _lock = c_lock.write().unwrap();\n    ///     panic!(); // the lock gets poisoned\n    /// }).join();\n    /// assert_eq!(lock.is_poisoned(), true);\n    /// ```\n    pub fn is_poisoned(&self) -> bool {\n        self.shards[0].lock.is_poisoned()\n    }\n\n    /// Returns a mutable reference to the underlying data.\n    ///\n    /// Since this call borrows the lock mutably, no actual locking needs to take place.\n    ///\n    /// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n    /// operation panics.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    ///\n    /// let mut lock = ShardedLock::new(0);\n    /// *lock.get_mut().unwrap() = 10;\n    /// assert_eq!(*lock.read().unwrap(), 10);\n    /// ```\n    pub fn get_mut(&mut self) -> LockResult<&mut T> {\n        let is_poisoned = self.is_poisoned();\n        let inner = unsafe { &mut *self.value.get() };\n\n        if is_poisoned {\n            Err(PoisonError::new(inner))\n        } else {\n            Ok(inner)\n        }\n    }\n\n    /// Attempts to acquire this lock with shared read access.\n    ///\n    /// If the access could not be granted at this time, an error is returned. Otherwise, a guard\n    /// is returned which will release the shared access when it is dropped. This method does not\n    /// provide any guarantees with respect to the ordering of whether contentious readers or\n    /// writers will acquire the lock first.\n    ///\n    /// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n    /// operation panics.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    ///\n    /// let lock = ShardedLock::new(1);\n    ///\n    /// match lock.try_read() {\n    ///     Ok(n) => assert_eq!(*n, 1),\n    ///     Err(_) => unreachable!(),\n    /// };\n    /// ```\n    pub fn try_read(&self) -> TryLockResult<ShardedLockReadGuard<'_, T>> {\n        // Take the current thread index and map it to a shard index. Thread indices will tend to\n        // distribute shards among threads equally, thus reducing contention due to read-locking.\n        let current_index = current_index().unwrap_or(0);\n        let shard_index = current_index & (self.shards.len() - 1);\n\n        match self.shards[shard_index].lock.try_read() {\n            Ok(guard) => Ok(ShardedLockReadGuard {\n                lock: self,\n                _guard: guard,\n                _marker: PhantomData,\n            }),\n            Err(TryLockError::Poisoned(err)) => {\n                let guard = ShardedLockReadGuard {\n                    lock: self,\n                    _guard: err.into_inner(),\n                    _marker: PhantomData,\n                };\n                Err(TryLockError::Poisoned(PoisonError::new(guard)))\n            }\n            Err(TryLockError::WouldBlock) => Err(TryLockError::WouldBlock),\n        }\n    }\n\n    /// Locks with shared read access, blocking the current thread until it can be acquired.\n    ///\n    /// The calling thread will be blocked until there are no more writers which hold the lock.\n    /// There may be other readers currently inside the lock when this method returns. This method\n    /// does not provide any guarantees with respect to the ordering of whether contentious readers\n    /// or writers will acquire the lock first.\n    ///\n    /// Returns a guard which will release the shared access when dropped.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    /// use std::sync::Arc;\n    /// use std::thread;\n    ///\n    /// let lock = Arc::new(ShardedLock::new(1));\n    /// let c_lock = lock.clone();\n    ///\n    /// let n = lock.read().unwrap();\n    /// assert_eq!(*n, 1);\n    ///\n    /// thread::spawn(move || {\n    ///     let r = c_lock.read();\n    ///     assert!(r.is_ok());\n    /// }).join().unwrap();\n    /// ```\n    pub fn read(&self) -> LockResult<ShardedLockReadGuard<'_, T>> {\n        // Take the current thread index and map it to a shard index. Thread indices will tend to\n        // distribute shards among threads equally, thus reducing contention due to read-locking.\n        let current_index = current_index().unwrap_or(0);\n        let shard_index = current_index & (self.shards.len() - 1);\n\n        match self.shards[shard_index].lock.read() {\n            Ok(guard) => Ok(ShardedLockReadGuard {\n                lock: self,\n                _guard: guard,\n                _marker: PhantomData,\n            }),\n            Err(err) => Err(PoisonError::new(ShardedLockReadGuard {\n                lock: self,\n                _guard: err.into_inner(),\n                _marker: PhantomData,\n            })),\n        }\n    }\n\n    /// Attempts to acquire this lock with exclusive write access.\n    ///\n    /// If the access could not be granted at this time, an error is returned. Otherwise, a guard\n    /// is returned which will release the exclusive access when it is dropped. This method does\n    /// not provide any guarantees with respect to the ordering of whether contentious readers or\n    /// writers will acquire the lock first.\n    ///\n    /// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n    /// operation panics.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    ///\n    /// let lock = ShardedLock::new(1);\n    ///\n    /// let n = lock.read().unwrap();\n    /// assert_eq!(*n, 1);\n    ///\n    /// assert!(lock.try_write().is_err());\n    /// ```\n    pub fn try_write(&self) -> TryLockResult<ShardedLockWriteGuard<'_, T>> {\n        let mut poisoned = false;\n        let mut blocked = None;\n\n        // Write-lock each shard in succession.\n        for (i, shard) in self.shards.iter().enumerate() {\n            let guard = match shard.lock.try_write() {\n                Ok(guard) => guard,\n                Err(TryLockError::Poisoned(err)) => {\n                    poisoned = true;\n                    err.into_inner()\n                }\n                Err(TryLockError::WouldBlock) => {\n                    blocked = Some(i);\n                    break;\n                }\n            };\n\n            // Store the guard into the shard.\n            unsafe {\n                let guard: RwLockWriteGuard<'static, ()> = mem::transmute(guard);\n                let dest: *mut _ = shard.write_guard.get();\n                *dest = Some(guard);\n            }\n        }\n\n        if let Some(i) = blocked {\n            // Unlock the shards in reverse order of locking.\n            for shard in self.shards[0..i].iter().rev() {\n                unsafe {\n                    let dest: *mut _ = shard.write_guard.get();\n                    let guard = mem::replace(&mut *dest, None);\n                    drop(guard);\n                }\n            }\n            Err(TryLockError::WouldBlock)\n        } else if poisoned {\n            let guard = ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            };\n            Err(TryLockError::Poisoned(PoisonError::new(guard)))\n        } else {\n            Ok(ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            })\n        }\n    }\n\n    /// Locks with exclusive write access, blocking the current thread until it can be acquired.\n    ///\n    /// The calling thread will be blocked until there are no more writers which hold the lock.\n    /// There may be other readers currently inside the lock when this method returns. This method\n    /// does not provide any guarantees with respect to the ordering of whether contentious readers\n    /// or writers will acquire the lock first.\n    ///\n    /// Returns a guard which will release the exclusive access when dropped.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    ///\n    /// let lock = ShardedLock::new(1);\n    ///\n    /// let mut n = lock.write().unwrap();\n    /// *n = 2;\n    ///\n    /// assert!(lock.try_read().is_err());\n    /// ```\n    pub fn write(&self) -> LockResult<ShardedLockWriteGuard<'_, T>> {\n        let mut poisoned = false;\n\n        // Write-lock each shard in succession.\n        for shard in self.shards.iter() {\n            let guard = match shard.lock.write() {\n                Ok(guard) => guard,\n                Err(err) => {\n                    poisoned = true;\n                    err.into_inner()\n                }\n            };\n\n            // Store the guard into the shard.\n            unsafe {\n                let guard: RwLockWriteGuard<'_, ()> = guard;\n                let guard: RwLockWriteGuard<'static, ()> = mem::transmute(guard);\n                let dest: *mut _ = shard.write_guard.get();\n                *dest = Some(guard);\n            }\n        }\n\n        if poisoned {\n            Err(PoisonError::new(ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            }))\n        } else {\n            Ok(ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            })\n        }\n    }\n}","impl<T: ?Sized> UnwindSafe for ShardedLock<T> {}","impl<T: Default> Default for ShardedLock<T> {\n    fn default() -> ShardedLock<T> {\n        ShardedLock::new(Default::default())\n    }\n}","impl<T> From<T> for ShardedLock<T> {\n    fn from(t: T) -> Self {\n        ShardedLock::new(t)\n    }\n}","impl<T> ShardedLock<T> {\n    /// Creates a new sharded reader-writer lock.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    ///\n    /// let lock = ShardedLock::new(5);\n    /// ```\n    pub fn new(value: T) -> ShardedLock<T> {\n        ShardedLock {\n            shards: (0..NUM_SHARDS)\n                .map(|_| {\n                    CachePadded::new(Shard {\n                        lock: RwLock::new(()),\n                        write_guard: UnsafeCell::new(None),\n                    })\n                })\n                .collect::<Vec<_>>()\n                .into_boxed_slice(),\n            value: UnsafeCell::new(value),\n        }\n    }\n\n    /// Consumes this lock, returning the underlying data.\n    ///\n    /// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n    /// operation panics.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::ShardedLock;\n    ///\n    /// let lock = ShardedLock::new(String::new());\n    /// {\n    ///     let mut s = lock.write().unwrap();\n    ///     *s = \"modified\".to_owned();\n    /// }\n    /// assert_eq!(lock.into_inner().unwrap(), \"modified\");\n    /// ```\n    pub fn into_inner(self) -> LockResult<T> {\n        let is_poisoned = self.is_poisoned();\n        let inner = self.value.into_inner();\n\n        if is_poisoned {\n            Err(PoisonError::new(inner))\n        } else {\n            Ok(inner)\n        }\n    }\n}","unsafe impl<T: ?Sized + Send + Sync> Sync for ShardedLock<T> {}","unsafe impl<T: ?Sized + Send> Send for ShardedLock<T> {}"],"sync::sharded_lock::ShardedLockReadGuard":["impl<T: ?Sized + fmt::Display> fmt::Display for ShardedLockReadGuard<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        (**self).fmt(f)\n    }\n}","impl<T: ?Sized> Deref for ShardedLockReadGuard<'_, T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        unsafe { &*self.lock.value.get() }\n    }\n}","impl<T: fmt::Debug> fmt::Debug for ShardedLockReadGuard<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"ShardedLockReadGuard\")\n            .field(\"lock\", &self.lock)\n            .finish()\n    }\n}","unsafe impl<T: ?Sized + Sync> Sync for ShardedLockReadGuard<'_, T> {}"],"sync::sharded_lock::ShardedLockWriteGuard":["impl<T: ?Sized + fmt::Display> fmt::Display for ShardedLockWriteGuard<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        (**self).fmt(f)\n    }\n}","impl<T: ?Sized> Deref for ShardedLockWriteGuard<'_, T> {\n    type Target = T;\n\n    fn deref(&self) -> &T {\n        unsafe { &*self.lock.value.get() }\n    }\n}","impl<T: ?Sized> DerefMut for ShardedLockWriteGuard<'_, T> {\n    fn deref_mut(&mut self) -> &mut T {\n        unsafe { &mut *self.lock.value.get() }\n    }\n}","impl<T: ?Sized> Drop for ShardedLockWriteGuard<'_, T> {\n    fn drop(&mut self) {\n        // Unlock the shards in reverse order of locking.\n        for shard in self.lock.shards.iter().rev() {\n            unsafe {\n                let dest: *mut _ = shard.write_guard.get();\n                let guard = mem::replace(&mut *dest, None);\n                drop(guard);\n            }\n        }\n    }\n}","impl<T: fmt::Debug> fmt::Debug for ShardedLockWriteGuard<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.debug_struct(\"ShardedLockWriteGuard\")\n            .field(\"lock\", &self.lock)\n            .finish()\n    }\n}","unsafe impl<T: ?Sized + Sync> Sync for ShardedLockWriteGuard<'_, T> {}"],"sync::sharded_lock::THREAD_INDICES":["impl $crate::LazyStatic for $N {\n            fn initialize(lazy: &Self) {\n                let _ = &**lazy;\n            }\n        }","impl $crate::__Deref for $N {\n            type Target = $T;\n            fn deref(&self) -> &$T {\n                #[inline(always)]\n                fn __static_ref_initialize() -> $T { $e }\n\n                #[inline(always)]\n                fn __stability() -> &'static $T {\n                    __lazy_static_create!(LAZY, $T);\n                    LAZY.get(__static_ref_initialize)\n                }\n                __stability()\n            }\n        }"],"sync::wait_group::WaitGroup":["impl Clone for WaitGroup {\n    fn clone(&self) -> WaitGroup {\n        let mut count = self.inner.count.lock().unwrap();\n        *count += 1;\n\n        WaitGroup {\n            inner: self.inner.clone(),\n        }\n    }\n}","impl Default for WaitGroup {\n    fn default() -> Self {\n        Self {\n            inner: Arc::new(Inner {\n                cvar: Condvar::new(),\n                count: Mutex::new(1),\n            }),\n        }\n    }\n}","impl Drop for WaitGroup {\n    fn drop(&mut self) {\n        let mut count = self.inner.count.lock().unwrap();\n        *count -= 1;\n\n        if *count == 0 {\n            self.inner.cvar.notify_all();\n        }\n    }\n}","impl WaitGroup {\n    /// Creates a new wait group and returns the single reference to it.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::WaitGroup;\n    ///\n    /// let wg = WaitGroup::new();\n    /// ```\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Drops this reference and waits until all other references are dropped.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::sync::WaitGroup;\n    /// use std::thread;\n    ///\n    /// let wg = WaitGroup::new();\n    ///\n    /// thread::spawn({\n    ///     let wg = wg.clone();\n    ///     move || {\n    ///         // Block until both threads have reached `wait()`.\n    ///         wg.wait();\n    ///     }\n    /// });\n    ///\n    /// // Block until both threads have reached `wait()`.\n    /// wg.wait();\n    /// ```\n    pub fn wait(self) {\n        if *self.inner.count.lock().unwrap() == 1 {\n            return;\n        }\n\n        let inner = self.inner.clone();\n        drop(self);\n\n        let mut count = inner.count.lock().unwrap();\n        while *count > 0 {\n            count = inner.cvar.wait(count).unwrap();\n        }\n    }\n}","impl fmt::Debug for WaitGroup {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        let count: &usize = &*self.inner.count.lock().unwrap();\n        f.debug_struct(\"WaitGroup\").field(\"count\", count).finish()\n    }\n}"],"thread::Scope":["impl fmt::Debug for Scope<'_> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"Scope { .. }\")\n    }\n}","impl<'env> Scope<'env> {\n    /// Spawns a scoped thread.\n    ///\n    /// This method is similar to the [`spawn`] function in Rust's standard library. The difference\n    /// is that this thread is scoped, meaning it's guaranteed to terminate before the scope exits,\n    /// allowing it to reference variables outside the scope.\n    ///\n    /// The scoped thread is passed a reference to this scope as an argument, which can be used for\n    /// spawning nested threads.\n    ///\n    /// The returned handle can be used to manually join the thread before the scope exits.\n    ///\n    /// [`spawn`]: https://doc.rust-lang.org/std/thread/fn.spawn.html\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    ///\n    /// thread::scope(|s| {\n    ///     let handle = s.spawn(|_| {\n    ///         println!(\"A child thread is running\");\n    ///         42\n    ///     });\n    ///\n    ///     // Join the thread and retrieve its result.\n    ///     let res = handle.join().unwrap();\n    ///     assert_eq!(res, 42);\n    /// }).unwrap();\n    /// ```\n    pub fn spawn<'scope, F, T>(&'scope self, f: F) -> ScopedJoinHandle<'scope, T>\n    where\n        F: FnOnce(&Scope<'env>) -> T,\n        F: Send + 'env,\n        T: Send + 'env,\n    {\n        self.builder().spawn(f).unwrap()\n    }\n\n    /// Creates a builder that can configure a thread before spawning.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    ///\n    /// thread::scope(|s| {\n    ///     s.builder()\n    ///         .spawn(|_| println!(\"A child thread is running\"))\n    ///         .unwrap();\n    /// }).unwrap();\n    /// ```\n    pub fn builder<'scope>(&'scope self) -> ScopedThreadBuilder<'scope, 'env> {\n        ScopedThreadBuilder {\n            scope: self,\n            builder: thread::Builder::new(),\n        }\n    }\n}","unsafe impl Sync for Scope<'_> {}"],"thread::ScopedJoinHandle":["impl<T> JoinHandleExt for ScopedJoinHandle<'_, T> {\n            fn as_pthread_t(&self) -> RawPthread {\n                // Borrow the handle. The handle will surely be available because the root scope waits\n                // for nested scopes before joining remaining threads.\n                let handle = self.handle.lock().unwrap();\n                handle.as_ref().unwrap().as_pthread_t()\n            }\n            fn into_pthread_t(self) -> RawPthread {\n                self.as_pthread_t()\n            }\n        }","impl<T> ScopedJoinHandle<'_, T> {\n    /// Waits for the thread to finish and returns its result.\n    ///\n    /// If the child thread panics, an error is returned.\n    ///\n    /// # Panics\n    ///\n    /// This function may panic on some platforms if a thread attempts to join itself or otherwise\n    /// may create a deadlock with joining threads.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    ///\n    /// thread::scope(|s| {\n    ///     let handle1 = s.spawn(|_| println!(\"I'm a happy thread :)\"));\n    ///     let handle2 = s.spawn(|_| panic!(\"I'm a sad thread :(\"));\n    ///\n    ///     // Join the first thread and verify that it succeeded.\n    ///     let res = handle1.join();\n    ///     assert!(res.is_ok());\n    ///\n    ///     // Join the second thread and verify that it panicked.\n    ///     let res = handle2.join();\n    ///     assert!(res.is_err());\n    /// }).unwrap();\n    /// ```\n    pub fn join(self) -> thread::Result<T> {\n        // Take out the handle. The handle will surely be available because the root scope waits\n        // for nested scopes before joining remaining threads.\n        let handle = self.handle.lock().unwrap().take().unwrap();\n\n        // Join the thread and then take the result out of its inner closure.\n        handle\n            .join()\n            .map(|()| self.result.lock().unwrap().take().unwrap())\n    }\n\n    /// Returns a handle to the underlying thread.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    ///\n    /// thread::scope(|s| {\n    ///     let handle = s.spawn(|_| println!(\"A child thread is running\"));\n    ///     println!(\"The child thread ID: {:?}\", handle.thread().id());\n    /// }).unwrap();\n    /// ```\n    pub fn thread(&self) -> &thread::Thread {\n        &self.thread\n    }\n}","impl<T> fmt::Debug for ScopedJoinHandle<'_, T> {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        f.pad(\"ScopedJoinHandle { .. }\")\n    }\n}","unsafe impl<T> Send for ScopedJoinHandle<'_, T> {}","unsafe impl<T> Sync for ScopedJoinHandle<'_, T> {}"],"thread::ScopedThreadBuilder":["Debug","impl<'scope, 'env> ScopedThreadBuilder<'scope, 'env> {\n    /// Sets the name for the new thread.\n    ///\n    /// The name must not contain null bytes. For more information about named threads, see\n    /// [here][naming-threads].\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    /// use std::thread::current;\n    ///\n    /// thread::scope(|s| {\n    ///     s.builder()\n    ///         .name(\"my thread\".to_string())\n    ///         .spawn(|_| assert_eq!(current().name(), Some(\"my thread\")))\n    ///         .unwrap();\n    /// }).unwrap();\n    /// ```\n    ///\n    /// [naming-threads]: https://doc.rust-lang.org/std/thread/index.html#naming-threads\n    pub fn name(mut self, name: String) -> ScopedThreadBuilder<'scope, 'env> {\n        self.builder = self.builder.name(name);\n        self\n    }\n\n    /// Sets the size of the stack for the new thread.\n    ///\n    /// The stack size is measured in bytes.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    ///\n    /// thread::scope(|s| {\n    ///     s.builder()\n    ///         .stack_size(32 * 1024)\n    ///         .spawn(|_| println!(\"Running a child thread\"))\n    ///         .unwrap();\n    /// }).unwrap();\n    /// ```\n    pub fn stack_size(mut self, size: usize) -> ScopedThreadBuilder<'scope, 'env> {\n        self.builder = self.builder.stack_size(size);\n        self\n    }\n\n    /// Spawns a scoped thread with this configuration.\n    ///\n    /// The scoped thread is passed a reference to this scope as an argument, which can be used for\n    /// spawning nested threads.\n    ///\n    /// The returned handle can be used to manually join the thread before the scope exits.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use crossbeam_utils::thread;\n    ///\n    /// thread::scope(|s| {\n    ///     let handle = s.builder()\n    ///         .spawn(|_| {\n    ///             println!(\"A child thread is running\");\n    ///             42\n    ///         })\n    ///         .unwrap();\n    ///\n    ///     // Join the thread and retrieve its result.\n    ///     let res = handle.join().unwrap();\n    ///     assert_eq!(res, 42);\n    /// }).unwrap();\n    /// ```\n    pub fn spawn<F, T>(self, f: F) -> io::Result<ScopedJoinHandle<'scope, T>>\n    where\n        F: FnOnce(&Scope<'env>) -> T,\n        F: Send + 'env,\n        T: Send + 'env,\n    {\n        // The result of `f` will be stored here.\n        let result = SharedOption::default();\n\n        // Spawn the thread and grab its join handle and thread handle.\n        let (handle, thread) = {\n            let result = Arc::clone(&result);\n\n            // A clone of the scope that will be moved into the new thread.\n            let scope = Scope::<'env> {\n                handles: Arc::clone(&self.scope.handles),\n                wait_group: self.scope.wait_group.clone(),\n                _marker: PhantomData,\n            };\n\n            // Spawn the thread.\n            let handle = {\n                let closure = move || {\n                    // Make sure the scope is inside the closure with the proper `'env` lifetime.\n                    let scope: Scope<'env> = scope;\n\n                    // Run the closure.\n                    let res = f(&scope);\n\n                    // Store the result if the closure didn't panic.\n                    *result.lock().unwrap() = Some(res);\n                };\n\n                // Allocate `clsoure` on the heap and erase the `'env` bound.\n                let closure: Box<dyn FnOnce() + Send + 'env> = Box::new(closure);\n                let closure: Box<dyn FnOnce() + Send + 'static> =\n                    unsafe { mem::transmute(closure) };\n\n                // Finally, spawn the closure.\n                self.builder.spawn(move || closure())?\n            };\n\n            let thread = handle.thread().clone();\n            let handle = Arc::new(Mutex::new(Some(handle)));\n            (handle, thread)\n        };\n\n        // Add the handle to the shared list of join handles.\n        self.scope.handles.lock().unwrap().push(Arc::clone(&handle));\n\n        Ok(ScopedJoinHandle {\n            handle,\n            result,\n            thread,\n            _marker: PhantomData,\n        })\n    }\n}"]},"single_path_import":{"atomic::atomic_cell::AtomicCell":"atomic::AtomicCell","atomic::consume::AtomicConsume":"atomic::AtomicConsume","backoff::Backoff":"Backoff","cache_padded::CachePadded":"CachePadded","sync::parker::Parker":"sync::Parker","sync::parker::Unparker":"sync::Unparker","sync::sharded_lock::ShardedLock":"sync::ShardedLock","sync::sharded_lock::ShardedLockReadGuard":"sync::ShardedLockReadGuard","sync::sharded_lock::ShardedLockWriteGuard":"sync::ShardedLockWriteGuard","sync::wait_group::WaitGroup":"sync::WaitGroup"},"srcs":{"<<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n                        f.write_str(\"<locked>\")\n                    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<atomic::atomic_cell::AtomicCell<T> as std::default::Default>::default":["fn default() -> AtomicCell<T>{\n        AtomicCell::new(T::default())\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"<atomic::atomic_cell::AtomicCell<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"AtomicCell\")\n            .field(\"value\", &self.load())\n            .finish()\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"<atomic::seq_lock::SeqLockWriteGuard as std::ops::Drop>::drop":["#[inline]\nfn drop(&mut self){\n        // Release the lock and increment the stamp.\n        self.lock\n            .state\n            .store(self.state.wrapping_add(2), Ordering::Release);\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"<backoff::Backoff as std::default::Default>::default":["fn default() -> Backoff{\n        Backoff::new()\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"<backoff::Backoff as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"Backoff\")\n            .field(\"step\", &self.step)\n            .field(\"is_completed\", &self.is_completed())\n            .finish()\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"<cache_padded::CachePadded<T> as lazy_static::__Deref>::deref":["fn deref(&self) -> &T{\n        &self.value\n    }","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"<cache_padded::CachePadded<T> as std::convert::From<T>>::from":["fn from(t: T) -> Self{\n        CachePadded::new(t)\n    }","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"<cache_padded::CachePadded<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"CachePadded\")\n            .field(\"value\", &self.value)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"<cache_padded::CachePadded<T> as std::ops::DerefMut>::deref_mut":["fn deref_mut(&mut self) -> &mut T{\n        &mut self.value\n    }","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"<std::sync::atomic::AtomicBool as atomic::consume::AtomicConsume>::load_consume":["#[inline]\nfn load_consume(&self) -> Self::Val{\n            self.load(Ordering::Acquire)\n        }","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))"],"<std::sync::atomic::AtomicIsize as atomic::consume::AtomicConsume>::load_consume":["#[inline]\nfn load_consume(&self) -> Self::Val{\n            self.load(Ordering::Acquire)\n        }","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))"],"<std::sync::atomic::AtomicPtr<T> as atomic::consume::AtomicConsume>::load_consume":["#[inline]\nfn load_consume(&self) -> Self::Val{\n            self.load(Ordering::Acquire)\n        }","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))"],"<std::sync::atomic::AtomicUsize as atomic::consume::AtomicConsume>::load_consume":["#[inline]\nfn load_consume(&self) -> Self::Val{\n            self.load(Ordering::Acquire)\n        }","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))"],"<sync::parker::Parker as std::default::Default>::default":["fn default() -> Self{\n        Self {\n            unparker: Unparker {\n                inner: Arc::new(Inner {\n                    state: AtomicUsize::new(EMPTY),\n                    lock: Mutex::new(()),\n                    cvar: Condvar::new(),\n                }),\n            },\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"<sync::parker::Parker as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Parker { .. }\")\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"<sync::parker::Unparker as std::clone::Clone>::clone":["fn clone(&self) -> Unparker{\n        Unparker {\n            inner: self.inner.clone(),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"<sync::parker::Unparker as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Unparker { .. }\")\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"<sync::sharded_lock::Registration as std::ops::Drop>::drop":["fn drop(&mut self){\n        let mut indices = THREAD_INDICES.lock().unwrap();\n        indices.mapping.remove(&self.thread_id);\n        indices.free_list.push(self.index);\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLock<T> as std::convert::From<T>>::from":["fn from(t: T) -> Self{\n        ShardedLock::new(t)\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLock<T> as std::default::Default>::default":["fn default() -> ShardedLock<T>{\n        ShardedLock::new(Default::default())\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        match self.try_read() {\n            Ok(guard) => f\n                .debug_struct(\"ShardedLock\")\n                .field(\"data\", &&*guard)\n                .finish(),\n            Err(TryLockError::Poisoned(err)) => f\n                .debug_struct(\"ShardedLock\")\n                .field(\"data\", &&**err.get_ref())\n                .finish(),\n            Err(TryLockError::WouldBlock) => {\n                struct LockedPlaceholder;\n                impl fmt::Debug for LockedPlaceholder {\n                    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n                        f.write_str(\"<locked>\")\n                    }\n                }\n                f.debug_struct(\"ShardedLock\")\n                    .field(\"data\", &LockedPlaceholder)\n                    .finish()\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder":["struct LockedPlaceholder;","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as lazy_static::__Deref>::deref":["fn deref(&self) -> &T{\n        unsafe { &*self.lock.value.get() }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"ShardedLockReadGuard\")\n            .field(\"lock\", &self.lock)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as std::fmt::Display>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        (**self).fmt(f)\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as lazy_static::__Deref>::deref":["fn deref(&self) -> &T{\n        unsafe { &*self.lock.value.get() }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.debug_struct(\"ShardedLockWriteGuard\")\n            .field(\"lock\", &self.lock)\n            .finish()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::fmt::Display>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        (**self).fmt(f)\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::ops::DerefMut>::deref_mut":["fn deref_mut(&mut self) -> &mut T{\n        unsafe { &mut *self.lock.value.get() }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::ops::Drop>::drop":["fn drop(&mut self){\n        // Unlock the shards in reverse order of locking.\n        for shard in self.lock.shards.iter().rev() {\n            unsafe {\n                let dest: *mut _ = shard.write_guard.get();\n                let guard = mem::replace(&mut *dest, None);\n                drop(guard);\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::LazyStatic>::initialize":["fn initialize(lazy: &Self){\n                let _ = &**lazy;\n            }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref":["fn deref(&self) -> &$T{\n                #[inline(always)]\n                fn __static_ref_initialize() -> $T { $e }\n\n                #[inline(always)]\n                fn __stability() -> &'static $T {\n                    __lazy_static_create!(LAZY, $T);\n                    LAZY.get(__static_ref_initialize)\n                }\n                __stability()\n            }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref::__stability":["#[inline(always)]\nfn __stability() -> &'static $T{\n                    __lazy_static_create!(LAZY, $T);\n                    LAZY.get(__static_ref_initialize)\n                }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref::__static_ref_initialize":["#[inline(always)]\nfn __static_ref_initialize() -> $T{ $e }","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"<sync::wait_group::WaitGroup as std::clone::Clone>::clone":["fn clone(&self) -> WaitGroup{\n        let mut count = self.inner.count.lock().unwrap();\n        *count += 1;\n\n        WaitGroup {\n            inner: self.inner.clone(),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"<sync::wait_group::WaitGroup as std::default::Default>::default":["fn default() -> Self{\n        Self {\n            inner: Arc::new(Inner {\n                cvar: Condvar::new(),\n                count: Mutex::new(1),\n            }),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"<sync::wait_group::WaitGroup as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        let count: &usize = &*self.inner.count.lock().unwrap();\n        f.debug_struct(\"WaitGroup\").field(\"count\", count).finish()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"<sync::wait_group::WaitGroup as std::ops::Drop>::drop":["fn drop(&mut self){\n        let mut count = self.inner.count.lock().unwrap();\n        *count -= 1;\n\n        if *count == 0 {\n            self.inner.cvar.notify_all();\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"<thread::Scope<'_> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"Scope { .. }\")\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"<thread::ScopedJoinHandle<'_, T> as std::fmt::Debug>::fmt":["fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result{\n        f.pad(\"ScopedJoinHandle { .. }\")\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"<thread::ScopedJoinHandle<'_, T> as std::os::unix::thread::JoinHandleExt>::as_pthread_t":["fn as_pthread_t(&self) -> RawPthread{\n                // Borrow the handle. The handle will surely be available because the root scope waits\n                // for nested scopes before joining remaining threads.\n                let handle = self.handle.lock().unwrap();\n                handle.as_ref().unwrap().as_pthread_t()\n            }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"<thread::ScopedJoinHandle<'_, T> as std::os::unix::thread::JoinHandleExt>::into_pthread_t":["fn into_pthread_t(self) -> RawPthread{\n                self.as_pthread_t()\n            }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"atomic::atomic_cell::AtomicCell":["/// A thread-safe mutable memory location.\n///\n/// This type is equivalent to [`Cell`], except it can also be shared among multiple threads.\n///\n/// Operations on `AtomicCell`s use atomic instructions whenever possible, and synchronize using\n/// global locks otherwise. You can call [`AtomicCell::<T>::is_lock_free()`] to check whether\n/// atomic instructions or locks will be used.\n///\n/// Atomic loads use the [`Acquire`] ordering and atomic stores use the [`Release`] ordering.\n///\n/// [`Cell`]: https://doc.rust-lang.org/std/cell/struct.Cell.html\n/// [`AtomicCell::<T>::is_lock_free()`]: struct.AtomicCell.html#method.is_lock_free\n/// [`Acquire`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html#variant.Acquire\n/// [`Release`]: https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html#variant.Release\n#[repr(transparent)]\npub struct AtomicCell<T: ?Sized> {\n    /// The inner value.\n    ///\n    /// If this value can be transmuted into a primitive atomic type, it will be treated as such.\n    /// Otherwise, all potentially concurrent operations on this data will be protected by a global\n    /// lock.\n    value: UnsafeCell<T>,\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::as_ptr":["/// Returns a raw pointer to the underlying data in this atomic cell.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(5);\n///\n/// let ptr = a.as_ptr();\n/// ```\n#[inline]\npub fn as_ptr(&self) -> *mut T{\n        self.value.get()\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::compare_and_swap":["/// If the current value equals `current`, stores `new` into the atomic cell.\n///\n/// The return value is always the previous value. If it is equal to `current`, then the value\n/// was updated.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(1);\n///\n/// assert_eq!(a.compare_and_swap(2, 3), 1);\n/// assert_eq!(a.load(), 1);\n///\n/// assert_eq!(a.compare_and_swap(1, 2), 1);\n/// assert_eq!(a.load(), 2);\n/// ```\npub fn compare_and_swap(&self, current: T, new: T) -> T{\n        match self.compare_exchange(current, new) {\n            Ok(v) => v,\n            Err(v) => v,\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::compare_exchange":["/// If the current value equals `current`, stores `new` into the atomic cell.\n///\n/// The return value is a result indicating whether the new value was written and containing\n/// the previous value. On success this value is guaranteed to be equal to `current`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(1);\n///\n/// assert_eq!(a.compare_exchange(2, 3), Err(1));\n/// assert_eq!(a.load(), 1);\n///\n/// assert_eq!(a.compare_exchange(1, 2), Ok(1));\n/// assert_eq!(a.load(), 2);\n/// ```\npub fn compare_exchange(&self, current: T, new: T) -> Result<T, T>{\n        unsafe { atomic_compare_exchange_weak(self.value.get(), current, new) }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::into_inner":["/// Unwraps the atomic cell and returns its inner value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(7);\n/// let v = a.into_inner();\n///\n/// assert_eq!(v, 7);\n/// ```\npub fn into_inner(self) -> T{\n        self.value.into_inner()\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::is_lock_free":["/// Returns `true` if operations on values of this type are lock-free.\n///\n/// If the compiler or the platform doesn't support the necessary atomic instructions,\n/// `AtomicCell<T>` will use global locks for every potentially concurrent atomic operation.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// // This type is internally represented as `AtomicUsize` so we can just use atomic\n/// // operations provided by it.\n/// assert_eq!(AtomicCell::<usize>::is_lock_free(), true);\n///\n/// // A wrapper struct around `isize`.\n/// struct Foo {\n///     bar: isize,\n/// }\n/// // `AtomicCell<Foo>` will be internally represented as `AtomicIsize`.\n/// assert_eq!(AtomicCell::<Foo>::is_lock_free(), true);\n///\n/// // Operations on zero-sized types are always lock-free.\n/// assert_eq!(AtomicCell::<()>::is_lock_free(), true);\n///\n/// // Very large types cannot be represented as any of the standard atomic types, so atomic\n/// // operations on them will have to use global locks for synchronization.\n/// assert_eq!(AtomicCell::<[u8; 1000]>::is_lock_free(), false);\n/// ```\npub fn is_lock_free() -> bool{\n        atomic_is_lock_free::<T>()\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::load":["/// Loads a value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(7);\n///\n/// assert_eq!(a.load(), 7);\n/// ```\npub fn load(&self) -> T{\n        unsafe { atomic_load(self.value.get()) }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::new":["/// Creates a new atomic cell initialized with `val`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(7);\n/// ```\npub const fn new(val: T) -> AtomicCell<T>{\n        AtomicCell {\n            value: UnsafeCell::new(val),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::store":["/// Stores `val` into the atomic cell.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(7);\n///\n/// assert_eq!(a.load(), 7);\n/// a.store(8);\n/// assert_eq!(a.load(), 8);\n/// ```\npub fn store(&self, val: T){\n        if mem::needs_drop::<T>() {\n            drop(self.swap(val));\n        } else {\n            unsafe {\n                atomic_store(self.value.get(), val);\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::swap":["/// Stores `val` into the atomic cell and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(7);\n///\n/// assert_eq!(a.load(), 7);\n/// assert_eq!(a.swap(8), 7);\n/// assert_eq!(a.load(), 8);\n/// ```\npub fn swap(&self, val: T) -> T{\n        unsafe { atomic_swap(self.value.get(), val) }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<T>::take":["/// Takes the value of the atomic cell, leaving `Default::default()` in its place.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(5);\n/// let five = a.take();\n///\n/// assert_eq!(five, 5);\n/// assert_eq!(a.into_inner(), 0);\n/// ```\npub fn take(&self) -> T{\n        self.swap(Default::default())\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<bool>::fetch_and":["/// Applies logical \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(true);\n///\n/// assert_eq!(a.fetch_and(true), true);\n/// assert_eq!(a.load(), true);\n///\n/// assert_eq!(a.fetch_and(false), true);\n/// assert_eq!(a.load(), false);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: bool) -> bool{\n        let a = unsafe { &*(self.value.get() as *const AtomicBool) };\n        a.fetch_and(val, Ordering::AcqRel)\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<bool>::fetch_or":["/// Applies logical \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(false);\n///\n/// assert_eq!(a.fetch_or(false), false);\n/// assert_eq!(a.load(), false);\n///\n/// assert_eq!(a.fetch_or(true), false);\n/// assert_eq!(a.load(), true);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: bool) -> bool{\n        let a = unsafe { &*(self.value.get() as *const AtomicBool) };\n        a.fetch_or(val, Ordering::AcqRel)\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<bool>::fetch_xor":["/// Applies logical \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n/// let a = AtomicCell::new(true);\n///\n/// assert_eq!(a.fetch_xor(false), true);\n/// assert_eq!(a.load(), true);\n///\n/// assert_eq!(a.fetch_xor(true), true);\n/// assert_eq!(a.load(), false);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: bool) -> bool{\n        let a = unsafe { &*(self.value.get() as *const AtomicBool) };\n        a.fetch_xor(val, Ordering::AcqRel)\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i16>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i32>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i64>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<i8>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<isize>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u16>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u32>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u64>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<u8>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_add":["/// Increments the current value by `val` and returns the previous value.\n///\n/// The addition wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_add(3), 7);\n/// assert_eq!(a.load(), 10);\n/// ```\n#[inline]\npub fn fetch_add(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_add(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_and":["/// Applies bitwise \"and\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_and(3), 7);\n/// assert_eq!(a.load(), 3);\n/// ```\n#[inline]\npub fn fetch_and(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_and(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_or":["/// Applies bitwise \"or\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_or(16), 7);\n/// assert_eq!(a.load(), 23);\n/// ```\n#[inline]\npub fn fetch_or(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_or(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_sub":["/// Decrements the current value by `val` and returns the previous value.\n///\n/// The subtraction wraps on overflow.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_sub(3), 7);\n/// assert_eq!(a.load(), 4);\n/// ```\n#[inline]\npub fn fetch_sub(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_sub(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicCell::<usize>::fetch_xor":["/// Applies bitwise \"xor\" to the current value and returns the previous value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::atomic::AtomicCell;\n///\n#[doc = $example]\n///\n/// assert_eq!(a.fetch_xor(2), 7);\n/// assert_eq!(a.load(), 5);\n/// ```\n#[inline]\npub fn fetch_xor(&self, val: $t) -> $t{\n                let a = unsafe { &*(self.value.get() as *const $atomic) };\n                a.fetch_xor(val, Ordering::AcqRel)\n            }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicUnit":["/// An atomic `()`.\n///\n/// All operations are noops.\nstruct AtomicUnit;","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicUnit::compare_exchange_weak":["#[inline]\nfn compare_exchange_weak(\n        &self,\n        _current: (),\n        _new: (),\n        _success: Ordering,\n        _failure: Ordering,\n    ) -> Result<(), ()>{\n        Ok(())\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicUnit::load":["#[inline]\nfn load(&self, _order: Ordering){}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicUnit::store":["#[inline]\nfn store(&self, _val: (), _order: Ordering){}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::AtomicUnit::swap":["#[inline]\nfn swap(&self, _val: (), _order: Ordering){}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::atomic_compare_exchange_weak":["/// Atomically compares data at `dst` to `current` and, if equal byte-for-byte, exchanges data at\n/// `dst` with `new`.\n///\n/// Returns the old value on success, or the current value at `dst` on failure.\n///\n/// This operation uses the `AcqRel` ordering. If possible, an atomic instructions is used, and a\n/// global lock otherwise.\nunsafe fn atomic_compare_exchange_weak<T>(dst: *mut T, mut current: T, new: T) -> Result<T, T>\nwhere\n    T: Copy + Eq,{\n    atomic! {\n        T, a,\n        {\n            a = &*(dst as *const _ as *const _);\n            let mut current_raw = mem::transmute_copy(&current);\n            let new_raw = mem::transmute_copy(&new);\n\n            loop {\n                match a.compare_exchange_weak(\n                    current_raw,\n                    new_raw,\n                    Ordering::AcqRel,\n                    Ordering::Acquire,\n                ) {\n                    Ok(_) => break Ok(current),\n                    Err(previous_raw) => {\n                        let previous = mem::transmute_copy(&previous_raw);\n\n                        if !T::eq(&previous, &current) {\n                            break Err(previous);\n                        }\n\n                        // The compare-exchange operation has failed and didn't store `new`. The\n                        // failure is either spurious, or `previous` was semantically equal to\n                        // `current` but not byte-equal. Let's retry with `previous` as the new\n                        // `current`.\n                        current = previous;\n                        current_raw = previous_raw;\n                    }\n                }\n            }\n        },\n        {\n            let guard = lock(dst as usize).write();\n\n            if T::eq(&*dst, &current) {\n                Ok(ptr::replace(dst, new))\n            } else {\n                let val = ptr::read(dst);\n                // The value hasn't been changed. Drop the guard without incrementing the stamp.\n                guard.abort();\n                Err(val)\n            }\n        }\n    }\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::atomic_is_lock_free":["/// Returns `true` if operations on `AtomicCell<T>` are lock-free.\nfn atomic_is_lock_free<T>() -> bool{\n    atomic! { T, _a, true, false }\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::atomic_load":["/// Atomically reads data from `src`.\n///\n/// This operation uses the `Acquire` ordering. If possible, an atomic instructions is used, and a\n/// global lock otherwise.\nunsafe fn atomic_load<T>(src: *mut T) -> T\nwhere\n    T: Copy,{\n    atomic! {\n        T, a,\n        {\n            a = &*(src as *const _ as *const _);\n            mem::transmute_copy(&a.load(Ordering::Acquire))\n        },\n        {\n            let lock = lock(src as usize);\n\n            // Try doing an optimistic read first.\n            if let Some(stamp) = lock.optimistic_read() {\n                // We need a volatile read here because other threads might concurrently modify the\n                // value. In theory, data races are *always* UB, even if we use volatile reads and\n                // discard the data when a data race is detected. The proper solution would be to\n                // do atomic reads and atomic writes, but we can't atomically read and write all\n                // kinds of data since `AtomicU8` is not available on stable Rust yet.\n                let val = ptr::read_volatile(src);\n\n                if lock.validate_read(stamp) {\n                    return val;\n                }\n            }\n\n            // Grab a regular write lock so that writers don't starve this load.\n            let guard = lock.write();\n            let val = ptr::read(src);\n            // The value hasn't been changed. Drop the guard without incrementing the stamp.\n            guard.abort();\n            val\n        }\n    }\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::atomic_store":["/// Atomically writes `val` to `dst`.\n///\n/// This operation uses the `Release` ordering. If possible, an atomic instructions is used, and a\n/// global lock otherwise.\nunsafe fn atomic_store<T>(dst: *mut T, val: T){\n    atomic! {\n        T, a,\n        {\n            a = &*(dst as *const _ as *const _);\n            a.store(mem::transmute_copy(&val), Ordering::Release);\n            mem::forget(val);\n        },\n        {\n            let _guard = lock(dst as usize).write();\n            ptr::write(dst, val);\n        }\n    }\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::atomic_swap":["/// Atomically swaps data at `dst` with `val`.\n///\n/// This operation uses the `AcqRel` ordering. If possible, an atomic instructions is used, and a\n/// global lock otherwise.\nunsafe fn atomic_swap<T>(dst: *mut T, val: T) -> T{\n    atomic! {\n        T, a,\n        {\n            a = &*(dst as *const _ as *const _);\n            let res = mem::transmute_copy(&a.swap(mem::transmute_copy(&val), Ordering::AcqRel));\n            mem::forget(val);\n            res\n        },\n        {\n            let _guard = lock(dst as usize).write();\n            ptr::replace(dst, val)\n        }\n    }\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::can_transmute":["/// Returns `true` if values of type `A` can be transmuted into values of type `B`.\nfn can_transmute<A, B>() -> bool{\n    // Sizes must be equal, but alignment of `A` must be greater or equal than that of `B`.\n    mem::size_of::<A>() == mem::size_of::<B>() && mem::align_of::<A>() >= mem::align_of::<B>()\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::atomic_cell::lock":["/// Returns a reference to the global lock associated with the `AtomicCell` at address `addr`.\n///\n/// This function is used to protect atomic data which doesn't fit into any of the primitive atomic\n/// types in `std::sync::atomic`. Operations on such atomics must therefore use a global lock.\n///\n/// However, there is not only one global lock but an array of many locks, and one of them is\n/// picked based on the given address. Having many locks reduces contention and improves\n/// scalability.\n#[inline]\n#[must_use]\nfn lock(addr: usize) -> &'static SeqLock{\n    // The number of locks is a prime number because we want to make sure `addr % LEN` gets\n    // dispersed across all locks.\n    //\n    // Note that addresses are always aligned to some power of 2, depending on type `T` in\n    // `AtomicCell<T>`. If `LEN` was an even number, then `addr % LEN` would be an even number,\n    // too, which means only half of the locks would get utilized!\n    //\n    // It is also possible for addresses to accidentally get aligned to a number that is not a\n    // power of 2. Consider this example:\n    //\n    // ```\n    // #[repr(C)]\n    // struct Foo {\n    //     a: AtomicCell<u8>,\n    //     b: u8,\n    //     c: u8,\n    // }\n    // ```\n    //\n    // Now, if we have a slice of type `&[Foo]`, it is possible that field `a` in all items gets\n    // stored at addresses that are multiples of 3. It'd be too bad if `LEN` was divisible by 3.\n    // In order to protect from such cases, we simply choose a large prime number for `LEN`.\n    const LEN: usize = 97;\n\n    static LOCKS: [SeqLock; LEN] = [\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n        SeqLock::new(),\n    ];\n\n    // If the modulus is a constant number, the compiler will use crazy math to transform this into\n    // a sequence of cheap arithmetic operations rather than using the slow modulo instruction.\n    &LOCKS[addr % LEN]\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))"],"atomic::consume::AtomicConsume":["/// Trait which allows reading from primitive atomic types with \"consume\" ordering.\npub trait AtomicConsume {\n    /// Type returned by `load_consume`.\n    type Val;\n\n    /// Loads a value from the atomic using a \"consume\" memory ordering.\n    ///\n    /// This is similar to the \"acquire\" ordering, except that an ordering is\n    /// only guaranteed with operations that \"depend on\" the result of the load.\n    /// However consume loads are usually much faster than acquire loads on\n    /// architectures with a weak memory model since they don't require memory\n    /// fence instructions.\n    ///\n    /// The exact definition of \"depend on\" is a bit vague, but it works as you\n    /// would expect in practice since a lot of software, especially the Linux\n    /// kernel, rely on this behavior.\n    ///\n    /// This is currently only implemented on ARM and AArch64, where a fence\n    /// can be avoided. On other architectures this will fall back to a simple\n    /// `load(Ordering::Acquire)`.\n    fn load_consume(&self) -> Self::Val;\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))"],"atomic::seq_lock::SeqLock":["/// A simple stamped lock.\npub struct SeqLock {\n    /// The current state of the lock.\n    ///\n    /// All bits except the least significant one hold the current stamp. When locked, the state\n    /// equals 1 and doesn't contain a valid stamp.\n    state: AtomicUsize,\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"atomic::seq_lock::SeqLock::new":["pub const fn new() -> Self{\n        Self {\n            state: AtomicUsize::new(0),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"atomic::seq_lock::SeqLock::optimistic_read":["/// If not locked, returns the current stamp.\n///\n/// This method should be called before optimistic reads.\n#[inline]\npub fn optimistic_read(&self) -> Option<usize>{\n        let state = self.state.load(Ordering::Acquire);\n        if state == 1 {\n            None\n        } else {\n            Some(state)\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"atomic::seq_lock::SeqLock::validate_read":["/// Returns `true` if the current stamp is equal to `stamp`.\n///\n/// This method should be called after optimistic reads to check whether they are valid. The\n/// argument `stamp` should correspond to the one returned by method `optimistic_read`.\n#[inline]\npub fn validate_read(&self, stamp: usize) -> bool{\n        atomic::fence(Ordering::Acquire);\n        self.state.load(Ordering::Relaxed) == stamp\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"atomic::seq_lock::SeqLock::write":["/// Grabs the lock for writing.\n#[inline]\npub fn write(&'static self) -> SeqLockWriteGuard{\n        let backoff = Backoff::new();\n        loop {\n            let previous = self.state.swap(1, Ordering::Acquire);\n\n            if previous != 1 {\n                atomic::fence(Ordering::Release);\n\n                return SeqLockWriteGuard {\n                    lock: self,\n                    state: previous,\n                };\n            }\n\n            backoff.snooze();\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"atomic::seq_lock::SeqLockWriteGuard":["/// An RAII guard that releases the lock and increments the stamp when dropped.\npub struct SeqLockWriteGuard {\n    /// The parent lock.\n    lock: &'static SeqLock,\n\n    /// The stamp before locking.\n    state: usize,\n}","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"atomic::seq_lock::SeqLockWriteGuard::abort":["/// Releases the lock without incrementing the stamp.\n#[inline]\npub fn abort(self){\n        self.lock.state.store(self.state, Ordering::Release);\n\n        // We specifically don't want to call drop(), since that's\n        // what increments the stamp.\n        mem::forget(self);\n    }","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))"],"backoff::Backoff":["/// Performs exponential backoff in spin loops.\n///\n/// Backing off in spin loops reduces contention and improves overall performance.\n///\n/// This primitive can execute *YIELD* and *PAUSE* instructions, yield the current thread to the OS\n/// scheduler, and tell when is a good time to block the thread using a different synchronization\n/// mechanism. Each step of the back off procedure takes roughly twice as long as the previous\n/// step.\n///\n/// # Examples\n///\n/// Backing off in a lock-free loop:\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n/// use std::sync::atomic::AtomicUsize;\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// fn fetch_mul(a: &AtomicUsize, b: usize) -> usize {\n///     let backoff = Backoff::new();\n///     loop {\n///         let val = a.load(SeqCst);\n///         if a.compare_and_swap(val, val.wrapping_mul(b), SeqCst) == val {\n///             return val;\n///         }\n///         backoff.spin();\n///     }\n/// }\n/// ```\n///\n/// Waiting for an [`AtomicBool`] to become `true`:\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n/// use std::sync::atomic::AtomicBool;\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// fn spin_wait(ready: &AtomicBool) {\n///     let backoff = Backoff::new();\n///     while !ready.load(SeqCst) {\n///         backoff.snooze();\n///     }\n/// }\n/// ```\n///\n/// Waiting for an [`AtomicBool`] to become `true` and parking the thread after a long wait.\n/// Note that whoever sets the atomic variable to `true` must notify the parked thread by calling\n/// [`unpark()`]:\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n/// use std::sync::atomic::AtomicBool;\n/// use std::sync::atomic::Ordering::SeqCst;\n/// use std::thread;\n///\n/// fn blocking_wait(ready: &AtomicBool) {\n///     let backoff = Backoff::new();\n///     while !ready.load(SeqCst) {\n///         if backoff.is_completed() {\n///             thread::park();\n///         } else {\n///             backoff.snooze();\n///         }\n///     }\n/// }\n/// ```\n///\n/// [`is_completed`]: struct.Backoff.html#method.is_completed\n/// [`std::thread::park()`]: https://doc.rust-lang.org/std/thread/fn.park.html\n/// [`Condvar`]: https://doc.rust-lang.org/std/sync/struct.Condvar.html\n/// [`AtomicBool`]: https://doc.rust-lang.org/std/sync/atomic/struct.AtomicBool.html\n/// [`unpark()`]: https://doc.rust-lang.org/std/thread/struct.Thread.html#method.unpark\npub struct Backoff {\n    step: Cell<u32>,\n}","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"backoff::Backoff::is_completed":["/// Returns `true` if exponential backoff has completed and blocking the thread is advised.\n///\n/// # Examples\n///\n/// Waiting for an [`AtomicBool`] to become `true` and parking the thread after a long wait:\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n/// use std::sync::Arc;\n/// use std::sync::atomic::AtomicBool;\n/// use std::sync::atomic::Ordering::SeqCst;\n/// use std::thread;\n/// use std::time::Duration;\n///\n/// fn blocking_wait(ready: &AtomicBool) {\n///     let backoff = Backoff::new();\n///     while !ready.load(SeqCst) {\n///         if backoff.is_completed() {\n///             thread::park();\n///         } else {\n///             backoff.snooze();\n///         }\n///     }\n/// }\n///\n/// let ready = Arc::new(AtomicBool::new(false));\n/// let ready2 = ready.clone();\n/// let waiter = thread::current();\n///\n/// thread::spawn(move || {\n///     thread::sleep(Duration::from_millis(100));\n///     ready2.store(true, SeqCst);\n///     waiter.unpark();\n/// });\n///\n/// assert_eq!(ready.load(SeqCst), false);\n/// blocking_wait(&ready);\n/// assert_eq!(ready.load(SeqCst), true);\n/// ```\n///\n/// [`AtomicBool`]: https://doc.rust-lang.org/std/sync/atomic/struct.AtomicBool.html\n#[inline]\npub fn is_completed(&self) -> bool{\n        self.step.get() > YIELD_LIMIT\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"backoff::Backoff::new":["/// Creates a new `Backoff`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n///\n/// let backoff = Backoff::new();\n/// ```\n#[inline]\npub fn new() -> Self{\n        Backoff { step: Cell::new(0) }\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"backoff::Backoff::reset":["/// Resets the `Backoff`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n///\n/// let backoff = Backoff::new();\n/// backoff.reset();\n/// ```\n#[inline]\npub fn reset(&self){\n        self.step.set(0);\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"backoff::Backoff::snooze":["/// Backs off in a blocking loop.\n///\n/// This method should be used when we need to wait for another thread to make progress.\n///\n/// The processor may yield using the *YIELD* or *PAUSE* instruction and the current thread\n/// may yield by giving up a timeslice to the OS scheduler.\n///\n/// In `#[no_std]` environments, this method is equivalent to [`spin`].\n///\n/// If possible, use [`is_completed`] to check when it is advised to stop using backoff and\n/// block the current thread using a different synchronization mechanism instead.\n///\n/// [`spin`]: struct.Backoff.html#method.spin\n/// [`is_completed`]: struct.Backoff.html#method.is_completed\n///\n/// # Examples\n///\n/// Waiting for an [`AtomicBool`] to become `true`:\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n/// use std::sync::Arc;\n/// use std::sync::atomic::AtomicBool;\n/// use std::sync::atomic::Ordering::SeqCst;\n/// use std::thread;\n/// use std::time::Duration;\n///\n/// fn spin_wait(ready: &AtomicBool) {\n///     let backoff = Backoff::new();\n///     while !ready.load(SeqCst) {\n///         backoff.snooze();\n///     }\n/// }\n///\n/// let ready = Arc::new(AtomicBool::new(false));\n/// let ready2 = ready.clone();\n///\n/// thread::spawn(move || {\n///     thread::sleep(Duration::from_millis(100));\n///     ready2.store(true, SeqCst);\n/// });\n///\n/// assert_eq!(ready.load(SeqCst), false);\n/// spin_wait(&ready);\n/// assert_eq!(ready.load(SeqCst), true);\n/// ```\n///\n/// [`AtomicBool`]: https://doc.rust-lang.org/std/sync/atomic/struct.AtomicBool.html\n#[inline]\npub fn snooze(&self){\n        if self.step.get() <= SPIN_LIMIT {\n            for _ in 0..1 << self.step.get() {\n                atomic::spin_loop_hint();\n            }\n        } else {\n            #[cfg(not(feature = \"std\"))]\n            for _ in 0..1 << self.step.get() {\n                atomic::spin_loop_hint();\n            }\n\n            #[cfg(feature = \"std\")]\n            ::std::thread::yield_now();\n        }\n\n        if self.step.get() <= YIELD_LIMIT {\n            self.step.set(self.step.get() + 1);\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"backoff::Backoff::spin":["/// Backs off in a lock-free loop.\n///\n/// This method should be used when we need to retry an operation because another thread made\n/// progress.\n///\n/// The processor may yield using the *YIELD* or *PAUSE* instruction.\n///\n/// # Examples\n///\n/// Backing off in a lock-free loop:\n///\n/// ```\n/// use crossbeam_utils::Backoff;\n/// use std::sync::atomic::AtomicUsize;\n/// use std::sync::atomic::Ordering::SeqCst;\n///\n/// fn fetch_mul(a: &AtomicUsize, b: usize) -> usize {\n///     let backoff = Backoff::new();\n///     loop {\n///         let val = a.load(SeqCst);\n///         if a.compare_and_swap(val, val.wrapping_mul(b), SeqCst) == val {\n///             return val;\n///         }\n///         backoff.spin();\n///     }\n/// }\n///\n/// let a = AtomicUsize::new(7);\n/// assert_eq!(fetch_mul(&a, 8), 7);\n/// assert_eq!(a.load(SeqCst), 56);\n/// ```\n#[inline]\npub fn spin(&self){\n        for _ in 0..1 << self.step.get().min(SPIN_LIMIT) {\n            atomic::spin_loop_hint();\n        }\n\n        if self.step.get() <= SPIN_LIMIT {\n            self.step.set(self.step.get() + 1);\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))"],"cache_padded::CachePadded":["/// Pads and aligns a value to the length of a cache line.\n///\n/// In concurrent programming, sometimes it is desirable to make sure commonly accessed pieces of\n/// data are not placed into the same cache line. Updating an atomic value invalides the whole\n/// cache line it belongs to, which makes the next access to the same cache line slower for other\n/// CPU cores. Use `CachePadded` to ensure updating one piece of data doesn't invalidate other\n/// cached data.\n///\n/// # Size and alignment\n///\n/// Cache lines are assumed to be N bytes long, depending on the architecture:\n///\n/// * On x86-64 and aarch64, N = 128.\n/// * On all others, N = 64.\n///\n/// Note that N is just a reasonable guess and is not guaranteed to match the actual cache line\n/// length of the machine the program is running on. On modern Intel architectures, spatial\n/// prefetcher is pulling pairs of 64-byte cache lines at a time, so we pessimistically assume that\n/// cache lines are 128 bytes long.\n///\n/// The size of `CachePadded<T>` is the smallest multiple of N bytes large enough to accommodate\n/// a value of type `T`.\n///\n/// The alignment of `CachePadded<T>` is the maximum of N bytes and the alignment of `T`.\n///\n/// # Examples\n///\n/// Alignment and padding:\n///\n/// ```\n/// use crossbeam_utils::CachePadded;\n///\n/// let array = [CachePadded::new(1i8), CachePadded::new(2i8)];\n/// let addr1 = &*array[0] as *const i8 as usize;\n/// let addr2 = &*array[1] as *const i8 as usize;\n///\n/// assert!(addr2 - addr1 >= 64);\n/// assert_eq!(addr1 % 64, 0);\n/// assert_eq!(addr2 % 64, 0);\n/// ```\n///\n/// When building a concurrent queue with a head and a tail index, it is wise to place them in\n/// different cache lines so that concurrent threads pushing and popping elements don't invalidate\n/// each other's cache lines:\n///\n/// ```\n/// use crossbeam_utils::CachePadded;\n/// use std::sync::atomic::AtomicUsize;\n///\n/// struct Queue<T> {\n///     head: CachePadded<AtomicUsize>,\n///     tail: CachePadded<AtomicUsize>,\n///     buffer: *mut T,\n/// }\n/// ```\nrepr(align(128))\npub struct CachePadded<T> {\n    value: T,\n}","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"cache_padded::CachePadded::<T>::into_inner":["/// Returns the inner value.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::CachePadded;\n///\n/// let padded_value = CachePadded::new(7);\n/// let value = padded_value.into_inner();\n/// assert_eq!(value, 7);\n/// ```\npub fn into_inner(self) -> T{\n        self.value\n    }","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"cache_padded::CachePadded::<T>::new":["/// Pads and aligns a value to the length of a cache line.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::CachePadded;\n///\n/// let padded_value = CachePadded::new(1);\n/// ```\npub const fn new(t: T) -> CachePadded<T>{\n        CachePadded::<T> { value: t }\n    }","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))"],"sync::parker::Inner":["struct Inner {\n    state: AtomicUsize,\n    lock: Mutex<()>,\n    cvar: Condvar,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Inner::park":["fn park(&self, timeout: Option<Duration>){\n        // If we were previously notified then we consume this notification and return quickly.\n        if self\n            .state\n            .compare_exchange(NOTIFIED, EMPTY, SeqCst, SeqCst)\n            .is_ok()\n        {\n            return;\n        }\n\n        // If the timeout is zero, then there is no need to actually block.\n        if let Some(ref dur) = timeout {\n            if *dur == Duration::from_millis(0) {\n                return;\n            }\n        }\n\n        // Otherwise we need to coordinate going to sleep.\n        let mut m = self.lock.lock().unwrap();\n\n        match self.state.compare_exchange(EMPTY, PARKED, SeqCst, SeqCst) {\n            Ok(_) => {}\n            // Consume this notification to avoid spurious wakeups in the next park.\n            Err(NOTIFIED) => {\n                // We must read `state` here, even though we know it will be `NOTIFIED`. This is\n                // because `unpark` may have been called again since we read `NOTIFIED` in the\n                // `compare_exchange` above. We must perform an acquire operation that synchronizes\n                // with that `unpark` to observe any writes it made before the call to `unpark`. To\n                // do that we must read from the write it made to `state`.\n                let old = self.state.swap(EMPTY, SeqCst);\n                assert_eq!(old, NOTIFIED, \"park state changed unexpectedly\");\n                return;\n            }\n            Err(n) => panic!(\"inconsistent park_timeout state: {}\", n),\n        }\n\n        match timeout {\n            None => {\n                loop {\n                    // Block the current thread on the conditional variable.\n                    m = self.cvar.wait(m).unwrap();\n\n                    if self\n                        .state\n                        .compare_exchange(NOTIFIED, EMPTY, SeqCst, SeqCst)\n                        .is_ok()\n                    {\n                        // got a notification\n                        return;\n                    }\n\n                    // spurious wakeup, go back to sleep\n                }\n            }\n            Some(timeout) => {\n                // Wait with a timeout, and if we spuriously wake up or otherwise wake up from a\n                // notification we just want to unconditionally set `state` back to `EMPTY`, either\n                // consuming a notification or un-flagging ourselves as parked.\n                let (_m, _result) = self.cvar.wait_timeout(m, timeout).unwrap();\n\n                match self.state.swap(EMPTY, SeqCst) {\n                    NOTIFIED => {} // got a notification\n                    PARKED => {}   // no notification\n                    n => panic!(\"inconsistent park_timeout state: {}\", n),\n                }\n            }\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Inner::unpark":["pub fn unpark(&self){\n        // To ensure the unparked thread will observe any writes we made before this call, we must\n        // perform a release operation that `park` can synchronize with. To do that we must write\n        // `NOTIFIED` even if `state` is already `NOTIFIED`. That is why this must be a swap rather\n        // than a compare-and-swap that returns if it reads `NOTIFIED` on failure.\n        match self.state.swap(NOTIFIED, SeqCst) {\n            EMPTY => return,    // no one was waiting\n            NOTIFIED => return, // already unparked\n            PARKED => {}        // gotta go wake someone up\n            _ => panic!(\"inconsistent state in unpark\"),\n        }\n\n        // There is a period between when the parked thread sets `state` to `PARKED` (or last\n        // checked `state` in the case of a spurious wakeup) and when it actually waits on `cvar`.\n        // If we were to notify during this period it would be ignored and then when the parked\n        // thread went to sleep it would never wake up. Fortunately, it has `lock` locked at this\n        // stage so we can acquire `lock` to wait until it is ready to receive the notification.\n        //\n        // Releasing `lock` before the call to `notify_one` means that when the parked thread wakes\n        // it doesn't get woken only to have to wait for us to release `lock`.\n        drop(self.lock.lock().unwrap());\n        self.cvar.notify_one();\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker":["/// A thread parking primitive.\n///\n/// Conceptually, each `Parker` has an associated token which is initially not present:\n///\n/// * The [`park`] method blocks the current thread unless or until the token is available, at\n///   which point it automatically consumes the token. It may also return *spuriously*, without\n///   consuming the token.\n///\n/// * The [`park_timeout`] method works the same as [`park`], but blocks for a specified maximum\n///   time.\n///\n/// * The [`unpark`] method atomically makes the token available if it wasn't already. Because the\n///   token is initially absent, [`unpark`] followed by [`park`] will result in the second call\n///   returning immediately.\n///\n/// In other words, each `Parker` acts a bit like a spinlock that can be locked and unlocked using\n/// [`park`] and [`unpark`].\n///\n/// # Examples\n///\n/// ```\n/// use std::thread;\n/// use std::time::Duration;\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// let u = p.unparker().clone();\n///\n/// // Make the token available.\n/// u.unpark();\n/// // Wakes up immediately and consumes the token.\n/// p.park();\n///\n/// thread::spawn(move || {\n///     thread::sleep(Duration::from_millis(500));\n///     u.unpark();\n/// });\n///\n/// // Wakes up when `u.unpark()` provides the token, but may also wake up\n/// // spuriously before that without consuming the token.\n/// p.park();\n/// ```\n///\n/// [`park`]: struct.Parker.html#method.park\n/// [`park_timeout`]: struct.Parker.html#method.park_timeout\n/// [`unpark`]: struct.Unparker.html#method.unpark\npub struct Parker {\n    unparker: Unparker,\n    _marker: PhantomData<*const ()>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker::from_raw":["/// Converts a raw pointer into a `Parker`.\n///\n/// # Safety\n///\n/// This method is safe to use only with pointers returned by [`Parker::into_raw`].\n///\n/// [`Parker::into_raw`]: struct.Parker.html#method.into_raw\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// let raw = Parker::into_raw(p);\n/// let p = unsafe { Parker::from_raw(raw) };\n/// ```\npub unsafe fn from_raw(ptr: *const ()) -> Parker{\n        Parker {\n            unparker: Unparker::from_raw(ptr),\n            _marker: PhantomData,\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker::into_raw":["/// Converts a `Parker` into a raw pointer.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// let raw = Parker::into_raw(p);\n/// ```\npub fn into_raw(this: Parker) -> *const (){\n        Unparker::into_raw(this.unparker)\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker::new":["/// Creates a new `Parker`.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// ```\n///\npub fn new() -> Parker{\n        Self::default()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker::park":["/// Blocks the current thread until the token is made available.\n///\n/// A call to `park` may wake up spuriously without consuming the token, and callers should be\n/// prepared for this possibility.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// let u = p.unparker().clone();\n///\n/// // Make the token available.\n/// u.unpark();\n///\n/// // Wakes up immediately and consumes the token.\n/// p.park();\n/// ```\npub fn park(&self){\n        self.unparker.inner.park(None);\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker::park_timeout":["/// Blocks the current thread until the token is made available, but only for a limited time.\n///\n/// A call to `park_timeout` may wake up spuriously without consuming the token, and callers\n/// should be prepared for this possibility.\n///\n/// # Examples\n///\n/// ```\n/// use std::time::Duration;\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n///\n/// // Waits for the token to become available, but will not wait longer than 500 ms.\n/// p.park_timeout(Duration::from_millis(500));\n/// ```\npub fn park_timeout(&self, timeout: Duration){\n        self.unparker.inner.park(Some(timeout));\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Parker::unparker":["/// Returns a reference to an associated [`Unparker`].\n///\n/// The returned [`Unparker`] doesn't have to be used by reference - it can also be cloned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// let u = p.unparker().clone();\n///\n/// // Make the token available.\n/// u.unpark();\n/// // Wakes up immediately and consumes the token.\n/// p.park();\n/// ```\n///\n/// [`park`]: struct.Parker.html#method.park\n/// [`park_timeout`]: struct.Parker.html#method.park_timeout\n///\n/// [`Unparker`]: struct.Unparker.html\npub fn unparker(&self) -> &Unparker{\n        &self.unparker\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Unparker":["/// Unparks a thread parked by the associated [`Parker`].\n///\n/// [`Parker`]: struct.Parker.html\npub struct Unparker {\n    inner: Arc<Inner>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Unparker::from_raw":["/// Converts a raw pointer into an `Unparker`.\n///\n/// # Safety\n///\n/// This method is safe to use only with pointers returned by [`Unparker::into_raw`].\n///\n/// [`Unparker::into_raw`]: struct.Unparker.html#method.into_raw\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::{Parker, Unparker};\n///\n/// let p = Parker::new();\n/// let u = p.unparker().clone();\n///\n/// let raw = Unparker::into_raw(u);\n/// let u = unsafe { Unparker::from_raw(raw) };\n/// ```\npub unsafe fn from_raw(ptr: *const ()) -> Unparker{\n        Unparker {\n            inner: Arc::from_raw(ptr as *const Inner),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Unparker::into_raw":["/// Converts an `Unparker` into a raw pointer.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::{Parker, Unparker};\n///\n/// let p = Parker::new();\n/// let u = p.unparker().clone();\n/// let raw = Unparker::into_raw(u);\n/// ```\npub fn into_raw(this: Unparker) -> *const (){\n        Arc::into_raw(this.inner) as *const ()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::parker::Unparker::unpark":["/// Atomically makes the token available if it is not already.\n///\n/// This method will wake up the thread blocked on [`park`] or [`park_timeout`], if there is\n/// any.\n///\n/// # Examples\n///\n/// ```\n/// use std::thread;\n/// use std::time::Duration;\n/// use crossbeam_utils::sync::Parker;\n///\n/// let p = Parker::new();\n/// let u = p.unparker().clone();\n///\n/// thread::spawn(move || {\n///     thread::sleep(Duration::from_millis(500));\n///     u.unpark();\n/// });\n///\n/// // Wakes up when `u.unpark()` provides the token, but may also wake up\n/// // spuriously before that without consuming the token.\n/// p.park();\n/// ```\n///\n/// [`park`]: struct.Parker.html#method.park\n/// [`park_timeout`]: struct.Parker.html#method.park_timeout\npub fn unpark(&self){\n        self.inner.unpark()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))"],"sync::sharded_lock::REGISTRATION::__getit":["inline\nunsafe fn __getit(\n                init: $crate::option::Option<&mut $crate::option::Option<$t>>,\n            ) -> $crate::option::Option<&'static $t>{\n                #[cfg(all(target_family = \"wasm\", not(target_feature = \"atomics\")))]\n                static __KEY: $crate::thread::__StaticLocalKeyInner<$t> =\n                    $crate::thread::__StaticLocalKeyInner::new();\n\n                #[thread_local]\n                #[cfg(all(\n                    target_thread_local,\n                    not(all(target_family = \"wasm\", not(target_feature = \"atomics\"))),\n                ))]\n                static __KEY: $crate::thread::__FastLocalKeyInner<$t> =\n                    $crate::thread::__FastLocalKeyInner::new();\n\n                #[cfg(all(\n                    not(target_thread_local),\n                    not(all(target_family = \"wasm\", not(target_feature = \"atomics\"))),\n                ))]\n                static __KEY: $crate::thread::__OsLocalKeyInner<$t> =\n                    $crate::thread::__OsLocalKeyInner::new();\n\n                // FIXME: remove the #[allow(...)] marker when macros don't\n                // raise warning for missing/extraneous unsafe blocks anymore.\n                // See https://github.com/rust-lang/rust/issues/74838.\n                #[allow(unused_unsafe)]\n                unsafe {\n                    __KEY.get(move || {\n                        if let $crate::option::Option::Some(init) = init {\n                            if let $crate::option::Option::Some(value) = init.take() {\n                                return value;\n                            } else if $crate::cfg!(debug_assertions) {\n                                $crate::unreachable!(\"missing default value\");\n                            }\n                        }\n                        __init()\n                    })\n                }\n            }","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })"],"sync::sharded_lock::REGISTRATION::__init":["#[inline]\nfn __init() -> $t{ $init }","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })"],"sync::sharded_lock::Registration":["/// A registration of a thread with an index.\n///\n/// When dropped, unregisters the thread and frees the reserved index.\nstruct Registration {\n    index: usize,\n    thread_id: ThreadId,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::Shard":["/// A shard containing a single reader-writer lock.\nstruct Shard {\n    /// The inner reader-writer lock.\n    lock: RwLock<()>,\n\n    /// The write-guard keeping this shard locked.\n    ///\n    /// Write operations will lock each shard and store the guard here. These guards get dropped at\n    /// the same time the big guard is dropped.\n    write_guard: UnsafeCell<Option<RwLockWriteGuard<'static, ()>>>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock":["/// A sharded reader-writer lock.\n///\n/// This lock is equivalent to [`RwLock`], except read operations are faster and write operations\n/// are slower.\n///\n/// A `ShardedLock` is internally made of a list of *shards*, each being a [`RwLock`] occupying a\n/// single cache line. Read operations will pick one of the shards depending on the current thread\n/// and lock it. Write operations need to lock all shards in succession.\n///\n/// By splitting the lock into shards, concurrent read operations will in most cases choose\n/// different shards and thus update different cache lines, which is good for scalability. However,\n/// write operations need to do more work and are therefore slower than usual.\n///\n/// The priority policy of the lock is dependent on the underlying operating system's\n/// implementation, and this type does not guarantee that any particular policy will be used.\n///\n/// # Poisoning\n///\n/// A `ShardedLock`, like [`RwLock`], will become poisoned on a panic. Note that it may only be\n/// poisoned if a panic occurs while a write operation is in progress. If a panic occurs in any\n/// read operation, the lock will not be poisoned.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let lock = ShardedLock::new(5);\n///\n/// // Any number of read locks can be held at once.\n/// {\n///     let r1 = lock.read().unwrap();\n///     let r2 = lock.read().unwrap();\n///     assert_eq!(*r1, 5);\n///     assert_eq!(*r2, 5);\n/// } // Read locks are dropped at this point.\n///\n/// // However, only one write lock may be held.\n/// {\n///     let mut w = lock.write().unwrap();\n///     *w += 1;\n///     assert_eq!(*w, 6);\n/// } // Write lock is dropped here.\n/// ```\n///\n/// [`RwLock`]: https://doc.rust-lang.org/std/sync/struct.RwLock.html\npub struct ShardedLock<T: ?Sized> {\n    /// A list of locks protecting the internal data.\n    shards: Box<[CachePadded<Shard>]>,\n\n    /// The internal data.\n    value: UnsafeCell<T>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::get_mut":["/// Returns a mutable reference to the underlying data.\n///\n/// Since this call borrows the lock mutably, no actual locking needs to take place.\n///\n/// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n/// operation panics.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let mut lock = ShardedLock::new(0);\n/// *lock.get_mut().unwrap() = 10;\n/// assert_eq!(*lock.read().unwrap(), 10);\n/// ```\npub fn get_mut(&mut self) -> LockResult<&mut T>{\n        let is_poisoned = self.is_poisoned();\n        let inner = unsafe { &mut *self.value.get() };\n\n        if is_poisoned {\n            Err(PoisonError::new(inner))\n        } else {\n            Ok(inner)\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::into_inner":["/// Consumes this lock, returning the underlying data.\n///\n/// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n/// operation panics.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let lock = ShardedLock::new(String::new());\n/// {\n///     let mut s = lock.write().unwrap();\n///     *s = \"modified\".to_owned();\n/// }\n/// assert_eq!(lock.into_inner().unwrap(), \"modified\");\n/// ```\npub fn into_inner(self) -> LockResult<T>{\n        let is_poisoned = self.is_poisoned();\n        let inner = self.value.into_inner();\n\n        if is_poisoned {\n            Err(PoisonError::new(inner))\n        } else {\n            Ok(inner)\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::is_poisoned":["/// Returns `true` if the lock is poisoned.\n///\n/// If another thread can still access the lock, it may become poisoned at any time. A `false`\n/// result should not be trusted without additional synchronization.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n/// use std::sync::Arc;\n/// use std::thread;\n///\n/// let lock = Arc::new(ShardedLock::new(0));\n/// let c_lock = lock.clone();\n///\n/// let _ = thread::spawn(move || {\n///     let _lock = c_lock.write().unwrap();\n///     panic!(); // the lock gets poisoned\n/// }).join();\n/// assert_eq!(lock.is_poisoned(), true);\n/// ```\npub fn is_poisoned(&self) -> bool{\n        self.shards[0].lock.is_poisoned()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::new":["/// Creates a new sharded reader-writer lock.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let lock = ShardedLock::new(5);\n/// ```\npub fn new(value: T) -> ShardedLock<T>{\n        ShardedLock {\n            shards: (0..NUM_SHARDS)\n                .map(|_| {\n                    CachePadded::new(Shard {\n                        lock: RwLock::new(()),\n                        write_guard: UnsafeCell::new(None),\n                    })\n                })\n                .collect::<Vec<_>>()\n                .into_boxed_slice(),\n            value: UnsafeCell::new(value),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::read":["/// Locks with shared read access, blocking the current thread until it can be acquired.\n///\n/// The calling thread will be blocked until there are no more writers which hold the lock.\n/// There may be other readers currently inside the lock when this method returns. This method\n/// does not provide any guarantees with respect to the ordering of whether contentious readers\n/// or writers will acquire the lock first.\n///\n/// Returns a guard which will release the shared access when dropped.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n/// use std::sync::Arc;\n/// use std::thread;\n///\n/// let lock = Arc::new(ShardedLock::new(1));\n/// let c_lock = lock.clone();\n///\n/// let n = lock.read().unwrap();\n/// assert_eq!(*n, 1);\n///\n/// thread::spawn(move || {\n///     let r = c_lock.read();\n///     assert!(r.is_ok());\n/// }).join().unwrap();\n/// ```\npub fn read(&self) -> LockResult<ShardedLockReadGuard<'_, T>>{\n        // Take the current thread index and map it to a shard index. Thread indices will tend to\n        // distribute shards among threads equally, thus reducing contention due to read-locking.\n        let current_index = current_index().unwrap_or(0);\n        let shard_index = current_index & (self.shards.len() - 1);\n\n        match self.shards[shard_index].lock.read() {\n            Ok(guard) => Ok(ShardedLockReadGuard {\n                lock: self,\n                _guard: guard,\n                _marker: PhantomData,\n            }),\n            Err(err) => Err(PoisonError::new(ShardedLockReadGuard {\n                lock: self,\n                _guard: err.into_inner(),\n                _marker: PhantomData,\n            })),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::try_read":["/// Attempts to acquire this lock with shared read access.\n///\n/// If the access could not be granted at this time, an error is returned. Otherwise, a guard\n/// is returned which will release the shared access when it is dropped. This method does not\n/// provide any guarantees with respect to the ordering of whether contentious readers or\n/// writers will acquire the lock first.\n///\n/// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n/// operation panics.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let lock = ShardedLock::new(1);\n///\n/// match lock.try_read() {\n///     Ok(n) => assert_eq!(*n, 1),\n///     Err(_) => unreachable!(),\n/// };\n/// ```\npub fn try_read(&self) -> TryLockResult<ShardedLockReadGuard<'_, T>>{\n        // Take the current thread index and map it to a shard index. Thread indices will tend to\n        // distribute shards among threads equally, thus reducing contention due to read-locking.\n        let current_index = current_index().unwrap_or(0);\n        let shard_index = current_index & (self.shards.len() - 1);\n\n        match self.shards[shard_index].lock.try_read() {\n            Ok(guard) => Ok(ShardedLockReadGuard {\n                lock: self,\n                _guard: guard,\n                _marker: PhantomData,\n            }),\n            Err(TryLockError::Poisoned(err)) => {\n                let guard = ShardedLockReadGuard {\n                    lock: self,\n                    _guard: err.into_inner(),\n                    _marker: PhantomData,\n                };\n                Err(TryLockError::Poisoned(PoisonError::new(guard)))\n            }\n            Err(TryLockError::WouldBlock) => Err(TryLockError::WouldBlock),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::try_write":["/// Attempts to acquire this lock with exclusive write access.\n///\n/// If the access could not be granted at this time, an error is returned. Otherwise, a guard\n/// is returned which will release the exclusive access when it is dropped. This method does\n/// not provide any guarantees with respect to the ordering of whether contentious readers or\n/// writers will acquire the lock first.\n///\n/// This method will return an error if the lock is poisoned. A lock gets poisoned when a write\n/// operation panics.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let lock = ShardedLock::new(1);\n///\n/// let n = lock.read().unwrap();\n/// assert_eq!(*n, 1);\n///\n/// assert!(lock.try_write().is_err());\n/// ```\npub fn try_write(&self) -> TryLockResult<ShardedLockWriteGuard<'_, T>>{\n        let mut poisoned = false;\n        let mut blocked = None;\n\n        // Write-lock each shard in succession.\n        for (i, shard) in self.shards.iter().enumerate() {\n            let guard = match shard.lock.try_write() {\n                Ok(guard) => guard,\n                Err(TryLockError::Poisoned(err)) => {\n                    poisoned = true;\n                    err.into_inner()\n                }\n                Err(TryLockError::WouldBlock) => {\n                    blocked = Some(i);\n                    break;\n                }\n            };\n\n            // Store the guard into the shard.\n            unsafe {\n                let guard: RwLockWriteGuard<'static, ()> = mem::transmute(guard);\n                let dest: *mut _ = shard.write_guard.get();\n                *dest = Some(guard);\n            }\n        }\n\n        if let Some(i) = blocked {\n            // Unlock the shards in reverse order of locking.\n            for shard in self.shards[0..i].iter().rev() {\n                unsafe {\n                    let dest: *mut _ = shard.write_guard.get();\n                    let guard = mem::replace(&mut *dest, None);\n                    drop(guard);\n                }\n            }\n            Err(TryLockError::WouldBlock)\n        } else if poisoned {\n            let guard = ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            };\n            Err(TryLockError::Poisoned(PoisonError::new(guard)))\n        } else {\n            Ok(ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            })\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLock::<T>::write":["/// Locks with exclusive write access, blocking the current thread until it can be acquired.\n///\n/// The calling thread will be blocked until there are no more writers which hold the lock.\n/// There may be other readers currently inside the lock when this method returns. This method\n/// does not provide any guarantees with respect to the ordering of whether contentious readers\n/// or writers will acquire the lock first.\n///\n/// Returns a guard which will release the exclusive access when dropped.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::ShardedLock;\n///\n/// let lock = ShardedLock::new(1);\n///\n/// let mut n = lock.write().unwrap();\n/// *n = 2;\n///\n/// assert!(lock.try_read().is_err());\n/// ```\npub fn write(&self) -> LockResult<ShardedLockWriteGuard<'_, T>>{\n        let mut poisoned = false;\n\n        // Write-lock each shard in succession.\n        for shard in self.shards.iter() {\n            let guard = match shard.lock.write() {\n                Ok(guard) => guard,\n                Err(err) => {\n                    poisoned = true;\n                    err.into_inner()\n                }\n            };\n\n            // Store the guard into the shard.\n            unsafe {\n                let guard: RwLockWriteGuard<'_, ()> = guard;\n                let guard: RwLockWriteGuard<'static, ()> = mem::transmute(guard);\n                let dest: *mut _ = shard.write_guard.get();\n                *dest = Some(guard);\n            }\n        }\n\n        if poisoned {\n            Err(PoisonError::new(ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            }))\n        } else {\n            Ok(ShardedLockWriteGuard {\n                lock: self,\n                _marker: PhantomData,\n            })\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLockReadGuard":["/// A guard used to release the shared read access of a [`ShardedLock`] when dropped.\n///\n/// [`ShardedLock`]: struct.ShardedLock.html\npub struct ShardedLockReadGuard<'a, T: ?Sized> {\n    lock: &'a ShardedLock<T>,\n    _guard: RwLockReadGuard<'a, ()>,\n    _marker: PhantomData<RwLockReadGuard<'a, T>>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::ShardedLockWriteGuard":["/// A guard used to release the exclusive write access of a [`ShardedLock`] when dropped.\n///\n/// [`ShardedLock`]: struct.ShardedLock.html\npub struct ShardedLockWriteGuard<'a, T: ?Sized> {\n    lock: &'a ShardedLock<T>,\n    _marker: PhantomData<RwLockWriteGuard<'a, T>>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::THREAD_INDICES":["#[allow(missing_copy_implementations)]\n#[allow(non_camel_case_types)]\n#[allow(dead_code)]\nstruct $N {__private_field: ()}","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))"],"sync::sharded_lock::ThreadIndices":["/// The global registry keeping track of registered threads and indices.\nstruct ThreadIndices {\n    /// Mapping from `ThreadId` to thread index.\n    mapping: HashMap<ThreadId, usize>,\n\n    /// A list of free indices.\n    free_list: Vec<usize>,\n\n    /// The next index to allocate if the free list is empty.\n    next_index: usize,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::sharded_lock::current_index":["/// Returns a `usize` that identifies the current thread.\n///\n/// Each thread is associated with an 'index'. While there are no particular guarantees, indices\n/// usually tend to be consecutive numbers between 0 and the number of running threads.\n///\n/// Since this function accesses TLS, `None` might be returned if the current thread's TLS is\n/// tearing down.\n#[inline]\nfn current_index() -> Option<usize>{\n    REGISTRATION.try_with(|reg| reg.index).ok()\n}","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))"],"sync::wait_group::Inner":["/// Inner state of a `WaitGroup`.\nstruct Inner {\n    cvar: Condvar,\n    count: Mutex<usize>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"sync::wait_group::WaitGroup":["/// Enables threads to synchronize the beginning or end of some computation.\n///\n/// # Wait groups vs barriers\n///\n/// `WaitGroup` is very similar to [`Barrier`], but there are a few differences:\n///\n/// * `Barrier` needs to know the number of threads at construction, while `WaitGroup` is cloned to\n///   register more threads.\n///\n/// * A `Barrier` can be reused even after all threads have synchronized, while a `WaitGroup`\n///   synchronizes threads only once.\n///\n/// * All threads wait for others to reach the `Barrier`. With `WaitGroup`, each thread can choose\n///   to either wait for other threads or to continue without blocking.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::WaitGroup;\n/// use std::thread;\n///\n/// // Create a new wait group.\n/// let wg = WaitGroup::new();\n///\n/// for _ in 0..4 {\n///     // Create another reference to the wait group.\n///     let wg = wg.clone();\n///\n///     thread::spawn(move || {\n///         // Do some work.\n///\n///         // Drop the reference to the wait group.\n///         drop(wg);\n///     });\n/// }\n///\n/// // Block until all threads have finished their work.\n/// wg.wait();\n/// ```\n///\n/// [`Barrier`]: https://doc.rust-lang.org/std/sync/struct.Barrier.html\npub struct WaitGroup {\n    inner: Arc<Inner>,\n}","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"sync::wait_group::WaitGroup::new":["/// Creates a new wait group and returns the single reference to it.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::WaitGroup;\n///\n/// let wg = WaitGroup::new();\n/// ```\npub fn new() -> Self{\n        Self::default()\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"sync::wait_group::WaitGroup::wait":["/// Drops this reference and waits until all other references are dropped.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::sync::WaitGroup;\n/// use std::thread;\n///\n/// let wg = WaitGroup::new();\n///\n/// thread::spawn({\n///     let wg = wg.clone();\n///     move || {\n///         // Block until both threads have reached `wait()`.\n///         wg.wait();\n///     }\n/// });\n///\n/// // Block until both threads have reached `wait()`.\n/// wg.wait();\n/// ```\npub fn wait(self){\n        if *self.inner.count.lock().unwrap() == 1 {\n            return;\n        }\n\n        let inner = self.inner.clone();\n        drop(self);\n\n        let mut count = inner.count.lock().unwrap();\n        while *count > 0 {\n            count = inner.cvar.wait(count).unwrap();\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))"],"thread::Scope":["/// A scope for spawning threads.\npub struct Scope<'env> {\n    /// The list of the thread join handles.\n    handles: SharedVec<SharedOption<thread::JoinHandle<()>>>,\n\n    /// Used to wait until all subscopes all dropped.\n    wait_group: WaitGroup,\n\n    /// Borrows data with invariant lifetime `'env`.\n    _marker: PhantomData<&'env mut &'env ()>,\n}","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::Scope::<'env>::builder":["/// Creates a builder that can configure a thread before spawning.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     s.builder()\n///         .spawn(|_| println!(\"A child thread is running\"))\n///         .unwrap();\n/// }).unwrap();\n/// ```\npub fn builder<'scope>(&'scope self) -> ScopedThreadBuilder<'scope, 'env>{\n        ScopedThreadBuilder {\n            scope: self,\n            builder: thread::Builder::new(),\n        }\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::Scope::<'env>::spawn":["/// Spawns a scoped thread.\n///\n/// This method is similar to the [`spawn`] function in Rust's standard library. The difference\n/// is that this thread is scoped, meaning it's guaranteed to terminate before the scope exits,\n/// allowing it to reference variables outside the scope.\n///\n/// The scoped thread is passed a reference to this scope as an argument, which can be used for\n/// spawning nested threads.\n///\n/// The returned handle can be used to manually join the thread before the scope exits.\n///\n/// [`spawn`]: https://doc.rust-lang.org/std/thread/fn.spawn.html\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     let handle = s.spawn(|_| {\n///         println!(\"A child thread is running\");\n///         42\n///     });\n///\n///     // Join the thread and retrieve its result.\n///     let res = handle.join().unwrap();\n///     assert_eq!(res, 42);\n/// }).unwrap();\n/// ```\npub fn spawn<'scope, F, T>(&'scope self, f: F) -> ScopedJoinHandle<'scope, T>\n    where\n        F: FnOnce(&Scope<'env>) -> T,\n        F: Send + 'env,\n        T: Send + 'env,{\n        self.builder().spawn(f).unwrap()\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedJoinHandle":["/// A handle that can be used to join its scoped thread.\npub struct ScopedJoinHandle<'scope, T> {\n    /// A join handle to the spawned thread.\n    handle: SharedOption<thread::JoinHandle<()>>,\n\n    /// Holds the result of the inner closure.\n    result: SharedOption<T>,\n\n    /// A handle to the the spawned thread.\n    thread: thread::Thread,\n\n    /// Borrows the parent scope with lifetime `'scope`.\n    _marker: PhantomData<&'scope ()>,\n}","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedJoinHandle::<'_, T>::join":["/// Waits for the thread to finish and returns its result.\n///\n/// If the child thread panics, an error is returned.\n///\n/// # Panics\n///\n/// This function may panic on some platforms if a thread attempts to join itself or otherwise\n/// may create a deadlock with joining threads.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     let handle1 = s.spawn(|_| println!(\"I'm a happy thread :)\"));\n///     let handle2 = s.spawn(|_| panic!(\"I'm a sad thread :(\"));\n///\n///     // Join the first thread and verify that it succeeded.\n///     let res = handle1.join();\n///     assert!(res.is_ok());\n///\n///     // Join the second thread and verify that it panicked.\n///     let res = handle2.join();\n///     assert!(res.is_err());\n/// }).unwrap();\n/// ```\npub fn join(self) -> thread::Result<T>{\n        // Take out the handle. The handle will surely be available because the root scope waits\n        // for nested scopes before joining remaining threads.\n        let handle = self.handle.lock().unwrap().take().unwrap();\n\n        // Join the thread and then take the result out of its inner closure.\n        handle\n            .join()\n            .map(|()| self.result.lock().unwrap().take().unwrap())\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedJoinHandle::<'_, T>::thread":["/// Returns a handle to the underlying thread.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     let handle = s.spawn(|_| println!(\"A child thread is running\"));\n///     println!(\"The child thread ID: {:?}\", handle.thread().id());\n/// }).unwrap();\n/// ```\npub fn thread(&self) -> &thread::Thread{\n        &self.thread\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedThreadBuilder":["/// Configures the properties of a new thread.\n///\n/// The two configurable properties are:\n///\n/// - [`name`]: Specifies an [associated name for the thread][naming-threads].\n/// - [`stack_size`]: Specifies the [desired stack size for the thread][stack-size].\n///\n/// The [`spawn`] method will take ownership of the builder and return an [`io::Result`] of the\n/// thread handle with the given configuration.\n///\n/// The [`Scope::spawn`] method uses a builder with default configuration and unwraps its return\n/// value. You may want to use this builder when you want to recover from a failure to launch a\n/// thread.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     s.builder()\n///         .spawn(|_| println!(\"Running a child thread\"))\n///         .unwrap();\n/// }).unwrap();\n/// ```\n///\n/// [`name`]: struct.ScopedThreadBuilder.html#method.name\n/// [`stack_size`]: struct.ScopedThreadBuilder.html#method.stack_size\n/// [`spawn`]: struct.ScopedThreadBuilder.html#method.spawn\n/// [`Scope::spawn`]: struct.Scope.html#method.spawn\n/// [`io::Result`]: https://doc.rust-lang.org/std/io/type.Result.html\n/// [naming-threads]: https://doc.rust-lang.org/std/thread/index.html#naming-threads\n/// [stack-size]: https://doc.rust-lang.org/std/thread/index.html#stack-size\npub struct ScopedThreadBuilder<'scope, 'env> {\n    scope: &'scope Scope<'env>,\n    builder: thread::Builder,\n}","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedThreadBuilder::<'scope, 'env>::name":["/// Sets the name for the new thread.\n///\n/// The name must not contain null bytes. For more information about named threads, see\n/// [here][naming-threads].\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n/// use std::thread::current;\n///\n/// thread::scope(|s| {\n///     s.builder()\n///         .name(\"my thread\".to_string())\n///         .spawn(|_| assert_eq!(current().name(), Some(\"my thread\")))\n///         .unwrap();\n/// }).unwrap();\n/// ```\n///\n/// [naming-threads]: https://doc.rust-lang.org/std/thread/index.html#naming-threads\npub fn name(mut self, name: String) -> ScopedThreadBuilder<'scope, 'env>{\n        self.builder = self.builder.name(name);\n        self\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedThreadBuilder::<'scope, 'env>::spawn":["/// Spawns a scoped thread with this configuration.\n///\n/// The scoped thread is passed a reference to this scope as an argument, which can be used for\n/// spawning nested threads.\n///\n/// The returned handle can be used to manually join the thread before the scope exits.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     let handle = s.builder()\n///         .spawn(|_| {\n///             println!(\"A child thread is running\");\n///             42\n///         })\n///         .unwrap();\n///\n///     // Join the thread and retrieve its result.\n///     let res = handle.join().unwrap();\n///     assert_eq!(res, 42);\n/// }).unwrap();\n/// ```\npub fn spawn<F, T>(self, f: F) -> io::Result<ScopedJoinHandle<'scope, T>>\n    where\n        F: FnOnce(&Scope<'env>) -> T,\n        F: Send + 'env,\n        T: Send + 'env,{\n        // The result of `f` will be stored here.\n        let result = SharedOption::default();\n\n        // Spawn the thread and grab its join handle and thread handle.\n        let (handle, thread) = {\n            let result = Arc::clone(&result);\n\n            // A clone of the scope that will be moved into the new thread.\n            let scope = Scope::<'env> {\n                handles: Arc::clone(&self.scope.handles),\n                wait_group: self.scope.wait_group.clone(),\n                _marker: PhantomData,\n            };\n\n            // Spawn the thread.\n            let handle = {\n                let closure = move || {\n                    // Make sure the scope is inside the closure with the proper `'env` lifetime.\n                    let scope: Scope<'env> = scope;\n\n                    // Run the closure.\n                    let res = f(&scope);\n\n                    // Store the result if the closure didn't panic.\n                    *result.lock().unwrap() = Some(res);\n                };\n\n                // Allocate `clsoure` on the heap and erase the `'env` bound.\n                let closure: Box<dyn FnOnce() + Send + 'env> = Box::new(closure);\n                let closure: Box<dyn FnOnce() + Send + 'static> =\n                    unsafe { mem::transmute(closure) };\n\n                // Finally, spawn the closure.\n                self.builder.spawn(move || closure())?\n            };\n\n            let thread = handle.thread().clone();\n            let handle = Arc::new(Mutex::new(Some(handle)));\n            (handle, thread)\n        };\n\n        // Add the handle to the shared list of join handles.\n        self.scope.handles.lock().unwrap().push(Arc::clone(&handle));\n\n        Ok(ScopedJoinHandle {\n            handle,\n            result,\n            thread,\n            _marker: PhantomData,\n        })\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::ScopedThreadBuilder::<'scope, 'env>::stack_size":["/// Sets the size of the stack for the new thread.\n///\n/// The stack size is measured in bytes.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// thread::scope(|s| {\n///     s.builder()\n///         .stack_size(32 * 1024)\n///         .spawn(|_| println!(\"Running a child thread\"))\n///         .unwrap();\n/// }).unwrap();\n/// ```\npub fn stack_size(mut self, size: usize) -> ScopedThreadBuilder<'scope, 'env>{\n        self.builder = self.builder.stack_size(size);\n        self\n    }","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"],"thread::scope":["/// Creates a new scope for spawning threads.\n///\n/// All child threads that haven't been manually joined will be automatically joined just before\n/// this function invocation ends. If all joined threads have successfully completed, `Ok` is\n/// returned with the return value of `f`. If any of the joined threads has panicked, an `Err` is\n/// returned containing errors from panicked threads.\n///\n/// # Examples\n///\n/// ```\n/// use crossbeam_utils::thread;\n///\n/// let var = vec![1, 2, 3];\n///\n/// thread::scope(|s| {\n///     s.spawn(|_| {\n///         println!(\"A child thread borrowing `var`: {:?}\", var);\n///     });\n/// }).unwrap();\n/// ```\npub fn scope<'env, F, R>(f: F) -> thread::Result<R>\nwhere\n    F: FnOnce(&Scope<'env>) -> R,{\n    let wg = WaitGroup::new();\n    let scope = Scope::<'env> {\n        handles: SharedVec::default(),\n        wait_group: wg.clone(),\n        _marker: PhantomData,\n    };\n\n    // Execute the scoped function, but catch any panics.\n    let result = panic::catch_unwind(panic::AssertUnwindSafe(|| f(&scope)));\n\n    // Wait until all nested scopes are dropped.\n    drop(scope.wait_group);\n    wg.wait();\n\n    // Join all remaining spawned threads.\n    let panics: Vec<_> = scope\n        .handles\n        .lock()\n        .unwrap()\n        // Filter handles that haven't been joined, join them, and collect errors.\n        .drain(..)\n        .filter_map(|handle| handle.lock().unwrap().take())\n        .filter_map(|handle| handle.join().err())\n        .collect();\n\n    // If `f` has panicked, resume unwinding.\n    // If any of the child threads have panicked, return the panic errors.\n    // Otherwise, everything is OK and return the result of `f`.\n    match result {\n        Err(err) => panic::resume_unwind(err),\n        Ok(res) => {\n            if panics.is_empty() {\n                Ok(res)\n            } else {\n                Err(Box::new(panics))\n            }\n        }\n    }\n}","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))"]},"struct_constructor":{"&'static atomic::seq_lock::SeqLock":["lock"],"&'static sync::sharded_lock::Registration":["__getit"],"&std::thread::Thread":["thread"],"&sync::parker::Unparker":["unparker"],"*const ()":["into_raw"],"<Self as atomic::consume::AtomicConsume>::Val":["load_consume"],"<std::sync::atomic::AtomicBool as atomic::consume::AtomicConsume>::Val":["load_consume"],"<std::sync::atomic::AtomicIsize as atomic::consume::AtomicConsume>::Val":["load_consume"],"<std::sync::atomic::AtomicPtr<T> as atomic::consume::AtomicConsume>::Val":["load_consume"],"<std::sync::atomic::AtomicUsize as atomic::consume::AtomicConsume>::Val":["load_consume"],"atomic::atomic_cell::AtomicCell":["default","new"],"atomic::seq_lock::SeqLock":["new"],"atomic::seq_lock::SeqLockWriteGuard":["write"],"backoff::Backoff":["default","new"],"bool":["atomic_is_lock_free","can_transmute","eq","is_completed","is_lock_free","is_poisoned","validate_read"],"cache_padded::CachePadded":["clone","default","from","new"],"std::sync::Mutex":["__static_ref_initialize"],"sync::parker::Parker":["default","from_raw","new"],"sync::parker::Unparker":["clone","from_raw"],"sync::sharded_lock::Registration":["__init"],"sync::sharded_lock::ShardedLock":["default","from","new"],"sync::sharded_lock::ShardedLockReadGuard":["read","try_read"],"sync::sharded_lock::ShardedLockWriteGuard":["try_write","write"],"sync::wait_group::WaitGroup":["clone","default","new"],"thread::ScopedJoinHandle":["spawn"],"thread::ScopedThreadBuilder":["builder"],"u64":["as_pthread_t","into_pthread_t"],"usize":["current_index","optimistic_read"]},"struct_to_trait":{"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder":["std::fmt::Debug"],"atomic::atomic_cell::AtomicCell":["std::default::Default","std::fmt::Debug","std::marker::Send","std::marker::Sync","std::panic::RefUnwindSafe","std::panic::UnwindSafe"],"atomic::seq_lock::SeqLockWriteGuard":["std::ops::Drop"],"backoff::Backoff":["std::default::Default","std::fmt::Debug"],"cache_padded::CachePadded":["lazy_static::__Deref","std::clone::Clone","std::cmp::Eq","std::cmp::PartialEq","std::convert::From","std::default::Default","std::fmt::Debug","std::hash::Hash","std::marker::Copy","std::marker::Send","std::marker::StructuralEq","std::marker::StructuralPartialEq","std::marker::Sync","std::ops::DerefMut"],"std::sync::atomic::AtomicBool":["atomic::consume::AtomicConsume"],"std::sync::atomic::AtomicIsize":["atomic::consume::AtomicConsume"],"std::sync::atomic::AtomicPtr":["atomic::consume::AtomicConsume"],"std::sync::atomic::AtomicUsize":["atomic::consume::AtomicConsume"],"sync::parker::Parker":["std::default::Default","std::fmt::Debug","std::marker::Send"],"sync::parker::Unparker":["std::clone::Clone","std::fmt::Debug","std::marker::Send","std::marker::Sync"],"sync::sharded_lock::Registration":["std::ops::Drop"],"sync::sharded_lock::ShardedLock":["std::convert::From","std::default::Default","std::fmt::Debug","std::marker::Send","std::marker::Sync","std::panic::RefUnwindSafe","std::panic::UnwindSafe"],"sync::sharded_lock::ShardedLockReadGuard":["lazy_static::__Deref","std::fmt::Debug","std::fmt::Display","std::marker::Sync"],"sync::sharded_lock::ShardedLockWriteGuard":["lazy_static::__Deref","std::fmt::Debug","std::fmt::Display","std::marker::Sync","std::ops::DerefMut","std::ops::Drop"],"sync::sharded_lock::THREAD_INDICES":["lazy_static::LazyStatic","lazy_static::__Deref"],"sync::wait_group::WaitGroup":["std::clone::Clone","std::default::Default","std::fmt::Debug","std::ops::Drop"],"thread::Scope":["std::fmt::Debug","std::marker::Sync"],"thread::ScopedJoinHandle":["std::fmt::Debug","std::marker::Send","std::marker::Sync","std::os::unix::thread::JoinHandleExt"],"thread::ScopedThreadBuilder":["std::fmt::Debug"]},"targets":{"<<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::fmt::Debug"],"<atomic::atomic_cell::AtomicCell<T> as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))","std::default::Default"],"<atomic::atomic_cell::AtomicCell<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))","std::fmt::Debug"],"<atomic::seq_lock::SeqLockWriteGuard as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))","std::ops::Drop"],"<backoff::Backoff as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))","std::default::Default"],"<backoff::Backoff as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))","std::fmt::Debug"],"<cache_padded::CachePadded<T> as lazy_static::__Deref>::deref":["deref","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))","lazy_static::__Deref"],"<cache_padded::CachePadded<T> as std::convert::From<T>>::from":["from","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))","std::convert::From"],"<cache_padded::CachePadded<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))","std::fmt::Debug"],"<cache_padded::CachePadded<T> as std::ops::DerefMut>::deref_mut":["deref_mut","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))","std::ops::DerefMut"],"<std::sync::atomic::AtomicBool as atomic::consume::AtomicConsume>::load_consume":["load_consume","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))","atomic::consume::AtomicConsume"],"<std::sync::atomic::AtomicIsize as atomic::consume::AtomicConsume>::load_consume":["load_consume","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))","atomic::consume::AtomicConsume"],"<std::sync::atomic::AtomicPtr<T> as atomic::consume::AtomicConsume>::load_consume":["load_consume","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))","atomic::consume::AtomicConsume"],"<std::sync::atomic::AtomicUsize as atomic::consume::AtomicConsume>::load_consume":["load_consume","Real(LocalPath(\"crossbeam-utils/src/atomic/consume.rs\"))","atomic::consume::AtomicConsume"],"<sync::parker::Parker as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))","std::default::Default"],"<sync::parker::Parker as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))","std::fmt::Debug"],"<sync::parker::Unparker as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))","std::clone::Clone"],"<sync::parker::Unparker as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))","std::fmt::Debug"],"<sync::sharded_lock::Registration as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::ops::Drop"],"<sync::sharded_lock::ShardedLock<T> as std::convert::From<T>>::from":["from","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::convert::From"],"<sync::sharded_lock::ShardedLock<T> as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::default::Default"],"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::fmt::Debug"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as lazy_static::__Deref>::deref":["deref","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","lazy_static::__Deref"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::fmt::Debug"],"<sync::sharded_lock::ShardedLockReadGuard<'_, T> as std::fmt::Display>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::fmt::Display"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as lazy_static::__Deref>::deref":["deref","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","lazy_static::__Deref"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::fmt::Debug"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::fmt::Display>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::fmt::Display"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::ops::DerefMut>::deref_mut":["deref_mut","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::ops::DerefMut"],"<sync::sharded_lock::ShardedLockWriteGuard<'_, T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))","std::ops::Drop"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::LazyStatic>::initialize":["initialize","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))","lazy_static::LazyStatic"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref":["deref","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))","lazy_static::__Deref"],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref::__stability":["__stability","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))",""],"<sync::sharded_lock::THREAD_INDICES as lazy_static::__Deref>::deref::__static_ref_initialize":["__static_ref_initialize","Real(LocalPath(\"/home/xiang/.cargo/registry/src/github.com-1ecc6299db9ec823/lazy_static-1.4.0/src/lib.rs\"))",""],"<sync::wait_group::WaitGroup as std::clone::Clone>::clone":["clone","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))","std::clone::Clone"],"<sync::wait_group::WaitGroup as std::default::Default>::default":["default","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))","std::default::Default"],"<sync::wait_group::WaitGroup as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))","std::fmt::Debug"],"<sync::wait_group::WaitGroup as std::ops::Drop>::drop":["drop","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))","std::ops::Drop"],"<thread::Scope<'_> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))","std::fmt::Debug"],"<thread::ScopedJoinHandle<'_, T> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))","std::fmt::Debug"],"<thread::ScopedJoinHandle<'_, T> as std::os::unix::thread::JoinHandleExt>::as_pthread_t":["as_pthread_t","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))","std::os::unix::thread::JoinHandleExt"],"<thread::ScopedJoinHandle<'_, T> as std::os::unix::thread::JoinHandleExt>::into_pthread_t":["into_pthread_t","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))","std::os::unix::thread::JoinHandleExt"],"atomic::atomic_cell::AtomicCell::<T>::as_ptr":["as_ptr","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::compare_and_swap":["compare_and_swap","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::compare_exchange":["compare_exchange","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::into_inner":["into_inner","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::is_lock_free":["is_lock_free","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::load":["load","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::new":["new","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::store":["store","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::swap":["swap","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<T>::take":["take","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<bool>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<bool>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<bool>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i16>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i16>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i16>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i16>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i16>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i32>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i32>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i32>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i32>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i32>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i64>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i64>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i64>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i64>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i64>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i8>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i8>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i8>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i8>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<i8>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<isize>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<isize>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<isize>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<isize>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<isize>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u16>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u16>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u16>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u16>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u16>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u32>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u32>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u32>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u32>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u32>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u64>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u64>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u64>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u64>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u64>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u8>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u8>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u8>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u8>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<u8>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<usize>::fetch_add":["fetch_add","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<usize>::fetch_and":["fetch_and","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<usize>::fetch_or":["fetch_or","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<usize>::fetch_sub":["fetch_sub","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicCell::<usize>::fetch_xor":["fetch_xor","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicUnit::compare_exchange_weak":["compare_exchange_weak","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicUnit::load":["load","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicUnit::store":["store","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::AtomicUnit::swap":["swap","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::atomic_compare_exchange_weak":["atomic_compare_exchange_weak","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::atomic_is_lock_free":["atomic_is_lock_free","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::atomic_load":["atomic_load","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::atomic_store":["atomic_store","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::atomic_swap":["atomic_swap","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::can_transmute":["can_transmute","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::atomic_cell::lock":["lock","Real(LocalPath(\"crossbeam-utils/src/atomic/atomic_cell.rs\"))",""],"atomic::seq_lock::SeqLock::new":["new","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))",""],"atomic::seq_lock::SeqLock::optimistic_read":["optimistic_read","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))",""],"atomic::seq_lock::SeqLock::validate_read":["validate_read","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))",""],"atomic::seq_lock::SeqLock::write":["write","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))",""],"atomic::seq_lock::SeqLockWriteGuard::abort":["abort","Real(LocalPath(\"crossbeam-utils/src/atomic/seq_lock.rs\"))",""],"backoff::Backoff::is_completed":["is_completed","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))",""],"backoff::Backoff::new":["new","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))",""],"backoff::Backoff::reset":["reset","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))",""],"backoff::Backoff::snooze":["snooze","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))",""],"backoff::Backoff::spin":["spin","Real(LocalPath(\"crossbeam-utils/src/backoff.rs\"))",""],"cache_padded::CachePadded::<T>::into_inner":["into_inner","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))",""],"cache_padded::CachePadded::<T>::new":["new","Real(LocalPath(\"crossbeam-utils/src/cache_padded.rs\"))",""],"sync::parker::Inner::park":["park","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Inner::unpark":["unpark","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Parker::from_raw":["from_raw","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Parker::into_raw":["into_raw","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Parker::new":["new","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Parker::park":["park","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Parker::park_timeout":["park_timeout","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Parker::unparker":["unparker","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Unparker::from_raw":["from_raw","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Unparker::into_raw":["into_raw","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::parker::Unparker::unpark":["unpark","Real(LocalPath(\"crossbeam-utils/src/sync/parker.rs\"))",""],"sync::sharded_lock::REGISTRATION::__getit":["__getit","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })",""],"sync::sharded_lock::REGISTRATION::__init":["__init","Real(Remapped { local_path: Some(\"/home/xiang/.rustup/toolchains/nightly-2022-12-10-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/local.rs\"), virtual_name: \"/rustc/dfe3fe710181738a2cb3060c23ec5efb3c68ca09/library/std/src/thread/local.rs\" })",""],"sync::sharded_lock::ShardedLock::<T>::get_mut":["get_mut","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::into_inner":["into_inner","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::is_poisoned":["is_poisoned","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::new":["new","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::read":["read","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::try_read":["try_read","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::try_write":["try_write","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::ShardedLock::<T>::write":["write","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::sharded_lock::current_index":["current_index","Real(LocalPath(\"crossbeam-utils/src/sync/sharded_lock.rs\"))",""],"sync::wait_group::WaitGroup::new":["new","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))",""],"sync::wait_group::WaitGroup::wait":["wait","Real(LocalPath(\"crossbeam-utils/src/sync/wait_group.rs\"))",""],"thread::Scope::<'env>::builder":["builder","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::Scope::<'env>::spawn":["spawn","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::ScopedJoinHandle::<'_, T>::join":["join","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::ScopedJoinHandle::<'_, T>::thread":["thread","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::ScopedThreadBuilder::<'scope, 'env>::name":["name","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::ScopedThreadBuilder::<'scope, 'env>::spawn":["spawn","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::ScopedThreadBuilder::<'scope, 'env>::stack_size":["stack_size","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""],"thread::scope":["scope","Real(LocalPath(\"crossbeam-utils/src/thread.rs\"))",""]},"trait_to_struct":{"atomic::consume::AtomicConsume":["std::sync::atomic::AtomicBool","std::sync::atomic::AtomicIsize","std::sync::atomic::AtomicPtr","std::sync::atomic::AtomicUsize"],"lazy_static::LazyStatic":["sync::sharded_lock::THREAD_INDICES"],"lazy_static::__Deref":["cache_padded::CachePadded","sync::sharded_lock::ShardedLockReadGuard","sync::sharded_lock::ShardedLockWriteGuard","sync::sharded_lock::THREAD_INDICES"],"std::clone::Clone":["cache_padded::CachePadded","sync::parker::Unparker","sync::wait_group::WaitGroup"],"std::cmp::Eq":["cache_padded::CachePadded"],"std::cmp::PartialEq":["cache_padded::CachePadded"],"std::convert::From":["cache_padded::CachePadded","sync::sharded_lock::ShardedLock"],"std::default::Default":["atomic::atomic_cell::AtomicCell","backoff::Backoff","cache_padded::CachePadded","sync::parker::Parker","sync::sharded_lock::ShardedLock","sync::wait_group::WaitGroup"],"std::fmt::Debug":["<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder","atomic::atomic_cell::AtomicCell","backoff::Backoff","cache_padded::CachePadded","sync::parker::Parker","sync::parker::Unparker","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard","sync::sharded_lock::ShardedLockWriteGuard","sync::wait_group::WaitGroup","thread::Scope","thread::ScopedJoinHandle","thread::ScopedThreadBuilder"],"std::fmt::Display":["sync::sharded_lock::ShardedLockReadGuard","sync::sharded_lock::ShardedLockWriteGuard"],"std::hash::Hash":["cache_padded::CachePadded"],"std::marker::Copy":["cache_padded::CachePadded"],"std::marker::Send":["atomic::atomic_cell::AtomicCell","cache_padded::CachePadded","sync::parker::Parker","sync::parker::Unparker","sync::sharded_lock::ShardedLock","thread::ScopedJoinHandle"],"std::marker::StructuralEq":["cache_padded::CachePadded"],"std::marker::StructuralPartialEq":["cache_padded::CachePadded"],"std::marker::Sync":["atomic::atomic_cell::AtomicCell","cache_padded::CachePadded","sync::parker::Unparker","sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard","sync::sharded_lock::ShardedLockWriteGuard","thread::Scope","thread::ScopedJoinHandle"],"std::ops::DerefMut":["cache_padded::CachePadded","sync::sharded_lock::ShardedLockWriteGuard"],"std::ops::Drop":["atomic::seq_lock::SeqLockWriteGuard","sync::sharded_lock::Registration","sync::sharded_lock::ShardedLockWriteGuard","sync::wait_group::WaitGroup"],"std::os::unix::thread::JoinHandleExt":["thread::ScopedJoinHandle"],"std::panic::RefUnwindSafe":["atomic::atomic_cell::AtomicCell","sync::sharded_lock::ShardedLock"],"std::panic::UnwindSafe":["atomic::atomic_cell::AtomicCell","sync::sharded_lock::ShardedLock"]},"type_to_def_path":{"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder":"<sync::sharded_lock::ShardedLock<T> as std::fmt::Debug>::fmt::LockedPlaceholder","atomic::atomic_cell::AtomicCell<T>":"atomic::atomic_cell::AtomicCell","atomic::atomic_cell::AtomicUnit":"atomic::atomic_cell::AtomicUnit","atomic::seq_lock::SeqLock":"atomic::seq_lock::SeqLock","atomic::seq_lock::SeqLockWriteGuard":"atomic::seq_lock::SeqLockWriteGuard","backoff::Backoff":"backoff::Backoff","cache_padded::CachePadded<T>":"cache_padded::CachePadded","sync::parker::Inner":"sync::parker::Inner","sync::parker::Parker":"sync::parker::Parker","sync::parker::Unparker":"sync::parker::Unparker","sync::sharded_lock::Registration":"sync::sharded_lock::Registration","sync::sharded_lock::Shard":"sync::sharded_lock::Shard","sync::sharded_lock::ShardedLock<T>":"sync::sharded_lock::ShardedLock","sync::sharded_lock::ShardedLockReadGuard<'a, T>":"sync::sharded_lock::ShardedLockReadGuard","sync::sharded_lock::ShardedLockWriteGuard<'a, T>":"sync::sharded_lock::ShardedLockWriteGuard","sync::sharded_lock::THREAD_INDICES":"sync::sharded_lock::THREAD_INDICES","sync::sharded_lock::ThreadIndices":"sync::sharded_lock::ThreadIndices","sync::wait_group::Inner":"sync::wait_group::Inner","sync::wait_group::WaitGroup":"sync::wait_group::WaitGroup","thread::Scope<'env>":"thread::Scope","thread::ScopedJoinHandle<'scope, T>":"thread::ScopedJoinHandle","thread::ScopedThreadBuilder<'scope, 'env>":"thread::ScopedThreadBuilder"}}